{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and visualazing datasets for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(10000, 1) #X are 100 1-d feature vector\n",
    "y = 3 * X + np.random.rand(10000, 1) #y are true label values of X\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"Features\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.axis([0,2, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Gradient Descent with multiple learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, weight: [[0.53469299]], train loss: [[10.92943377]]\n",
      "iteration: 2, weight: [[0.68631069]], train loss: [[9.81066542]]\n",
      "iteration: 3, weight: [[0.82984808]], train loss: [[8.80796661]]\n",
      "iteration: 4, weight: [[0.9657358]], train loss: [[7.90929543]]\n",
      "iteration: 5, weight: [[1.09438153]], train loss: [[7.10385924]]\n",
      "iteration: 6, weight: [[1.21617122]], train loss: [[6.38198513]]\n",
      "iteration: 7, weight: [[1.33147026]], train loss: [[5.73500375]]\n",
      "iteration: 8, weight: [[1.44062456]], train loss: [[5.15514514]]\n",
      "iteration: 9, weight: [[1.5439616]], train loss: [[4.63544549]]\n",
      "iteration: 10, weight: [[1.64179141]], train loss: [[4.16966344]]\n",
      "iteration: 11, weight: [[1.73440748]], train loss: [[3.75220516]]\n",
      "iteration: 12, weight: [[1.82208767]], train loss: [[3.37805719]]\n",
      "iteration: 13, weight: [[1.90509504]], train loss: [[3.04272618]]\n",
      "iteration: 14, weight: [[1.98367862]], train loss: [[2.74218496]]\n",
      "iteration: 15, weight: [[2.05807418]], train loss: [[2.47282418]]\n",
      "iteration: 16, weight: [[2.1285049]], train loss: [[2.23140894]]\n",
      "iteration: 17, weight: [[2.1951821]], train loss: [[2.01503996]]\n",
      "iteration: 18, weight: [[2.2583058]], train loss: [[1.82111874]]\n",
      "iteration: 19, weight: [[2.31806539]], train loss: [[1.64731639]]\n",
      "iteration: 20, weight: [[2.37464016]], train loss: [[1.49154562]]\n",
      "iteration: 21, weight: [[2.42819984]], train loss: [[1.35193569]]\n",
      "iteration: 22, weight: [[2.47890511]], train loss: [[1.22680996]]\n",
      "iteration: 23, weight: [[2.5269081]], train loss: [[1.11466573]]\n",
      "iteration: 24, weight: [[2.57235282]], train loss: [[1.0141562]]\n",
      "iteration: 25, weight: [[2.61537561]], train loss: [[0.92407429]]\n",
      "iteration: 26, weight: [[2.65610555]], train loss: [[0.84333816]]\n",
      "iteration: 27, weight: [[2.69466483]], train loss: [[0.77097822]]\n",
      "iteration: 28, weight: [[2.73116913]], train loss: [[0.70612544]]\n",
      "iteration: 29, weight: [[2.76572798]], train loss: [[0.64800099]]\n",
      "iteration: 30, weight: [[2.79844505]], train loss: [[0.59590681]]\n",
      "iteration: 31, weight: [[2.8294185]], train loss: [[0.54921728]]\n",
      "iteration: 32, weight: [[2.85874125]], train loss: [[0.50737168]]\n",
      "iteration: 33, weight: [[2.88650128]], train loss: [[0.46986745]]\n",
      "iteration: 34, weight: [[2.91278186]], train loss: [[0.43625421]]\n",
      "iteration: 35, weight: [[2.93766185]], train loss: [[0.40612825]]\n",
      "iteration: 36, weight: [[2.96121589]], train loss: [[0.37912779]]\n",
      "iteration: 37, weight: [[2.98351464]], train loss: [[0.35492857]]\n",
      "iteration: 38, weight: [[3.00462501]], train loss: [[0.33323996]]\n",
      "iteration: 39, weight: [[3.02461031]], train loss: [[0.31380149]]\n",
      "iteration: 40, weight: [[3.04353053]], train loss: [[0.29637971]]\n",
      "iteration: 41, weight: [[3.06144241]], train loss: [[0.2807654]]\n",
      "iteration: 42, weight: [[3.07839969]], train loss: [[0.26677104]]\n",
      "iteration: 43, weight: [[3.09445326]], train loss: [[0.25422856]]\n",
      "iteration: 44, weight: [[3.10965126]], train loss: [[0.24298734]]\n",
      "iteration: 45, weight: [[3.12403931]], train loss: [[0.23291236]]\n",
      "iteration: 46, weight: [[3.13766056]], train loss: [[0.22388264]]\n",
      "iteration: 47, weight: [[3.15055588]], train loss: [[0.21578973]]\n",
      "iteration: 48, weight: [[3.16276396]], train loss: [[0.20853644]]\n",
      "iteration: 49, weight: [[3.17432142]], train loss: [[0.20203566]]\n",
      "iteration: 50, weight: [[3.18526294]], train loss: [[0.19620932]]\n",
      "iteration: 51, weight: [[3.19562134]], train loss: [[0.19098746]]\n",
      "iteration: 52, weight: [[3.20542771]], train loss: [[0.18630734]]\n",
      "iteration: 53, weight: [[3.21471145]], train loss: [[0.18211278]]\n",
      "iteration: 54, weight: [[3.22350042]], train loss: [[0.1783534]]\n",
      "iteration: 55, weight: [[3.231821]], train loss: [[0.17498404]]\n",
      "iteration: 56, weight: [[3.23969814]], train loss: [[0.17196424]]\n",
      "iteration: 57, weight: [[3.24715548]], train loss: [[0.16925774]]\n",
      "iteration: 58, weight: [[3.25421538]], train loss: [[0.16683204]]\n",
      "iteration: 59, weight: [[3.26089904]], train loss: [[0.16465799]]\n",
      "iteration: 60, weight: [[3.2672265]], train loss: [[0.1627095]]\n",
      "iteration: 61, weight: [[3.27321674]], train loss: [[0.16096316]]\n",
      "iteration: 62, weight: [[3.27888774]], train loss: [[0.15939799]]\n",
      "iteration: 63, weight: [[3.28425651]], train loss: [[0.15799521]]\n",
      "iteration: 64, weight: [[3.28933915]], train loss: [[0.15673797]]\n",
      "iteration: 65, weight: [[3.29415093]], train loss: [[0.15561116]]\n",
      "iteration: 66, weight: [[3.29870626]], train loss: [[0.15460125]]\n",
      "iteration: 67, weight: [[3.30301882]], train loss: [[0.15369612]]\n",
      "iteration: 68, weight: [[3.30710155]], train loss: [[0.1528849]]\n",
      "iteration: 69, weight: [[3.31096669]], train loss: [[0.15215783]]\n",
      "iteration: 70, weight: [[3.31462585]], train loss: [[0.1515062]]\n",
      "iteration: 71, weight: [[3.31808999]], train loss: [[0.15092218]]\n",
      "iteration: 72, weight: [[3.32136952]], train loss: [[0.15039874]]\n",
      "iteration: 73, weight: [[3.32447427]], train loss: [[0.14992961]]\n",
      "iteration: 74, weight: [[3.32741355]], train loss: [[0.14950915]]\n",
      "iteration: 75, weight: [[3.33019619]], train loss: [[0.14913231]]\n",
      "iteration: 76, weight: [[3.33283053]], train loss: [[0.14879457]]\n",
      "iteration: 77, weight: [[3.33532448]], train loss: [[0.14849187]]\n",
      "iteration: 78, weight: [[3.33768551]], train loss: [[0.14822058]]\n",
      "iteration: 79, weight: [[3.33992071]], train loss: [[0.14797743]]\n",
      "iteration: 80, weight: [[3.3420368]], train loss: [[0.1477595]]\n",
      "iteration: 81, weight: [[3.3440401]], train loss: [[0.14756419]]\n",
      "iteration: 82, weight: [[3.34593665]], train loss: [[0.14738913]]\n",
      "iteration: 83, weight: [[3.34773212]], train loss: [[0.14723224]]\n",
      "iteration: 84, weight: [[3.3494319]], train loss: [[0.14709163]]\n",
      "iteration: 85, weight: [[3.35104109]], train loss: [[0.14696561]]\n",
      "iteration: 86, weight: [[3.35256452]], train loss: [[0.14685266]]\n",
      "iteration: 87, weight: [[3.35400677]], train loss: [[0.14675142]]\n",
      "iteration: 88, weight: [[3.35537215]], train loss: [[0.14666069]]\n",
      "iteration: 89, weight: [[3.35666476]], train loss: [[0.14657938]]\n",
      "iteration: 90, weight: [[3.35788849]], train loss: [[0.1465065]]\n",
      "iteration: 91, weight: [[3.359047]], train loss: [[0.14644118]]\n",
      "iteration: 92, weight: [[3.36014376]], train loss: [[0.14638264]]\n",
      "iteration: 93, weight: [[3.36118208]], train loss: [[0.14633017]]\n",
      "iteration: 94, weight: [[3.36216506]], train loss: [[0.14628314]]\n",
      "iteration: 95, weight: [[3.36309565]], train loss: [[0.146241]]\n",
      "iteration: 96, weight: [[3.36397665]], train loss: [[0.14620322]]\n",
      "iteration: 97, weight: [[3.3648107]], train loss: [[0.14616937]]\n",
      "iteration: 98, weight: [[3.36560029]], train loss: [[0.14613903]]\n",
      "iteration: 99, weight: [[3.36634781]], train loss: [[0.14611183]]\n",
      "iteration: 100, weight: [[3.36705549]], train loss: [[0.14608746]]\n",
      "iteration: 101, weight: [[3.36772545]], train loss: [[0.14606561]]\n",
      "iteration: 102, weight: [[3.36835971]], train loss: [[0.14604603]]\n",
      "iteration: 103, weight: [[3.36896016]], train loss: [[0.14602849]]\n",
      "iteration: 104, weight: [[3.36952862]], train loss: [[0.14601276]]\n",
      "iteration: 105, weight: [[3.37006678]], train loss: [[0.14599867]]\n",
      "iteration: 106, weight: [[3.37057626]], train loss: [[0.14598603]]\n",
      "iteration: 107, weight: [[3.37105859]], train loss: [[0.14597471]]\n",
      "iteration: 108, weight: [[3.37151521]], train loss: [[0.14596456]]\n",
      "iteration: 109, weight: [[3.3719475]], train loss: [[0.14595547]]\n",
      "iteration: 110, weight: [[3.37235674]], train loss: [[0.14594732]]\n",
      "iteration: 111, weight: [[3.37274418]], train loss: [[0.14594001]]\n",
      "iteration: 112, weight: [[3.37311097]], train loss: [[0.14593347]]\n",
      "iteration: 113, weight: [[3.37345822]], train loss: [[0.1459276]]\n",
      "iteration: 114, weight: [[3.37378695]], train loss: [[0.14592234]]\n",
      "iteration: 115, weight: [[3.37409817]], train loss: [[0.14591762]]\n",
      "iteration: 116, weight: [[3.3743928]], train loss: [[0.1459134]]\n",
      "iteration: 117, weight: [[3.37467173]], train loss: [[0.14590961]]\n",
      "iteration: 118, weight: [[3.37493579]], train loss: [[0.14590622]]\n",
      "iteration: 119, weight: [[3.37518578]], train loss: [[0.14590318]]\n",
      "iteration: 120, weight: [[3.37542245]], train loss: [[0.14590045]]\n",
      "iteration: 121, weight: [[3.37564651]], train loss: [[0.14589801]]\n",
      "iteration: 122, weight: [[3.37585862]], train loss: [[0.14589582]]\n",
      "iteration: 123, weight: [[3.37605943]], train loss: [[0.14589386]]\n",
      "iteration: 124, weight: [[3.37624954]], train loss: [[0.1458921]]\n",
      "iteration: 125, weight: [[3.37642951]], train loss: [[0.14589052]]\n",
      "iteration: 126, weight: [[3.3765999]], train loss: [[0.14588911]]\n",
      "iteration: 127, weight: [[3.3767612]], train loss: [[0.14588784]]\n",
      "iteration: 128, weight: [[3.37691391]], train loss: [[0.14588671]]\n",
      "iteration: 129, weight: [[3.37705848]], train loss: [[0.14588569]]\n",
      "iteration: 130, weight: [[3.37719534]], train loss: [[0.14588478]]\n",
      "iteration: 131, weight: [[3.37732491]], train loss: [[0.14588396]]\n",
      "iteration: 132, weight: [[3.37744758]], train loss: [[0.14588323]]\n",
      "iteration: 133, weight: [[3.3775637]], train loss: [[0.14588257]]\n",
      "iteration: 134, weight: [[3.37767364]], train loss: [[0.14588198]]\n",
      "iteration: 135, weight: [[3.37777772]], train loss: [[0.14588146]]\n",
      "iteration: 136, weight: [[3.37787626]], train loss: [[0.14588099]]\n",
      "iteration: 137, weight: [[3.37796954]], train loss: [[0.14588056]]\n",
      "iteration: 138, weight: [[3.37805785]], train loss: [[0.14588018]]\n",
      "iteration: 139, weight: [[3.37814145]], train loss: [[0.14587984]]\n",
      "iteration: 140, weight: [[3.3782206]], train loss: [[0.14587954]]\n",
      "iteration: 141, weight: [[3.37829553]], train loss: [[0.14587926]]\n",
      "iteration: 142, weight: [[3.37836647]], train loss: [[0.14587902]]\n",
      "iteration: 143, weight: [[3.37843362]], train loss: [[0.1458788]]\n",
      "iteration: 144, weight: [[3.3784972]], train loss: [[0.1458786]]\n",
      "iteration: 145, weight: [[3.37855739]], train loss: [[0.14587843]]\n",
      "iteration: 146, weight: [[3.37861437]], train loss: [[0.14587827]]\n",
      "iteration: 147, weight: [[3.37866832]], train loss: [[0.14587813]]\n",
      "iteration: 148, weight: [[3.37871939]], train loss: [[0.145878]]\n",
      "iteration: 149, weight: [[3.37876773]], train loss: [[0.14587789]]\n",
      "iteration: 150, weight: [[3.37881351]], train loss: [[0.14587778]]\n",
      "iteration: 151, weight: [[3.37885684]], train loss: [[0.14587769]]\n",
      "iteration: 152, weight: [[3.37889786]], train loss: [[0.14587761]]\n",
      "iteration: 153, weight: [[3.3789367]], train loss: [[0.14587754]]\n",
      "iteration: 154, weight: [[3.37897346]], train loss: [[0.14587747]]\n",
      "iteration: 155, weight: [[3.37900827]], train loss: [[0.14587741]]\n",
      "iteration: 156, weight: [[3.37904122]], train loss: [[0.14587736]]\n",
      "iteration: 157, weight: [[3.37907242]], train loss: [[0.14587731]]\n",
      "iteration: 158, weight: [[3.37910195]], train loss: [[0.14587727]]\n",
      "iteration: 159, weight: [[3.37912991]], train loss: [[0.14587723]]\n",
      "iteration: 160, weight: [[3.37915638]], train loss: [[0.1458772]]\n",
      "iteration: 161, weight: [[3.37918144]], train loss: [[0.14587717]]\n",
      "iteration: 162, weight: [[3.37920516]], train loss: [[0.14587714]]\n",
      "iteration: 163, weight: [[3.37922762]], train loss: [[0.14587712]]\n",
      "iteration: 164, weight: [[3.37924889]], train loss: [[0.14587709]]\n",
      "iteration: 165, weight: [[3.37926901]], train loss: [[0.14587707]]\n",
      "iteration: 166, weight: [[3.37928807]], train loss: [[0.14587706]]\n",
      "iteration: 167, weight: [[3.37930611]], train loss: [[0.14587704]]\n",
      "iteration: 168, weight: [[3.37932319]], train loss: [[0.14587703]]\n",
      "iteration: 169, weight: [[3.37933936]], train loss: [[0.14587701]]\n",
      "iteration: 170, weight: [[3.37935467]], train loss: [[0.145877]]\n",
      "iteration: 171, weight: [[3.37936916]], train loss: [[0.14587699]]\n",
      "iteration: 172, weight: [[3.37938288]], train loss: [[0.14587698]]\n",
      "iteration: 173, weight: [[3.37939586]], train loss: [[0.14587697]]\n",
      "iteration: 174, weight: [[3.37940816]], train loss: [[0.14587697]]\n",
      "iteration: 175, weight: [[3.3794198]], train loss: [[0.14587696]]\n",
      "iteration: 176, weight: [[3.37943082]], train loss: [[0.14587695]]\n",
      "iteration: 177, weight: [[3.37944125]], train loss: [[0.14587695]]\n",
      "iteration: 178, weight: [[3.37945113]], train loss: [[0.14587694]]\n",
      "iteration: 179, weight: [[3.37946048]], train loss: [[0.14587694]]\n",
      "iteration: 180, weight: [[3.37946933]], train loss: [[0.14587694]]\n",
      "iteration: 181, weight: [[3.37947771]], train loss: [[0.14587693]]\n",
      "iteration: 182, weight: [[3.37948565]], train loss: [[0.14587693]]\n",
      "iteration: 183, weight: [[3.37949316]], train loss: [[0.14587693]]\n",
      "iteration: 184, weight: [[3.37950027]], train loss: [[0.14587692]]\n",
      "iteration: 185, weight: [[3.379507]], train loss: [[0.14587692]]\n",
      "iteration: 186, weight: [[3.37951337]], train loss: [[0.14587692]]\n",
      "iteration: 187, weight: [[3.37951941]], train loss: [[0.14587692]]\n",
      "iteration: 188, weight: [[3.37952512]], train loss: [[0.14587692]]\n",
      "iteration: 189, weight: [[3.37953053]], train loss: [[0.14587692]]\n",
      "iteration: 190, weight: [[3.37953565]], train loss: [[0.14587691]]\n",
      "iteration: 191, weight: [[3.37954049]], train loss: [[0.14587691]]\n",
      "iteration: 192, weight: [[3.37954508]], train loss: [[0.14587691]]\n",
      "iteration: 193, weight: [[3.37954942]], train loss: [[0.14587691]]\n",
      "iteration: 194, weight: [[3.37955354]], train loss: [[0.14587691]]\n",
      "iteration: 195, weight: [[3.37955743]], train loss: [[0.14587691]]\n",
      "iteration: 196, weight: [[3.37956111]], train loss: [[0.14587691]]\n",
      "iteration: 197, weight: [[3.3795646]], train loss: [[0.14587691]]\n",
      "iteration: 198, weight: [[3.37956791]], train loss: [[0.14587691]]\n",
      "iteration: 199, weight: [[3.37957103]], train loss: [[0.14587691]]\n",
      "iteration: 200, weight: [[3.37957399]], train loss: [[0.14587691]]\n",
      "iteration: 201, weight: [[3.3795768]], train loss: [[0.14587691]]\n",
      "iteration: 202, weight: [[3.37957945]], train loss: [[0.14587691]]\n",
      "iteration: 203, weight: [[3.37958196]], train loss: [[0.14587691]]\n",
      "iteration: 204, weight: [[3.37958434]], train loss: [[0.14587691]]\n",
      "iteration: 205, weight: [[3.37958659]], train loss: [[0.14587691]]\n",
      "iteration: 206, weight: [[3.37958872]], train loss: [[0.14587691]]\n",
      "iteration: 207, weight: [[3.37959074]], train loss: [[0.14587691]]\n",
      "iteration: 208, weight: [[3.37959265]], train loss: [[0.1458769]]\n",
      "iteration: 209, weight: [[3.37959446]], train loss: [[0.1458769]]\n",
      "iteration: 210, weight: [[3.37959617]], train loss: [[0.1458769]]\n",
      "iteration: 211, weight: [[3.37959779]], train loss: [[0.1458769]]\n",
      "iteration: 212, weight: [[3.37959933]], train loss: [[0.1458769]]\n",
      "iteration: 213, weight: [[3.37960078]], train loss: [[0.1458769]]\n",
      "iteration: 214, weight: [[3.37960215]], train loss: [[0.1458769]]\n",
      "iteration: 215, weight: [[3.37960346]], train loss: [[0.1458769]]\n",
      "iteration: 216, weight: [[3.37960469]], train loss: [[0.1458769]]\n",
      "iteration: 217, weight: [[3.37960585]], train loss: [[0.1458769]]\n",
      "iteration: 218, weight: [[3.37960696]], train loss: [[0.1458769]]\n",
      "iteration: 219, weight: [[3.37960801]], train loss: [[0.1458769]]\n",
      "iteration: 220, weight: [[3.379609]], train loss: [[0.1458769]]\n",
      "iteration: 221, weight: [[3.37960993]], train loss: [[0.1458769]]\n",
      "iteration: 222, weight: [[3.37961082]], train loss: [[0.1458769]]\n",
      "iteration: 223, weight: [[3.37961166]], train loss: [[0.1458769]]\n",
      "iteration: 224, weight: [[3.37961246]], train loss: [[0.1458769]]\n",
      "iteration: 225, weight: [[3.37961321]], train loss: [[0.1458769]]\n",
      "iteration: 226, weight: [[3.37961392]], train loss: [[0.1458769]]\n",
      "iteration: 227, weight: [[3.3796146]], train loss: [[0.1458769]]\n",
      "iteration: 228, weight: [[3.37961523]], train loss: [[0.1458769]]\n",
      "iteration: 229, weight: [[3.37961584]], train loss: [[0.1458769]]\n",
      "iteration: 230, weight: [[3.37961641]], train loss: [[0.1458769]]\n",
      "iteration: 231, weight: [[3.37961695]], train loss: [[0.1458769]]\n",
      "iteration: 232, weight: [[3.37961747]], train loss: [[0.1458769]]\n",
      "iteration: 233, weight: [[3.37961795]], train loss: [[0.1458769]]\n",
      "iteration: 234, weight: [[3.37961841]], train loss: [[0.1458769]]\n",
      "iteration: 235, weight: [[3.37961885]], train loss: [[0.1458769]]\n",
      "iteration: 236, weight: [[3.37961926]], train loss: [[0.1458769]]\n",
      "iteration: 237, weight: [[3.37961965]], train loss: [[0.1458769]]\n",
      "iteration: 238, weight: [[3.37962002]], train loss: [[0.1458769]]\n",
      "iteration: 239, weight: [[3.37962037]], train loss: [[0.1458769]]\n",
      "iteration: 240, weight: [[3.3796207]], train loss: [[0.1458769]]\n",
      "iteration: 241, weight: [[3.37962101]], train loss: [[0.1458769]]\n",
      "iteration: 242, weight: [[3.37962131]], train loss: [[0.1458769]]\n",
      "iteration: 243, weight: [[3.37962159]], train loss: [[0.1458769]]\n",
      "iteration: 244, weight: [[3.37962186]], train loss: [[0.1458769]]\n",
      "iteration: 245, weight: [[3.37962211]], train loss: [[0.1458769]]\n",
      "iteration: 246, weight: [[3.37962235]], train loss: [[0.1458769]]\n",
      "iteration: 247, weight: [[3.37962257]], train loss: [[0.1458769]]\n",
      "iteration: 248, weight: [[3.37962279]], train loss: [[0.1458769]]\n",
      "iteration: 249, weight: [[3.37962299]], train loss: [[0.1458769]]\n",
      "iteration: 250, weight: [[3.37962318]], train loss: [[0.1458769]]\n",
      "iteration: 251, weight: [[3.37962336]], train loss: [[0.1458769]]\n",
      "iteration: 252, weight: [[3.37962353]], train loss: [[0.1458769]]\n",
      "iteration: 253, weight: [[3.3796237]], train loss: [[0.1458769]]\n",
      "iteration: 254, weight: [[3.37962385]], train loss: [[0.1458769]]\n",
      "iteration: 255, weight: [[3.379624]], train loss: [[0.1458769]]\n",
      "iteration: 256, weight: [[3.37962413]], train loss: [[0.1458769]]\n",
      "iteration: 257, weight: [[3.37962426]], train loss: [[0.1458769]]\n",
      "iteration: 258, weight: [[3.37962439]], train loss: [[0.1458769]]\n",
      "iteration: 259, weight: [[3.3796245]], train loss: [[0.1458769]]\n",
      "iteration: 260, weight: [[3.37962462]], train loss: [[0.1458769]]\n",
      "iteration: 261, weight: [[3.37962472]], train loss: [[0.1458769]]\n",
      "iteration: 262, weight: [[3.37962482]], train loss: [[0.1458769]]\n",
      "iteration: 263, weight: [[3.37962491]], train loss: [[0.1458769]]\n",
      "iteration: 264, weight: [[3.379625]], train loss: [[0.1458769]]\n",
      "iteration: 265, weight: [[3.37962509]], train loss: [[0.1458769]]\n",
      "iteration: 266, weight: [[3.37962517]], train loss: [[0.1458769]]\n",
      "iteration: 267, weight: [[3.37962524]], train loss: [[0.1458769]]\n",
      "iteration: 268, weight: [[3.37962531]], train loss: [[0.1458769]]\n",
      "iteration: 269, weight: [[3.37962538]], train loss: [[0.1458769]]\n",
      "iteration: 270, weight: [[3.37962544]], train loss: [[0.1458769]]\n",
      "iteration: 271, weight: [[3.37962551]], train loss: [[0.1458769]]\n",
      "iteration: 272, weight: [[3.37962556]], train loss: [[0.1458769]]\n",
      "iteration: 273, weight: [[3.37962562]], train loss: [[0.1458769]]\n",
      "iteration: 274, weight: [[3.37962567]], train loss: [[0.1458769]]\n",
      "iteration: 275, weight: [[3.37962572]], train loss: [[0.1458769]]\n",
      "iteration: 276, weight: [[3.37962576]], train loss: [[0.1458769]]\n",
      "iteration: 277, weight: [[3.37962581]], train loss: [[0.1458769]]\n",
      "iteration: 278, weight: [[3.37962585]], train loss: [[0.1458769]]\n",
      "iteration: 279, weight: [[3.37962589]], train loss: [[0.1458769]]\n",
      "iteration: 280, weight: [[3.37962592]], train loss: [[0.1458769]]\n",
      "iteration: 281, weight: [[3.37962596]], train loss: [[0.1458769]]\n",
      "iteration: 282, weight: [[3.37962599]], train loss: [[0.1458769]]\n",
      "iteration: 283, weight: [[3.37962602]], train loss: [[0.1458769]]\n",
      "iteration: 284, weight: [[3.37962605]], train loss: [[0.1458769]]\n",
      "iteration: 285, weight: [[3.37962608]], train loss: [[0.1458769]]\n",
      "iteration: 286, weight: [[3.37962611]], train loss: [[0.1458769]]\n",
      "iteration: 287, weight: [[3.37962613]], train loss: [[0.1458769]]\n",
      "iteration: 288, weight: [[3.37962616]], train loss: [[0.1458769]]\n",
      "iteration: 289, weight: [[3.37962618]], train loss: [[0.1458769]]\n",
      "iteration: 290, weight: [[3.3796262]], train loss: [[0.1458769]]\n",
      "iteration: 291, weight: [[3.37962622]], train loss: [[0.1458769]]\n",
      "iteration: 292, weight: [[3.37962624]], train loss: [[0.1458769]]\n",
      "iteration: 293, weight: [[3.37962626]], train loss: [[0.1458769]]\n",
      "iteration: 294, weight: [[3.37962628]], train loss: [[0.1458769]]\n",
      "iteration: 295, weight: [[3.37962629]], train loss: [[0.1458769]]\n",
      "iteration: 296, weight: [[3.37962631]], train loss: [[0.1458769]]\n",
      "iteration: 297, weight: [[3.37962632]], train loss: [[0.1458769]]\n",
      "iteration: 298, weight: [[3.37962634]], train loss: [[0.1458769]]\n",
      "iteration: 299, weight: [[3.37962635]], train loss: [[0.1458769]]\n",
      "iteration: 300, weight: [[3.37962636]], train loss: [[0.1458769]]\n",
      "iteration: 301, weight: [[3.37962637]], train loss: [[0.1458769]]\n",
      "iteration: 302, weight: [[3.37962638]], train loss: [[0.1458769]]\n",
      "iteration: 303, weight: [[3.3796264]], train loss: [[0.1458769]]\n",
      "iteration: 304, weight: [[3.37962641]], train loss: [[0.1458769]]\n",
      "iteration: 305, weight: [[3.37962641]], train loss: [[0.1458769]]\n",
      "iteration: 306, weight: [[3.37962642]], train loss: [[0.1458769]]\n",
      "iteration: 307, weight: [[3.37962643]], train loss: [[0.1458769]]\n",
      "iteration: 308, weight: [[3.37962644]], train loss: [[0.1458769]]\n",
      "iteration: 309, weight: [[3.37962645]], train loss: [[0.1458769]]\n",
      "iteration: 310, weight: [[3.37962645]], train loss: [[0.1458769]]\n",
      "iteration: 311, weight: [[3.37962646]], train loss: [[0.1458769]]\n",
      "iteration: 312, weight: [[3.37962647]], train loss: [[0.1458769]]\n",
      "iteration: 313, weight: [[3.37962647]], train loss: [[0.1458769]]\n",
      "iteration: 314, weight: [[3.37962648]], train loss: [[0.1458769]]\n",
      "iteration: 315, weight: [[3.37962649]], train loss: [[0.1458769]]\n",
      "iteration: 316, weight: [[3.37962649]], train loss: [[0.1458769]]\n",
      "iteration: 317, weight: [[3.3796265]], train loss: [[0.1458769]]\n",
      "iteration: 318, weight: [[3.3796265]], train loss: [[0.1458769]]\n",
      "iteration: 319, weight: [[3.3796265]], train loss: [[0.1458769]]\n",
      "iteration: 320, weight: [[3.37962651]], train loss: [[0.1458769]]\n",
      "iteration: 321, weight: [[3.37962651]], train loss: [[0.1458769]]\n",
      "iteration: 322, weight: [[3.37962652]], train loss: [[0.1458769]]\n",
      "iteration: 323, weight: [[3.37962652]], train loss: [[0.1458769]]\n",
      "iteration: 324, weight: [[3.37962652]], train loss: [[0.1458769]]\n",
      "iteration: 325, weight: [[3.37962653]], train loss: [[0.1458769]]\n",
      "iteration: 326, weight: [[3.37962653]], train loss: [[0.1458769]]\n",
      "iteration: 327, weight: [[3.37962653]], train loss: [[0.1458769]]\n",
      "iteration: 328, weight: [[3.37962653]], train loss: [[0.1458769]]\n",
      "iteration: 329, weight: [[3.37962654]], train loss: [[0.1458769]]\n",
      "iteration: 330, weight: [[3.37962654]], train loss: [[0.1458769]]\n",
      "iteration: 331, weight: [[3.37962654]], train loss: [[0.1458769]]\n",
      "iteration: 332, weight: [[3.37962654]], train loss: [[0.1458769]]\n",
      "iteration: 333, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 334, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 335, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 336, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 337, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 338, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 339, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 340, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 341, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 342, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 343, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 344, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 345, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 346, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 347, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 348, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 349, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 350, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 351, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 352, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 353, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 354, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 355, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 356, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 357, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 358, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 359, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 360, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 361, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 362, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 363, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 364, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 365, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 366, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 367, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 368, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 369, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 370, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 371, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 372, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 373, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 374, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 375, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 376, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 377, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 378, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 379, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 380, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 381, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 382, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 383, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 384, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 385, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 386, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 387, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 388, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 389, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 390, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 391, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 392, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 393, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 394, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 395, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 396, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 397, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 398, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 399, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 400, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 401, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 402, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 403, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 404, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 405, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 406, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 407, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 408, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 409, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 410, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 411, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 412, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 413, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 414, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 415, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 416, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 417, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 418, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 419, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 420, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 421, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 422, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 423, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 424, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 425, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 426, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 427, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 428, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 429, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 430, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 431, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 432, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 433, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 434, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 435, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 436, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 437, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 438, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 439, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 440, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 441, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 442, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 443, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 444, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 445, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 446, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 447, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 448, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 449, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 450, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 451, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 452, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 453, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 454, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 455, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 456, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 457, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 458, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 459, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 460, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 461, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 462, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 463, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 464, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 465, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 466, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 467, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 468, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 469, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 470, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 471, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 472, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 473, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 474, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 475, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 476, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 477, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 478, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 479, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 480, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 481, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 482, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 483, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 484, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 485, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 486, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 487, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 488, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 489, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 490, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 491, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 492, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 493, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 494, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 495, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 496, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 497, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 498, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 499, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 500, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 501, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 502, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 503, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 504, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 505, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 506, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 507, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 508, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 509, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 510, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 511, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 512, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 513, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 514, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 515, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 516, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 517, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 518, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 519, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 520, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 521, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 522, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 523, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 524, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 525, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 526, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 527, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 528, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 529, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 530, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 531, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 532, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 533, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 534, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 535, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 536, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 537, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 538, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 539, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 540, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 541, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 542, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 543, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 544, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 545, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 546, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 547, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 548, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 549, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 550, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 551, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 552, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 553, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 554, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 555, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 556, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 557, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 558, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 559, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 560, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 561, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 562, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 563, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 564, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 565, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 566, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 567, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 568, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 569, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 570, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 571, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 572, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 573, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 574, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 575, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 576, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 577, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 578, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 579, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 580, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 581, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 582, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 583, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 584, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 585, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 586, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 587, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 588, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 589, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 590, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 591, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 592, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 593, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 594, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 595, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 596, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 597, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 598, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 599, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 600, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 601, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 602, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 603, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 604, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 605, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 606, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 607, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 608, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 609, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 610, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 611, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 612, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 613, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 614, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 615, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 616, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 617, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 618, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 619, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 620, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 621, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 622, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 623, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 624, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 625, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 626, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 627, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 628, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 629, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 630, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 631, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 632, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 633, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 634, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 635, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 636, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 637, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 638, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 639, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 640, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 641, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 642, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 643, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 644, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 645, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 646, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 647, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 648, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 649, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 650, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 651, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 652, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 653, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 654, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 655, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 656, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 657, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 658, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 659, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 660, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 661, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 662, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 663, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 664, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 665, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 666, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 667, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 668, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 669, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 670, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 671, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 672, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 673, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 674, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 675, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 676, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 677, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 678, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 679, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 680, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 681, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 682, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 683, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 684, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 685, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 686, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 687, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 688, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 689, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 690, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 691, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 692, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 693, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 694, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 695, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 696, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 697, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 698, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 699, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 700, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 701, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 702, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 703, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 704, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 705, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 706, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 707, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 708, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 709, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 710, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 711, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 712, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 713, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 714, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 715, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 716, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 717, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 718, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 719, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 720, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 721, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 722, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 723, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 724, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 725, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 726, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 727, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 728, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 729, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 730, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 731, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 732, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 733, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 734, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 735, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 736, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 737, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 738, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 739, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 740, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 741, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 742, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 743, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 744, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 745, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 746, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 747, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 748, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 749, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 750, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 751, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 752, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 753, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 754, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 755, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 756, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 757, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 758, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 759, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 760, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 761, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 762, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 763, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 764, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 765, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 766, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 767, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 768, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 769, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 770, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 771, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 772, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 773, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 774, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 775, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 776, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 777, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 778, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 779, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 780, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 781, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 782, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 783, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 784, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 785, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 786, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 787, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 788, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 789, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 790, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 791, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 792, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 793, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 794, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 795, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 796, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 797, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 798, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 799, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 800, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 801, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 802, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 803, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 804, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 805, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 806, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 807, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 808, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 809, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 810, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 811, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 812, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 813, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 814, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 815, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 816, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 817, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 818, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 819, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 820, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 821, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 822, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 823, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 824, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 825, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 826, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 827, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 828, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 829, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 830, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 831, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 832, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 833, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 834, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 835, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 836, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 837, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 838, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 839, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 840, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 841, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 842, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 843, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 844, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 845, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 846, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 847, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 848, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 849, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 850, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 851, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 852, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 853, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 854, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 855, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 856, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 857, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 858, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 859, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 860, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 861, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 862, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 863, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 864, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 865, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 866, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 867, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 868, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 869, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 870, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 871, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 872, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 873, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 874, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 875, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 876, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 877, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 878, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 879, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 880, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 881, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 882, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 883, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 884, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 885, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 886, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 887, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 888, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 889, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 890, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 891, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 892, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 893, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 894, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 895, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 896, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 897, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 898, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 899, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 900, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 901, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 902, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 903, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 904, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 905, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 906, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 907, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 908, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 909, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 910, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 911, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 912, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 913, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 914, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 915, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 916, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 917, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 918, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 919, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 920, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 921, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 922, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 923, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 924, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 925, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 926, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 927, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 928, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 929, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 930, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 931, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 932, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 933, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 934, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 935, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 936, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 937, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 938, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 939, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 940, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 941, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 942, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 943, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 944, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 945, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 946, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 947, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 948, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 949, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 950, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 951, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 952, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 953, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 954, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 955, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 956, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 957, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 958, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 959, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 960, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 961, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 962, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 963, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 964, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 965, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 966, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 967, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 968, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 969, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 970, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 971, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 972, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 973, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 974, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 975, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 976, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 977, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 978, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 979, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 980, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 981, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 982, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 983, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 984, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 985, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 986, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 987, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 988, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 989, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 990, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 991, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 992, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 993, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 994, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 995, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 996, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 997, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 998, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 999, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 1000, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 1, weight: [[4.37836191]], train loss: [[1.47485734]]\n",
      "iteration: 2, weight: [[3.04769861]], train loss: [[0.29266994]]\n",
      "iteration: 3, weight: [[3.48994227]], train loss: [[0.16209098]]\n",
      "iteration: 4, weight: [[3.34296335]], train loss: [[0.14766784]]\n",
      "iteration: 5, weight: [[3.39181154]], train loss: [[0.14607472]]\n",
      "iteration: 6, weight: [[3.37557693]], train loss: [[0.14589875]]\n",
      "iteration: 7, weight: [[3.38097248]], train loss: [[0.14587932]]\n",
      "iteration: 8, weight: [[3.37917928]], train loss: [[0.14587717]]\n",
      "iteration: 9, weight: [[3.37977524]], train loss: [[0.14587693]]\n",
      "iteration: 10, weight: [[3.37957717]], train loss: [[0.14587691]]\n",
      "iteration: 11, weight: [[3.379643]], train loss: [[0.1458769]]\n",
      "iteration: 12, weight: [[3.37962112]], train loss: [[0.1458769]]\n",
      "iteration: 13, weight: [[3.3796284]], train loss: [[0.1458769]]\n",
      "iteration: 14, weight: [[3.37962598]], train loss: [[0.1458769]]\n",
      "iteration: 15, weight: [[3.37962678]], train loss: [[0.1458769]]\n",
      "iteration: 16, weight: [[3.37962652]], train loss: [[0.1458769]]\n",
      "iteration: 17, weight: [[3.3796266]], train loss: [[0.1458769]]\n",
      "iteration: 18, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 19, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 20, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 21, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 22, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 23, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 24, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 25, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 26, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 27, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 28, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 29, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 30, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 31, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 32, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 33, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 34, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 35, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 36, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 37, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 38, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 39, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 40, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 41, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 42, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 43, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 44, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 45, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 46, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 47, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 48, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 49, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 50, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 51, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 52, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 53, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 54, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 55, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 56, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 57, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 58, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 59, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 60, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 61, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 62, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 63, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 64, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 65, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 66, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 67, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 68, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 69, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 70, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 71, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 72, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 73, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 74, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 75, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 76, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 77, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 78, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 79, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 80, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 81, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 82, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 83, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 84, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 85, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 86, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 87, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 88, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 89, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 90, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 91, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 92, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 93, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 94, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 95, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 96, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 97, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 98, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 99, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 100, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 101, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 102, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 103, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 104, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 105, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 106, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 107, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 108, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 109, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 110, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 111, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 112, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 113, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 114, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 115, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 116, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 117, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 118, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 119, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 120, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 121, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 122, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 123, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 124, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 125, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 126, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 127, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 128, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 129, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 130, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 131, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 132, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 133, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 134, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 135, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 136, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 137, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 138, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 139, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 140, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 141, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 142, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 143, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 144, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 145, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 146, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 147, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 148, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 149, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 150, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 151, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 152, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 153, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 154, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 155, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 156, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 157, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 158, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 159, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 160, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 161, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 162, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 163, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 164, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 165, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 166, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 167, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 168, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 169, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 170, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 171, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 172, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 173, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 174, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 175, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 176, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 177, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 178, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 179, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 180, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 181, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 182, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 183, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 184, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 185, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 186, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 187, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 188, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 189, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 190, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 191, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 192, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 193, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 194, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 195, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 196, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 197, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 198, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 199, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 200, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 201, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 202, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 203, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 204, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 205, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 206, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 207, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 208, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 209, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 210, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 211, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 212, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 213, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 214, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 215, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 216, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 217, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 218, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 219, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 220, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 221, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 222, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 223, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 224, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 225, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 226, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 227, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 228, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 229, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 230, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 231, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 232, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 233, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 234, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 235, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 236, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 237, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 238, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 239, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 240, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 241, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 242, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 243, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 244, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 245, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 246, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 247, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 248, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 249, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 250, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 251, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 252, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 253, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 254, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 255, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 256, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 257, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 258, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 259, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 260, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 261, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 262, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 263, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 264, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 265, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 266, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 267, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 268, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 269, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 270, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 271, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 272, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 273, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 274, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 275, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 276, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 277, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 278, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 279, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 280, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 281, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 282, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 283, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 284, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 285, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 286, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 287, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 288, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 289, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 290, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 291, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 292, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 293, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 294, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 295, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 296, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 297, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 298, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 299, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 300, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 301, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 302, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 303, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 304, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 305, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 306, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 307, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 308, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 309, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 310, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 311, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 312, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 313, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 314, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 315, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 316, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 317, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 318, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 319, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 320, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 321, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 322, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 323, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 324, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 325, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 326, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 327, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 328, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 329, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 330, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 331, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 332, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 333, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 334, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 335, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 336, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 337, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 338, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 339, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 340, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 341, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 342, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 343, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 344, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 345, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 346, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 347, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 348, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 349, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 350, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 351, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 352, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 353, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 354, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 355, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 356, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 357, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 358, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 359, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 360, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 361, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 362, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 363, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 364, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 365, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 366, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 367, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 368, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 369, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 370, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 371, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 372, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 373, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 374, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 375, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 376, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 377, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 378, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 379, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 380, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 381, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 382, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 383, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 384, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 385, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 386, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 387, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 388, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 389, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 390, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 391, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 392, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 393, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 394, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 395, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 396, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 397, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 398, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 399, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 400, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 401, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 402, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 403, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 404, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 405, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 406, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 407, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 408, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 409, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 410, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 411, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 412, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 413, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 414, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 415, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 416, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 417, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 418, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 419, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 420, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 421, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 422, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 423, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 424, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 425, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 426, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 427, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 428, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 429, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 430, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 431, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 432, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 433, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 434, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 435, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 436, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 437, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 438, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 439, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 440, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 441, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 442, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 443, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 444, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 445, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 446, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 447, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 448, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 449, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 450, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 451, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 452, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 453, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 454, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 455, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 456, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 457, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 458, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 459, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 460, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 461, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 462, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 463, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 464, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 465, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 466, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 467, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 468, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 469, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 470, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 471, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 472, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 473, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 474, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 475, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 476, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 477, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 478, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 479, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 480, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 481, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 482, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 483, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 484, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 485, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 486, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 487, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 488, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 489, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 490, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 491, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 492, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 493, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 494, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 495, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 496, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 497, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 498, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 499, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 500, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 501, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 502, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 503, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 504, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 505, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 506, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 507, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 508, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 509, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 510, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 511, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 512, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 513, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 514, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 515, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 516, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 517, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 518, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 519, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 520, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 521, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 522, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 523, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 524, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 525, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 526, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 527, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 528, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 529, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 530, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 531, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 532, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 533, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 534, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 535, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 536, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 537, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 538, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 539, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 540, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 541, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 542, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 543, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 544, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 545, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 546, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 547, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 548, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 549, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 550, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 551, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 552, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 553, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 554, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 555, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 556, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 557, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 558, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 559, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 560, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 561, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 562, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 563, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 564, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 565, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 566, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 567, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 568, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 569, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 570, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 571, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 572, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 573, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 574, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 575, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 576, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 577, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 578, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 579, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 580, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 581, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 582, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 583, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 584, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 585, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 586, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 587, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 588, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 589, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 590, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 591, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 592, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 593, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 594, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 595, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 596, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 597, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 598, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 599, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 600, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 601, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 602, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 603, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 604, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 605, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 606, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 607, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 608, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 609, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 610, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 611, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 612, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 613, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 614, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 615, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 616, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 617, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 618, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 619, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 620, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 621, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 622, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 623, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 624, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 625, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 626, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 627, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 628, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 629, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 630, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 631, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 632, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 633, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 634, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 635, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 636, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 637, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 638, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 639, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 640, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 641, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 642, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 643, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 644, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 645, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 646, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 647, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 648, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 649, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 650, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 651, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 652, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 653, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 654, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 655, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 656, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 657, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 658, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 659, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 660, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 661, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 662, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 663, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 664, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 665, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 666, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 667, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 668, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 669, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 670, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 671, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 672, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 673, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 674, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 675, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 676, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 677, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 678, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 679, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 680, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 681, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 682, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 683, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 684, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 685, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 686, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 687, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 688, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 689, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 690, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 691, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 692, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 693, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 694, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 695, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 696, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 697, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 698, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 699, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 700, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 701, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 702, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 703, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 704, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 705, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 706, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 707, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 708, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 709, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 710, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 711, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 712, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 713, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 714, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 715, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 716, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 717, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 718, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 719, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 720, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 721, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 722, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 723, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 724, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 725, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 726, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 727, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 728, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 729, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 730, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 731, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 732, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 733, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 734, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 735, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 736, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 737, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 738, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 739, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 740, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 741, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 742, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 743, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 744, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 745, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 746, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 747, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 748, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 749, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 750, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 751, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 752, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 753, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 754, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 755, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 756, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 757, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 758, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 759, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 760, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 761, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 762, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 763, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 764, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 765, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 766, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 767, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 768, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 769, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 770, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 771, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 772, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 773, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 774, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 775, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 776, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 777, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 778, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 779, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 780, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 781, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 782, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 783, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 784, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 785, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 786, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 787, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 788, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 789, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 790, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 791, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 792, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 793, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 794, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 795, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 796, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 797, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 798, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 799, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 800, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 801, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 802, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 803, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 804, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 805, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 806, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 807, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 808, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 809, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 810, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 811, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 812, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 813, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 814, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 815, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 816, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 817, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 818, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 819, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 820, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 821, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 822, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 823, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 824, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 825, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 826, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 827, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 828, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 829, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 830, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 831, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 832, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 833, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 834, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 835, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 836, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 837, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 838, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 839, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 840, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 841, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 842, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 843, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 844, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 845, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 846, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 847, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 848, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 849, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 850, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 851, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 852, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 853, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 854, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 855, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 856, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 857, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 858, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 859, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 860, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 861, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 862, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 863, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 864, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 865, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 866, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 867, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 868, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 869, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 870, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 871, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 872, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 873, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 874, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 875, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 876, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 877, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 878, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 879, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 880, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 881, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 882, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 883, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 884, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 885, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 886, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 887, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 888, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 889, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 890, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 891, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 892, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 893, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 894, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 895, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 896, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 897, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 898, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 899, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 900, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 901, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 902, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 903, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 904, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 905, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 906, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 907, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 908, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 909, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 910, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 911, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 912, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 913, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 914, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 915, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 916, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 917, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 918, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 919, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 920, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 921, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 922, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 923, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 924, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 925, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 926, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 927, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 928, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 929, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 930, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 931, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 932, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 933, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 934, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 935, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 936, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 937, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 938, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 939, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 940, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 941, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 942, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 943, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 944, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 945, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 946, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 947, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 948, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 949, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 950, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 951, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 952, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 953, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 954, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 955, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 956, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 957, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 958, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 959, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 960, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 961, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 962, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 963, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 964, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 965, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 966, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 967, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 968, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 969, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 970, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 971, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 972, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 973, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 974, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 975, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 976, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 977, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 978, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 979, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 980, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 981, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 982, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 983, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 984, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 985, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 986, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 987, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 988, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 989, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 990, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 991, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 992, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 993, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 994, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 995, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 996, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 997, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 998, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 999, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 1000, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 1, weight: [[8.38218369]], train loss: [[33.48866231]]\n",
      "iteration: 2, weight: [[-4.94811306]], train loss: [[92.54589237]]\n",
      "iteration: 3, weight: [[17.24278615]], train loss: [[256.20615364]]\n",
      "iteration: 4, weight: [[-19.69832753]], train loss: [[709.74386768]]\n",
      "iteration: 5, weight: [[41.7974175]], train loss: [[1966.59422621]]\n",
      "iteration: 6, weight: [[-60.57433799]], train loss: [[5449.59635557]]\n",
      "iteration: 7, weight: [[109.84357169]], train loss: [[15101.74289053]]\n",
      "iteration: 8, weight: [[-173.8505371]], train loss: [[41849.9127947]]\n",
      "iteration: 9, weight: [[298.41407122]], train loss: [[115974.83315621]]\n",
      "iteration: 10, weight: [[-487.76319986]], train loss: [[321390.89856503]]\n",
      "iteration: 11, weight: [[820.9834025]], train loss: [[890642.90885046]]\n",
      "iteration: 12, weight: [[-1357.68257043]], train loss: [[2468162.42062327]]\n",
      "iteration: 13, weight: [[2269.13519022]], train loss: [[6839807.60598879]]\n",
      "iteration: 14, weight: [[-3768.41587653]], train loss: [[18954574.79847331]]\n",
      "iteration: 15, weight: [[6282.27464314]], train loss: [[52527195.15188564]]\n",
      "iteration: 16, weight: [[-10449.07533723]], train loss: [[1.45564133e+08]]\n",
      "iteration: 17, weight: [[17403.54549692]], train loss: [[4.03389457e+08]]\n",
      "iteration: 18, weight: [[-28962.61672199]], train loss: [[1.11787877e+09]]\n",
      "iteration: 19, weight: [[48222.97423949]], train loss: [[3.09788204e+09]]\n",
      "iteration: 20, weight: [[-80267.61387398]], train loss: [[8.58489609e+09]]\n",
      "iteration: 21, weight: [[133630.22674742]], train loss: [[2.37905898e+10]]\n",
      "iteration: 22, weight: [[-222444.77371948]], train loss: [[6.59288311e+10]]\n",
      "iteration: 23, weight: [[370312.05631286]], train loss: [[1.82702943e+11]]\n",
      "iteration: 24, weight: [[-616448.20231038]], train loss: [[5.06309072e+11]]\n",
      "iteration: 25, weight: [[1026208.21034551]], train loss: [[1.40309112e+12]]\n",
      "iteration: 26, weight: [[-1708316.27669079]], train loss: [[3.88826667e+12]]\n",
      "iteration: 27, weight: [[2843837.24280944]], train loss: [[1.07752216e+13]]\n",
      "iteration: 28, weight: [[-4734117.08282804]], train loss: [[2.98604519e+13]]\n",
      "iteration: 29, weight: [[7880877.45007875]], train loss: [[8.27497218e+13]]\n",
      "iteration: 30, weight: [[-13119260.61183563]], train loss: [[2.29317241e+14]]\n",
      "iteration: 31, weight: [[21839597.0768523]], train loss: [[6.35487299e+14]]\n",
      "iteration: 32, weight: [[-36356293.22133397]], train loss: [[1.76107172e+15]]\n",
      "iteration: 33, weight: [[60522205.43437089]], train loss: [[4.88030774e+15]]\n",
      "iteration: 34, weight: [[-1.00751098e+08]], train loss: [[1.35243803e+16]]\n",
      "iteration: 35, weight: [[1.67720017e+08]], train loss: [[3.74789609e+16]]\n",
      "iteration: 36, weight: [[-2.79202926e+08]], train loss: [[1.03862246e+17]]\n",
      "iteration: 37, weight: [[4.64788161e+08]], train loss: [[2.87824577e+17]]\n",
      "iteration: 38, weight: [[-7.73731246e+08]], train loss: [[7.97623685e+17]]\n",
      "iteration: 39, weight: [[1.28802776e+09]], train loss: [[2.21038644e+18]]\n",
      "iteration: 40, weight: [[-2.14417537e+09]], train loss: [[6.1254553e+18]]\n",
      "iteration: 41, weight: [[3.56940139e+09]], train loss: [[1.69749515e+19]]\n",
      "iteration: 42, weight: [[-5.94197022e+09]], train loss: [[4.70412342e+19]]\n",
      "iteration: 43, weight: [[9.89157741e+09]], train loss: [[1.30361358e+20]]\n",
      "iteration: 44, weight: [[-1.64664749e+10]], train loss: [[3.61259304e+20]]\n",
      "iteration: 45, weight: [[2.74116843e+10]], train loss: [[1.00112708e+21]]\n",
      "iteration: 46, weight: [[-4.56321366e+10]], train loss: [[2.77433803e+21]]\n",
      "iteration: 47, weight: [[7.59636611e+10]], train loss: [[7.68828619e+21]]\n",
      "iteration: 48, weight: [[-1.26456446e+11]], train loss: [[2.13058913e+22]]\n",
      "iteration: 49, weight: [[2.10511611e+11]], train loss: [[5.90431982e+22]]\n",
      "iteration: 50, weight: [[-3.50437955e+11]], train loss: [[1.63621376e+23]]\n",
      "iteration: 51, weight: [[5.8337286e+11]], train loss: [[4.53429955e+23]]\n",
      "iteration: 52, weight: [[-9.71138795e+11]], train loss: [[1.25655173e+24]]\n",
      "iteration: 53, weight: [[1.61665142e+12]], train loss: [[3.48217458e+24]]\n",
      "iteration: 54, weight: [[-2.69123406e+12]], train loss: [[9.64985321e+24]]\n",
      "iteration: 55, weight: [[4.48008809e+12]], train loss: [[2.67418146e+25]]\n",
      "iteration: 56, weight: [[-7.45798726e+12]], train loss: [[7.41073085e+25]]\n",
      "iteration: 57, weight: [[1.24152858e+13]], train loss: [[2.05367259e+26]]\n",
      "iteration: 58, weight: [[-2.06676836e+13]], train loss: [[5.69116758e+26]]\n",
      "iteration: 59, weight: [[3.44054218e+13]], train loss: [[1.5771447e+27]]\n",
      "iteration: 60, weight: [[-5.72745875e+13]], train loss: [[4.3706065e+27]]\n",
      "iteration: 61, weight: [[9.53448091e+13]], train loss: [[1.21118888e+28]]\n",
      "iteration: 62, weight: [[-1.58720176e+14]], train loss: [[3.35646436e+28]]\n",
      "iteration: 63, weight: [[2.64220932e+14]], train loss: [[9.30148321e+28]]\n",
      "iteration: 64, weight: [[-4.39847678e+14]], train loss: [[2.57764066e+29]]\n",
      "iteration: 65, weight: [[7.32212918e+14]], train loss: [[7.14319556e+29]]\n",
      "iteration: 66, weight: [[-1.21891233e+15]], train loss: [[1.97953282e+30]]\n",
      "iteration: 67, weight: [[2.02911917e+15]], train loss: [[5.48571035e+30]]\n",
      "iteration: 68, weight: [[-3.37786771e+15]], train loss: [[1.52020809e+31]]\n",
      "iteration: 69, weight: [[5.62312477e+15]], train loss: [[4.21282294e+31]]\n",
      "iteration: 70, weight: [[-9.36079649e+15]], train loss: [[1.16746367e+32]]\n",
      "iteration: 71, weight: [[1.55828858e+16]], train loss: [[3.23529241e+32]]\n",
      "iteration: 72, weight: [[-2.59407764e+16]], train loss: [[8.96568974e+32]]\n",
      "iteration: 73, weight: [[4.31835213e+16]], train loss: [[2.48458508e+33]]\n",
      "iteration: 74, weight: [[-7.18874596e+16]], train loss: [[6.88531857e+33]]\n",
      "iteration: 75, weight: [[1.19670807e+17]], train loss: [[1.90806957e+34]]\n",
      "iteration: 76, weight: [[-1.99215581e+17]], train loss: [[5.28767036e+34]]\n",
      "iteration: 77, weight: [[3.31633494e+17]], train loss: [[1.46532696e+35]]\n",
      "iteration: 78, weight: [[-5.52069138e+17]], train loss: [[4.06073553e+35]]\n",
      "iteration: 79, weight: [[9.19027597e+17]], train loss: [[1.12531698e+36]]\n",
      "iteration: 80, weight: [[-1.52990208e+18]], train loss: [[3.11849491e+36]]\n",
      "iteration: 81, weight: [[2.54682274e+18]], train loss: [[8.64201878e+36]]\n",
      "iteration: 82, weight: [[-4.23968707e+18]], train loss: [[2.39488891e+37]]\n",
      "iteration: 83, weight: [[7.0577925e+18]], train loss: [[6.63675123e+37]]\n",
      "iteration: 84, weight: [[-1.17490829e+19]], train loss: [[1.83918622e+38]]\n",
      "iteration: 85, weight: [[1.9558658e+19]], train loss: [[5.09677981e+38]]\n",
      "iteration: 86, weight: [[-3.25592308e+19]], train loss: [[1.41242709e+39]]\n",
      "iteration: 87, weight: [[5.42012396e+19]], train loss: [[3.91413865e+39]]\n",
      "iteration: 88, weight: [[-9.02286174e+19]], train loss: [[1.08469184e+40]]\n",
      "iteration: 89, weight: [[1.50203269e+20]], train loss: [[3.00591391e+40]]\n",
      "iteration: 90, weight: [[-2.50042867e+20]], train loss: [[8.33003267e+40]]\n",
      "iteration: 91, weight: [[4.16245501e+20]], train loss: [[2.30843086e+41]]\n",
      "iteration: 92, weight: [[-6.92922456e+20]], train loss: [[6.39715742e+41]]\n",
      "iteration: 93, weight: [[1.15350563e+21]], train loss: [[1.77278964e+42]]\n",
      "iteration: 94, weight: [[-1.92023686e+21]], train loss: [[4.91278062e+42]]\n",
      "iteration: 95, weight: [[3.19661171e+21]], train loss: [[1.36143696e+43]]\n",
      "iteration: 96, weight: [[-5.32138853e+21]], train loss: [[3.77283406e+43]]\n",
      "iteration: 97, weight: [[8.8584972e+21]], train loss: [[1.0455333e+44]]\n",
      "iteration: 98, weight: [[-1.47467099e+22]], train loss: [[2.89739721e+44]]\n",
      "iteration: 99, weight: [[2.45487972e+22]], train loss: [[8.02930961e+44]]\n",
      "iteration: 100, weight: [[-4.08662984e+22]], train loss: [[2.22509404e+45]]\n",
      "iteration: 101, weight: [[6.80299866e+22]], train loss: [[6.16621321e+45]]\n",
      "iteration: 102, weight: [[-1.13249285e+23]], train loss: [[1.70879004e+46]]\n",
      "iteration: 103, weight: [[1.88525696e+23]], train loss: [[4.735424e+46]]\n",
      "iteration: 104, weight: [[-3.13838078e+23]], train loss: [[1.31228764e+47]]\n",
      "iteration: 105, weight: [[5.2244517e+23]], train loss: [[3.63663073e+47]]\n",
      "iteration: 106, weight: [[-8.6971268e+23]], train loss: [[1.00778844e+48]]\n",
      "iteration: 107, weight: [[1.44780771e+24]], train loss: [[2.79279808e+48]]\n",
      "iteration: 108, weight: [[-2.41016052e+24]], train loss: [[7.73944296e+48]]\n",
      "iteration: 109, weight: [[4.01218594e+24]], train loss: [[2.14476577e+49]]\n",
      "iteration: 110, weight: [[-6.67907216e+24]], train loss: [[5.94360634e+49]]\n",
      "iteration: 111, weight: [[1.11186285e+25]], train loss: [[1.6471009e+50]]\n",
      "iteration: 112, weight: [[-1.85091426e+25]], train loss: [[4.56447014e+50]]\n",
      "iteration: 113, weight: [[3.08121061e+25]], train loss: [[1.26491265e+51]]\n",
      "iteration: 114, weight: [[-5.12928072e+25]], train loss: [[3.50534448e+51]]\n",
      "iteration: 115, weight: [[8.538696e+25]], train loss: [[9.71406198e+51]]\n",
      "iteration: 116, weight: [[-1.42143379e+26]], train loss: [[2.69197509e+52]]\n",
      "iteration: 117, weight: [[2.36625595e+26]], train loss: [[7.46004081e+52]]\n",
      "iteration: 118, weight: [[-3.93909815e+26]], train loss: [[2.06733744e+53]]\n",
      "iteration: 119, weight: [[6.55740315e+26]], train loss: [[5.7290358e+53]]\n",
      "iteration: 120, weight: [[-1.09160865e+27]], train loss: [[1.5876388e+54]]\n",
      "iteration: 121, weight: [[1.81719717e+27]], train loss: [[4.3996879e+54]]\n",
      "iteration: 122, weight: [[-3.02508189e+27]], train loss: [[1.21924796e+55]]\n",
      "iteration: 123, weight: [[5.03584342e+27]], train loss: [[3.3787978e+55]]\n",
      "iteration: 124, weight: [[-8.38315124e+27]], train loss: [[9.36337397e+55]]\n",
      "iteration: 125, weight: [[1.39554031e+28]], train loss: [[2.59479191e+56]]\n",
      "iteration: 126, weight: [[-2.32315116e+28]], train loss: [[7.19072537e+56]]\n",
      "iteration: 127, weight: [[3.86734175e+28]], train loss: [[1.99270435e+57]]\n",
      "iteration: 128, weight: [[-6.43795052e+28]], train loss: [[5.5222115e+57]]\n",
      "iteration: 129, weight: [[1.07172341e+29]], train loss: [[1.53032334e+58]]\n",
      "iteration: 130, weight: [[-1.78409428e+29]], train loss: [[4.24085447e+58]]\n",
      "iteration: 131, weight: [[2.96997562e+29]], train loss: [[1.1752318e+59]]\n",
      "iteration: 132, weight: [[-4.94410822e+29]], train loss: [[3.25681959e+59]]\n",
      "iteration: 133, weight: [[8.23043996e+29]], train loss: [[9.02534618e+59]]\n",
      "iteration: 134, weight: [[-1.37011851e+30]], train loss: [[2.50111716e+60]]\n",
      "iteration: 135, weight: [[2.28083158e+30]], train loss: [[6.93113251e+60]]\n",
      "iteration: 136, weight: [[-3.7968925e+30]], train loss: [[1.9207656e+61]]\n",
      "iteration: 137, weight: [[6.3206739e+30]], train loss: [[5.32285377e+61]]\n",
      "iteration: 138, weight: [[-1.05220041e+31]], train loss: [[1.47507704e+62]]\n",
      "iteration: 139, weight: [[1.75159441e+31]], train loss: [[4.08775509e+62]]\n",
      "iteration: 140, weight: [[-2.9158732e+31]], train loss: [[1.13280468e+63]]\n",
      "iteration: 141, weight: [[4.8540441e+31]], train loss: [[3.13924493e+63]]\n",
      "iteration: 142, weight: [[-8.08051054e+31]], train loss: [[8.69952155e+63]]\n",
      "iteration: 143, weight: [[1.34515981e+32]], train loss: [[2.41082416e+64]]\n",
      "iteration: 144, weight: [[-2.23928292e+32]], train loss: [[6.68091123e+64]]\n",
      "iteration: 145, weight: [[3.72772658e+32]], train loss: [[1.85142391e+65]]\n",
      "iteration: 146, weight: [[-6.20553364e+32]], train loss: [[5.13069307e+65]]\n",
      "iteration: 147, weight: [[1.03303305e+33]], train loss: [[1.42182518e+66]]\n",
      "iteration: 148, weight: [[-1.71968657e+33]], train loss: [[3.94018277e+66]]\n",
      "iteration: 149, weight: [[2.86275633e+33]], train loss: [[1.09190922e+67]]\n",
      "iteration: 150, weight: [[-4.76562063e+33]], train loss: [[3.02591484e+67]]\n",
      "iteration: 151, weight: [[7.9333123e+33]], train loss: [[8.38545954e+67]]\n",
      "iteration: 152, weight: [[-1.32065577e+34]], train loss: [[2.32379083e+68]]\n",
      "iteration: 153, weight: [[2.19849113e+34]], train loss: [[6.43972321e+68]]\n",
      "iteration: 154, weight: [[-3.65982063e+34]], train loss: [[1.78458553e+69]]\n",
      "iteration: 155, weight: [[6.09249083e+34]], train loss: [[4.94546957e+69]]\n",
      "iteration: 156, weight: [[-1.01421485e+35]], train loss: [[1.37049577e+70]]\n",
      "iteration: 157, weight: [[1.68835998e+35]], train loss: [[3.79793797e+70]]\n",
      "iteration: 158, weight: [[-2.81060707e+35]], train loss: [[1.05249014e+71]]\n",
      "iteration: 159, weight: [[4.67880793e+35]], train loss: [[2.91667608e+71]]\n",
      "iteration: 160, weight: [[-7.78879548e+35]], train loss: [[8.0827355e+71]]\n",
      "iteration: 161, weight: [[1.29659811e+36]], train loss: [[2.2398995e+72]]\n",
      "iteration: 162, weight: [[-2.15844242e+36]], train loss: [[6.20724233e+72]]\n",
      "iteration: 163, weight: [[3.59315168e+36]], train loss: [[1.72016009e+73]]\n",
      "iteration: 164, weight: [[-5.98150725e+36]], train loss: [[4.76693284e+73]]\n",
      "iteration: 165, weight: [[9.95739457e+36]], train loss: [[1.32101941e+74]]\n",
      "iteration: 166, weight: [[-1.65760405e+37]], train loss: [[3.66082836e+74]]\n",
      "iteration: 167, weight: [[2.75940777e+37]], train loss: [[1.01449412e+75]]\n",
      "iteration: 168, weight: [[-4.59357664e+37]], train loss: [[2.81138096e+75]]\n",
      "iteration: 169, weight: [[7.64691126e+37]], train loss: [[7.79094012e+75]]\n",
      "iteration: 170, weight: [[-1.27297869e+38]], train loss: [[2.15903674e+76]]\n",
      "iteration: 171, weight: [[2.11912325e+38]], train loss: [[5.98315425e+76]]\n",
      "iteration: 172, weight: [[-3.5276972e+38]], train loss: [[1.65806047e+77]]\n",
      "iteration: 173, weight: [[5.8725454e+38]], train loss: [[4.59484148e+77]]\n",
      "iteration: 174, weight: [[-9.77600616e+38]], train loss: [[1.2733292e+78]]\n",
      "iteration: 175, weight: [[1.62740839e+39]], train loss: [[3.52866855e+78]]\n",
      "iteration: 176, weight: [[-2.70914115e+39]], train loss: [[9.77869798e+78]]\n",
      "iteration: 177, weight: [[4.50989797e+39]], train loss: [[2.70988711e+79]]\n",
      "iteration: 178, weight: [[-7.50761165e+39]], train loss: [[7.50967887e+79]]\n",
      "iteration: 179, weight: [[1.24978953e+40]], train loss: [[2.08109321e+80]]\n",
      "iteration: 180, weight: [[-2.08052034e+40]], train loss: [[5.767156e+80]]\n",
      "iteration: 181, weight: [[3.46343507e+40]], train loss: [[1.59820272e+81]]\n",
      "iteration: 182, weight: [[-5.76556846e+40]], train loss: [[4.4289628e+81]]\n",
      "iteration: 183, weight: [[9.597922e+40]], train loss: [[1.22736066e+82]]\n",
      "iteration: 184, weight: [[-1.59776278e+41]], train loss: [[3.40127985e+82]]\n",
      "iteration: 185, weight: [[2.65979021e+41]], train loss: [[9.42567654e+82]]\n",
      "iteration: 186, weight: [[-4.42774362e+41]], train loss: [[2.61205729e+83]]\n",
      "iteration: 187, weight: [[7.3708496e+41]], train loss: [[7.23857145e+83]]\n",
      "iteration: 188, weight: [[-1.2270228e+42]], train loss: [[2.00596353e+84]]\n",
      "iteration: 189, weight: [[2.04262064e+42]], train loss: [[5.55895551e+84]]\n",
      "iteration: 190, weight: [[-3.40034356e+42]], train loss: [[1.5405059e+85]]\n",
      "iteration: 191, weight: [[5.66054025e+42]], train loss: [[4.26907251e+85]]\n",
      "iteration: 192, weight: [[-9.42308191e+42]], train loss: [[1.18305163e+86]]\n",
      "iteration: 193, weight: [[1.56865721e+43]], train loss: [[3.27849001e+86]]\n",
      "iteration: 194, weight: [[-2.61133826e+43]], train loss: [[9.08539956e+86]]\n",
      "iteration: 195, weight: [[4.34708584e+43]], train loss: [[2.51775924e+87]]\n",
      "iteration: 196, weight: [[-7.23657886e+43]], train loss: [[6.97725129e+87]]\n",
      "iteration: 197, weight: [[1.2046708e+44]], train loss: [[1.9335461e+88]]\n",
      "iteration: 198, weight: [[-2.00541134e+44]], train loss: [[5.35827129e+88]]\n",
      "iteration: 199, weight: [[3.33840136e+44]], train loss: [[1.48489199e+89]]\n",
      "iteration: 200, weight: [[-5.55742528e+44]], train loss: [[4.11495444e+89]]\n",
      "iteration: 201, weight: [[9.25142677e+44]], train loss: [[1.1403422e+90]]\n",
      "iteration: 202, weight: [[-1.54008184e+45]], train loss: [[3.16013302e+90]]\n",
      "iteration: 203, weight: [[2.56376894e+45]], train loss: [[8.75740694e+90]]\n",
      "iteration: 204, weight: [[-4.26789735e+45]], train loss: [[2.42686544e+91]]\n",
      "iteration: 205, weight: [[7.10475406e+45]], train loss: [[6.72536507e+91]]\n",
      "iteration: 206, weight: [[-1.18272597e+46]], train loss: [[1.86374302e+92]]\n",
      "iteration: 207, weight: [[1.96887986e+46]], train loss: [[5.16483198e+92]]\n",
      "iteration: 208, weight: [[-3.27758753e+46]], train loss: [[1.4312858e+93]]\n",
      "iteration: 209, weight: [[5.4561887e+46]], train loss: [[3.96640019e+93]]\n",
      "iteration: 210, weight: [[-9.08289859e+46]], train loss: [[1.09917463e+94]]\n",
      "iteration: 211, weight: [[1.51202701e+47]], train loss: [[3.04604885e+94]]\n",
      "iteration: 212, weight: [[-2.51706617e+47]], train loss: [[8.4412552e+94]]\n",
      "iteration: 213, weight: [[4.1901514e+47]], train loss: [[2.33925301e+95]]\n",
      "iteration: 214, weight: [[-6.97533064e+47]], train loss: [[6.48257222e+95]]\n",
      "iteration: 215, weight: [[1.1611809e+48]], train loss: [[1.79645991e+96]]\n",
      "iteration: 216, weight: [[-1.93301385e+48]], train loss: [[4.97837603e+96]]\n",
      "iteration: 217, weight: [[3.21788151e+48]], train loss: [[1.37961486e+97]]\n",
      "iteration: 218, weight: [[-5.3567963e+48]], train loss: [[3.82320891e+97]]\n",
      "iteration: 219, weight: [[8.91744039e+48]], train loss: [[1.05949326e+98]]\n",
      "iteration: 220, weight: [[-1.48448324e+49]], train loss: [[2.93608324e+98]]\n",
      "iteration: 221, weight: [[2.47121415e+49]], train loss: [[8.13651688e+98]]\n",
      "iteration: 222, weight: [[-4.1138217e+49]], train loss: [[2.25480348e+99]]\n",
      "iteration: 223, weight: [[6.84826486e+49]], train loss: [[6.24854444e+99]]\n",
      "iteration: 224, weight: [[-1.1400283e+50]], train loss: [[1.73160579e+100]]\n",
      "iteration: 225, weight: [[1.89780119e+50]], train loss: [[4.79865134e+100]]\n",
      "iteration: 226, weight: [[-3.15926312e+50]], train loss: [[1.32980929e+101]]\n",
      "iteration: 227, weight: [[5.25921447e+50]], train loss: [[3.68518699e+101]]\n",
      "iteration: 228, weight: [[-8.75499626e+50]], train loss: [[1.02124442e+102]]\n",
      "iteration: 229, weight: [[1.45744122e+51]], train loss: [[2.8300875e+102]]\n",
      "iteration: 230, weight: [[-2.42619739e+51]], train loss: [[7.84277994e+102]]\n",
      "iteration: 231, weight: [[4.03888246e+51]], train loss: [[2.17340267e+103]]\n",
      "iteration: 232, weight: [[-6.72351376e+51]], train loss: [[6.02296532e+103]]\n",
      "iteration: 233, weight: [[1.11926103e+52]], train loss: [[1.66909297e+104]]\n",
      "iteration: 234, weight: [[-1.86322998e+52]], train loss: [[4.62541491e+104]]\n",
      "iteration: 235, weight: [[3.10171256e+52]], train loss: [[1.28180176e+105]]\n",
      "iteration: 236, weight: [[-5.16341023e+52]], train loss: [[3.55214781e+105]]\n",
      "iteration: 237, weight: [[8.59551128e+52]], train loss: [[9.84376405e+105]]\n",
      "iteration: 238, weight: [[-1.43089181e+53]], train loss: [[2.72791832e+106]]\n",
      "iteration: 239, weight: [[2.38200068e+53]], train loss: [[7.55964721e+106]]\n",
      "iteration: 240, weight: [[-3.96530835e+53]], train loss: [[2.09494051e+107]]\n",
      "iteration: 241, weight: [[6.60103519e+53]], train loss: [[5.80552984e+107]]\n",
      "iteration: 242, weight: [[-1.09887206e+54]], train loss: [[1.60883694e+108]]\n",
      "iteration: 243, weight: [[1.82928854e+54]], train loss: [[4.45843249e+108]]\n",
      "iteration: 244, weight: [[-3.04521035e+54]], train loss: [[1.23552734e+109]]\n",
      "iteration: 245, weight: [[5.06935121e+54]], train loss: [[3.42391148e+109]]\n",
      "iteration: 246, weight: [[-8.43893155e+54]], train loss: [[9.48839367e+109]]\n",
      "iteration: 247, weight: [[1.40482604e+55]], train loss: [[2.62943756e+110]]\n",
      "iteration: 248, weight: [[-2.33860908e+55]], train loss: [[7.28673588e+110]]\n",
      "iteration: 249, weight: [[3.8930745e+55]], train loss: [[2.01931092e+111]]\n",
      "iteration: 250, weight: [[-6.48078774e+55]], train loss: [[5.59594402e+111]]\n",
      "iteration: 251, weight: [[1.07885451e+56]], train loss: [[1.55075621e+112]]\n",
      "iteration: 252, weight: [[-1.79596539e+56]], train loss: [[4.29747832e+112]]\n",
      "iteration: 253, weight: [[2.98973742e+56]], train loss: [[1.19092349e+113]]\n",
      "iteration: 254, weight: [[-4.97700561e+56]], train loss: [[3.30030462e+113]]\n",
      "iteration: 255, weight: [[8.28520415e+56]], train loss: [[9.14585253e+113]]\n",
      "iteration: 256, weight: [[-1.37923509e+57]], train loss: [[2.53451205e+114]]\n",
      "iteration: 257, weight: [[2.29600791e+57]], train loss: [[7.02367694e+114]]\n",
      "iteration: 258, weight: [[-3.82215649e+57]], train loss: [[1.94641164e+115]]\n",
      "iteration: 259, weight: [[6.36273077e+57]], train loss: [[5.39392447e+115]]\n",
      "iteration: 260, weight: [[-1.05920161e+58]], train loss: [[1.49477225e+116]]\n",
      "iteration: 261, weight: [[1.76324927e+58]], train loss: [[4.14233477e+116]]\n",
      "iteration: 262, weight: [[-2.93527501e+58]], train loss: [[1.14792988e+117]]\n",
      "iteration: 263, weight: [[4.88634223e+58]], train loss: [[3.1811601e+117]]\n",
      "iteration: 264, weight: [[-8.13427712e+58]], train loss: [[8.81567749e+117]]\n",
      "iteration: 265, weight: [[1.35411032e+59]], train loss: [[2.44301346e+118]]\n",
      "iteration: 266, weight: [[-2.25418279e+59]], train loss: [[6.77011471e+118]]\n",
      "iteration: 267, weight: [[3.75253035e+59]], train loss: [[1.87614411e+119]]\n",
      "iteration: 268, weight: [[-6.24682438e+59]], train loss: [[5.19919804e+119]]\n",
      "iteration: 269, weight: [[1.03990671e+60]], train loss: [[1.44080938e+120]]\n",
      "iteration: 270, weight: [[-1.73112913e+60]], train loss: [[3.99279206e+120]]\n",
      "iteration: 271, weight: [[2.88180471e+60]], train loss: [[1.10648839e+121]]\n",
      "iteration: 272, weight: [[-4.7973304e+60]], train loss: [[3.06631683e+121]]\n",
      "iteration: 273, weight: [[7.98609944e+60]], train loss: [[8.49742213e+121]]\n",
      "iteration: 274, weight: [[-1.32944323e+61]], train loss: [[2.35481807e+122]]\n",
      "iteration: 275, weight: [[2.21311958e+61]], train loss: [[6.52570634e+122]]\n",
      "iteration: 276, weight: [[-3.68417256e+61]], train loss: [[1.8084133e+123]]\n",
      "iteration: 277, weight: [[6.1330294e+61]], train loss: [[5.01150144e+123]]\n",
      "iteration: 278, weight: [[-1.0209633e+62]], train loss: [[1.38879462e+124]]\n",
      "iteration: 279, weight: [[1.69959409e+62]], train loss: [[3.84864801e+124]]\n",
      "iteration: 280, weight: [[-2.82930845e+62]], train loss: [[1.06654297e+125]]\n",
      "iteration: 281, weight: [[4.70994006e+62]], train loss: [[2.95561952e+125]]\n",
      "iteration: 282, weight: [[-7.84062103e+62]], train loss: [[8.19065612e+125]]\n",
      "iteration: 283, weight: [[1.30522549e+63]], train loss: [[2.26980662e+126]]\n",
      "iteration: 284, weight: [[-2.17280439e+63]], train loss: [[6.29012138e+126]]\n",
      "iteration: 285, weight: [[3.61706e+63]], train loss: [[1.74312765e+127]]\n",
      "iteration: 286, weight: [[-6.02130736e+63]], train loss: [[4.83058089e+127]]\n",
      "iteration: 287, weight: [[1.00236497e+64]], train loss: [[1.33865765e+128]]\n",
      "iteration: 288, weight: [[-1.66863352e+64]], train loss: [[3.70970771e+128]]\n",
      "iteration: 289, weight: [[2.77776848e+64]], train loss: [[1.02803964e+129]]\n",
      "iteration: 290, weight: [[-4.62414165e+64]], train loss: [[2.8489185e+129]]\n",
      "iteration: 291, weight: [[7.69779272e+64]], train loss: [[7.89496469e+129]]\n",
      "iteration: 292, weight: [[-1.28144891e+65]], train loss: [[2.18786418e+130]]\n",
      "iteration: 293, weight: [[2.1332236e+65]], train loss: [[6.06304128e+130]]\n",
      "iteration: 294, weight: [[-3.55117e+65]], train loss: [[1.68019888e+131]]\n",
      "iteration: 295, weight: [[5.91162049e+65]], train loss: [[4.65619177e+131]]\n",
      "iteration: 296, weight: [[-9.84105433e+65]], train loss: [[1.29033068e+132]]\n",
      "iteration: 297, weight: [[1.63823693e+66]], train loss: [[3.5757833e+132]]\n",
      "iteration: 298, weight: [[-2.72716739e+66]], train loss: [[9.90926307e+132]]\n",
      "iteration: 299, weight: [[4.53990619e+66]], train loss: [[2.7460695e+133]]\n",
      "iteration: 300, weight: [[-7.55756624e+66]], train loss: [[7.60994804e+133]]\n",
      "iteration: 301, weight: [[1.25810546e+67]], train loss: [[2.10887995e+134]]\n",
      "iteration: 302, weight: [[-2.09436383e+67]], train loss: [[5.84415902e+134]]\n",
      "iteration: 303, weight: [[3.48648027e+67]], train loss: [[1.61954191e+135]]\n",
      "iteration: 304, weight: [[-5.80393174e+67]], train loss: [[4.48809827e+135]]\n",
      "iteration: 305, weight: [[9.66178522e+67]], train loss: [[1.24374837e+136]]\n",
      "iteration: 306, weight: [[-1.60839407e+68]], train loss: [[3.44669371e+136]]\n",
      "iteration: 307, weight: [[2.67748808e+68]], train loss: [[9.5515281e+136]]\n",
      "iteration: 308, weight: [[-4.4572052e+68]], train loss: [[2.64693346e+137]]\n",
      "iteration: 309, weight: [[7.41989419e+68]], train loss: [[7.33522079e+137]]\n",
      "iteration: 310, weight: [[-1.23518724e+69]], train loss: [[2.03274713e+138]]\n",
      "iteration: 311, weight: [[2.05621195e+69]], train loss: [[5.63317864e+138]]\n",
      "iteration: 312, weight: [[-3.42296897e+69]], train loss: [[1.56107472e+139]]\n",
      "iteration: 313, weight: [[5.69820469e+69]], train loss: [[4.32607314e+139]]\n",
      "iteration: 314, weight: [[-9.48578176e+69]], train loss: [[1.19884773e+140]]\n",
      "iteration: 315, weight: [[1.57909483e+70]], train loss: [[3.32226438e+140]]\n",
      "iteration: 316, weight: [[-2.62871374e+70]], train loss: [[9.20670774e+140]]\n",
      "iteration: 317, weight: [[4.37601073e+70]], train loss: [[2.55137634e+141]]\n",
      "iteration: 318, weight: [[-7.28473004e+70]], train loss: [[7.0704115e+141]]\n",
      "iteration: 319, weight: [[1.21268651e+71]], train loss: [[1.95936279e+142]]\n",
      "iteration: 320, weight: [[-2.01875506e+71]], train loss: [[5.42981488e+142]]\n",
      "iteration: 321, weight: [[3.36061462e+71]], train loss: [[1.50471826e+143]]\n",
      "iteration: 322, weight: [[-5.59440361e+71]], train loss: [[4.16989728e+143]]\n",
      "iteration: 323, weight: [[9.31298446e+71]], train loss: [[1.15556804e+144]]\n",
      "iteration: 324, weight: [[-1.55032932e+72]], train loss: [[3.20232709e+144]]\n",
      "iteration: 325, weight: [[2.5808279e+72]], train loss: [[8.87433577e+144]]\n",
      "iteration: 326, weight: [[-4.29629534e+72]], train loss: [[2.45926893e+145]]\n",
      "iteration: 327, weight: [[7.15202809e+72]], train loss: [[6.81516209e+145]]\n",
      "iteration: 328, weight: [[-1.19059566e+73]], train loss: [[1.88862771e+146]]\n",
      "iteration: 329, weight: [[1.9819805e+73]], train loss: [[5.23379277e+146]]\n",
      "iteration: 330, weight: [[-3.29939614e+73]], train loss: [[1.45039632e+147]]\n",
      "iteration: 331, weight: [[5.49249341e+73]], train loss: [[4.01935954e+147]]\n",
      "iteration: 332, weight: [[-9.14333492e+73]], train loss: [[1.1138508e+148]]\n",
      "iteration: 333, weight: [[1.52208782e+74]], train loss: [[3.08671967e+148]]\n",
      "iteration: 334, weight: [[-2.53381437e+74]], train loss: [[8.55396277e+148]]\n",
      "iteration: 335, weight: [[4.21803208e+74]], train loss: [[2.3704867e+149]]\n",
      "iteration: 336, weight: [[-7.02174351e+74]], train loss: [[6.56912747e+149]]\n",
      "iteration: 337, weight: [[1.16890723e+75]], train loss: [[1.82044623e+150]]\n",
      "iteration: 338, weight: [[-1.94587585e+75]], train loss: [[5.04484727e+150]]\n",
      "iteration: 339, weight: [[3.23929284e+75]], train loss: [[1.39803547e+151]]\n",
      "iteration: 340, weight: [[-5.39243967e+75]], train loss: [[3.87425637e+151]]\n",
      "iteration: 341, weight: [[8.97677579e+75]], train loss: [[1.0736396e+152]]\n",
      "iteration: 342, weight: [[-1.49436078e+76]], train loss: [[2.97528579e+152]]\n",
      "iteration: 343, weight: [[2.48765726e+76]], train loss: [[8.24515559e+152]]\n",
      "iteration: 344, weight: [[-4.14119449e+76]], train loss: [[2.2849096e+153]]\n",
      "iteration: 345, weight: [[6.89383225e+76]], train loss: [[6.33197496e+153]]\n",
      "iteration: 346, weight: [[-1.14761389e+77]], train loss: [[1.75472618e+154]]\n",
      "iteration: 347, weight: [[1.91042889e+77]], train loss: [[4.8627229e+154]]\n",
      "iteration: 348, weight: [[-3.18028442e+77]], train loss: [[1.34756489e+155]]\n",
      "iteration: 349, weight: [[5.29420854e+77]], train loss: [[3.73439158e+155]]\n",
      "iteration: 350, weight: [[-8.81325077e+77]], train loss: [[1.03488007e+156]]\n",
      "iteration: 351, weight: [[1.46713883e+78]], train loss: [[2.8678748e+156]]\n",
      "iteration: 352, weight: [[-2.44234097e+78]], train loss: [[7.94749667e+156]]\n",
      "iteration: 353, weight: [[4.06575662e+78]], train loss: [[2.20242192e+157]]\n",
      "iteration: 354, weight: [[-6.76825107e+78]], train loss: [[6.1033839e+157]]\n",
      "iteration: 355, weight: [[1.12670843e+79]], train loss: [[1.69137869e+158]]\n",
      "iteration: 356, weight: [[-1.87562765e+79]], train loss: [[4.68717341e+158]]\n",
      "iteration: 357, weight: [[3.12235091e+79]], train loss: [[1.29891636e+159]]\n",
      "iteration: 358, weight: [[-5.19776684e+79]], train loss: [[3.59957606e+159]]\n",
      "iteration: 359, weight: [[8.6527046e+79]], train loss: [[9.97519791e+159]]\n",
      "iteration: 360, weight: [[-1.44041276e+80]], train loss: [[2.76434146e+160]]\n",
      "iteration: 361, weight: [[2.39785018e+80]], train loss: [[7.66058356e+160]]\n",
      "iteration: 362, weight: [[-3.99169295e+80]], train loss: [[2.12291214e+161]]\n",
      "iteration: 363, weight: [[6.64495754e+80]], train loss: [[5.88304523e+161]]\n",
      "iteration: 364, weight: [[-1.1061838e+81]], train loss: [[1.63031811e+162]]\n",
      "iteration: 365, weight: [[1.84146037e+81]], train loss: [[4.51796145e+162]]\n",
      "iteration: 366, weight: [[-3.06547275e+81]], train loss: [[1.2520241e+163]]\n",
      "iteration: 367, weight: [[5.10308196e+81]], train loss: [[3.46962752e+163]]\n",
      "iteration: 368, weight: [[-8.49508301e+81]], train loss: [[9.61508262e+163]]\n",
      "iteration: 369, weight: [[1.41417355e+82]], train loss: [[2.66454578e+164]]\n",
      "iteration: 370, weight: [[-2.35416985e+82]], train loss: [[7.38402832e+164]]\n",
      "iteration: 371, weight: [[3.91897846e+82]], train loss: [[2.04627275e+165]]\n",
      "iteration: 372, weight: [[-6.52390999e+82]], train loss: [[5.67066102e+165]]\n",
      "iteration: 373, weight: [[1.08603305e+83]], train loss: [[1.57146189e+166]]\n",
      "iteration: 374, weight: [[-1.80791549e+83]], train loss: [[4.35485822e+166]]\n",
      "iteration: 375, weight: [[3.00963071e+83]], train loss: [[1.20682469e+167]]\n",
      "iteration: 376, weight: [[-5.01012191e+83]], train loss: [[3.34437026e+167]]\n",
      "iteration: 377, weight: [[8.34033273e+83]], train loss: [[9.26796788e+167]]\n",
      "iteration: 378, weight: [[-1.38841232e+84]], train loss: [[2.56835284e+168]]\n",
      "iteration: 379, weight: [[2.31128523e+84]], train loss: [[7.11745702e+168]]\n",
      "iteration: 380, weight: [[-3.84758858e+84]], train loss: [[1.97240012e+169]]\n",
      "iteration: 381, weight: [[6.40506749e+84]], train loss: [[5.46594411e+169]]\n",
      "iteration: 382, weight: [[-1.06624938e+85]], train loss: [[1.51473044e+170]]\n",
      "iteration: 383, weight: [[1.77498169e+85]], train loss: [[4.19764319e+170]]\n",
      "iteration: 384, weight: [[-2.95480592e+85]], train loss: [[1.16325703e+171]]\n",
      "iteration: 385, weight: [[4.91885526e+85]], train loss: [[3.22363493e+171]]\n",
      "iteration: 386, weight: [[-8.18840145e+85]], train loss: [[8.93338435e+171]]\n",
      "iteration: 387, weight: [[1.36312038e+86]], train loss: [[2.47563256e+172]]\n",
      "iteration: 388, weight: [[-2.26918181e+86]], train loss: [[6.86050922e+172]]\n",
      "iteration: 389, weight: [[3.77749916e+86]], train loss: [[1.90119437e+173]]\n",
      "iteration: 390, weight: [[-6.28838987e+86]], train loss: [[5.26861769e+173]]\n",
      "iteration: 391, weight: [[1.0468261e+87]], train loss: [[1.46004705e+174]]\n",
      "iteration: 392, weight: [[-1.74264782e+87]], train loss: [[4.04610379e+174]]\n",
      "iteration: 393, weight: [[2.90097983e+87]], train loss: [[1.12126221e+175]]\n",
      "iteration: 394, weight: [[-4.82925116e+87]], train loss: [[3.10725827e+175]]\n",
      "iteration: 395, weight: [[8.03923782e+87]], train loss: [[8.61087964e+175]]\n",
      "iteration: 396, weight: [[-1.33828916e+88]], train loss: [[2.38625958e+176]]\n",
      "iteration: 397, weight: [[2.22784536e+88]], train loss: [[6.61283752e+176]]\n",
      "iteration: 398, weight: [[-3.70868652e+88]], train loss: [[1.83255922e+177]]\n",
      "iteration: 399, weight: [[6.17383771e+88]], train loss: [[5.07841497e+177]]\n",
      "iteration: 400, weight: [[-1.02775664e+89]], train loss: [[1.4073378e+178]]\n",
      "iteration: 401, weight: [[1.71090295e+89]], train loss: [[3.90003512e+178]]\n",
      "iteration: 402, weight: [[-2.84813427e+89]], train loss: [[1.08078345e+179]]\n",
      "iteration: 403, weight: [[4.74127933e+89]], train loss: [[2.99508293e+179]]\n",
      "iteration: 404, weight: [[-7.89279142e+89]], train loss: [[8.30001769e+179]]\n",
      "iteration: 405, weight: [[1.31391028e+90]], train loss: [[2.30011306e+180]]\n",
      "iteration: 406, weight: [[-2.18726192e+90]], train loss: [[6.37410703e+180]]\n",
      "iteration: 407, weight: [[3.64112741e+90]], train loss: [[1.76640188e+181]]\n",
      "iteration: 408, weight: [[-6.06137229e+90]], train loss: [[4.89507877e+181]]\n",
      "iteration: 409, weight: [[1.00903456e+91]], train loss: [[1.3565314e+182]]\n",
      "iteration: 410, weight: [[-1.67973637e+91]], train loss: [[3.75923969e+182]]\n",
      "iteration: 411, weight: [[2.79625136e+91]], train loss: [[1.04176601e+183]]\n",
      "iteration: 412, weight: [[-4.65491004e+91]], train loss: [[2.88695724e+183]]\n",
      "iteration: 413, weight: [[7.74901275e+91]], train loss: [[8.0003782e+183]]\n",
      "iteration: 414, weight: [[-1.28997549e+92]], train loss: [[2.21707653e+184]]\n",
      "iteration: 415, weight: [[2.14741777e+92]], train loss: [[6.14399496e+184]]\n",
      "iteration: 416, weight: [[-3.57479898e+92]], train loss: [[1.70263289e+185]]\n",
      "iteration: 417, weight: [[5.95095558e+92]], train loss: [[4.7183612e+185]]\n",
      "iteration: 418, weight: [[-9.90653532e+92]], train loss: [[1.30755917e+186]]\n",
      "iteration: 419, weight: [[1.64913753e+93]], train loss: [[3.62352713e+186]]\n",
      "iteration: 420, weight: [[-2.74531358e+93]], train loss: [[1.00415715e+187]]\n",
      "iteration: 421, weight: [[4.57011409e+93]], train loss: [[2.782735e+187]]\n",
      "iteration: 422, weight: [[-7.60785322e+93]], train loss: [[7.71155601e+187]]\n",
      "iteration: 423, weight: [[1.26647671e+94]], train loss: [[2.1370377e+188]]\n",
      "iteration: 424, weight: [[-2.10829943e+94]], train loss: [[5.92219019e+188]]\n",
      "iteration: 425, weight: [[3.50967882e+94]], train loss: [[1.64116602e+189]]\n",
      "iteration: 426, weight: [[-5.84255028e+94]], train loss: [[4.54802333e+189]]\n",
      "iteration: 427, weight: [[9.72607338e+94]], train loss: [[1.26035489e+190]]\n",
      "iteration: 428, weight: [[-1.61909609e+95]], train loss: [[3.49271393e+190]]\n",
      "iteration: 429, weight: [[2.69530371e+95]], train loss: [[9.67906003e+190]]\n",
      "iteration: 430, weight: [[-4.48686282e+95]], train loss: [[2.6822753e+191]]\n",
      "iteration: 431, weight: [[7.46926512e+95]], train loss: [[7.4331606e+191]]\n",
      "iteration: 432, weight: [[-1.243406e+96]], train loss: [[2.05988836e+192]]\n",
      "iteration: 433, weight: [[2.06989369e+96]], train loss: [[5.7083928e+192]]\n",
      "iteration: 434, weight: [[-3.44574492e+96]], train loss: [[1.58191818e+193]]\n",
      "iteration: 435, weight: [[5.73611974e+96]], train loss: [[4.38383483e+193]]\n",
      "iteration: 436, weight: [[-9.54889882e+96]], train loss: [[1.21485473e+194]]\n",
      "iteration: 437, weight: [[1.58960191e+97]], train loss: [[3.36662323e+194]]\n",
      "iteration: 438, weight: [[-2.64620483e+97]], train loss: [[9.32963563e+194]]\n",
      "iteration: 439, weight: [[4.40512809e+97]], train loss: [[2.5854423e+195]]\n",
      "iteration: 440, weight: [[-7.3332016e+97]], train loss: [[7.16481557e+195]]\n",
      "iteration: 441, weight: [[1.22075555e+98]], train loss: [[1.98552419e+196]]\n",
      "iteration: 442, weight: [[-2.03218757e+98]], train loss: [[5.50231373e+196]]\n",
      "iteration: 443, weight: [[3.38297567e+98]], train loss: [[1.52480924e+197]]\n",
      "iteration: 444, weight: [[-5.63162798e+98]], train loss: [[4.22557371e+197]]\n",
      "iteration: 445, weight: [[9.37495175e+98]], train loss: [[1.17099718e+198]]\n",
      "iteration: 446, weight: [[-1.560645e+99]], train loss: [[3.24508454e+198]]\n",
      "iteration: 447, weight: [[2.59800037e+99]], train loss: [[8.99282583e+198]]\n",
      "iteration: 448, weight: [[-4.32488228e+99]], train loss: [[2.49210507e+199]]\n",
      "iteration: 449, weight: [[7.19961668e+99]], train loss: [[6.90615808e+199]]\n",
      "iteration: 450, weight: [[-1.19851772e+100]], train loss: [[1.91384465e+200]]\n",
      "iteration: 451, weight: [[1.99516832e+100]], train loss: [[5.30367433e+200]]\n",
      "iteration: 452, weight: [[-3.32134985e+100]], train loss: [[1.469762e+201]]\n",
      "iteration: 453, weight: [[5.52903969e+100]], train loss: [[4.07302599e+201]]\n",
      "iteration: 454, weight: [[-9.20417338e+100]], train loss: [[1.12872293e+202]]\n",
      "iteration: 455, weight: [[1.53221558e+101]], train loss: [[3.12793353e+202]]\n",
      "iteration: 456, weight: [[-2.55067401e+101]], train loss: [[8.66817522e+202]]\n",
      "iteration: 457, weight: [[4.24609827e+101]], train loss: [[2.40213742e+203]]\n",
      "iteration: 458, weight: [[-7.0684652e+101]], train loss: [[6.65683841e+203]]\n",
      "iteration: 459, weight: [[1.17668497e+102]], train loss: [[1.84475281e+204]]\n",
      "iteration: 460, weight: [[-1.95882343e+102]], train loss: [[5.11220603e+204]]\n",
      "iteration: 461, weight: [[3.26084664e+102]], train loss: [[1.41670202e+205]]\n",
      "iteration: 462, weight: [[-5.4283202e+102]], train loss: [[3.92598541e+205]]\n",
      "iteration: 463, weight: [[9.03650599e+102]], train loss: [[1.08797483e+206]]\n",
      "iteration: 464, weight: [[-1.50430405e+103]], train loss: [[3.01501179e+206]]\n",
      "iteration: 465, weight: [[2.50420978e+103]], train loss: [[8.35524484e+206]]\n",
      "iteration: 466, weight: [[-4.16874942e+103]], train loss: [[2.31541769e+207]]\n",
      "iteration: 467, weight: [[6.93970283e+103]], train loss: [[6.41651944e+207]]\n",
      "iteration: 468, weight: [[-1.15524995e+104]], train loss: [[1.77815527e+208]]\n",
      "iteration: 469, weight: [[1.92314062e+104]], train loss: [[4.92764993e+208]]\n",
      "iteration: 470, weight: [[-3.20144558e+104]], train loss: [[1.36555757e+209]]\n",
      "iteration: 471, weight: [[5.32943546e+104]], train loss: [[3.78425314e+209]]\n",
      "iteration: 472, weight: [[-8.8718929e+104]], train loss: [[1.04869778e+210]]\n",
      "iteration: 473, weight: [[1.47690096e+105]], train loss: [[2.90616664e+210]]\n",
      "iteration: 474, weight: [[-2.45859196e+105]], train loss: [[8.05361158e+210]]\n",
      "iteration: 475, weight: [[4.09280959e+105]], train loss: [[2.23182864e+211]]\n",
      "iteration: 476, weight: [[-6.81328606e+105]], train loss: [[6.18487624e+211]]\n",
      "iteration: 477, weight: [[1.13420539e+106]], train loss: [[1.71396196e+212]]\n",
      "iteration: 478, weight: [[-1.88810782e+106]], train loss: [[4.74975651e+212]]\n",
      "iteration: 479, weight: [[3.1431266e+106]], train loss: [[1.31625948e+213]]\n",
      "iteration: 480, weight: [[-5.23235205e+106]], train loss: [[3.64763757e+213]]\n",
      "iteration: 481, weight: [[8.71027848e+106]], train loss: [[1.01083867e+214]]\n",
      "iteration: 482, weight: [[-1.44999707e+107]], train loss: [[2.80125093e+214]]\n",
      "iteration: 483, weight: [[2.41380513e+107]], train loss: [[7.76286761e+214]]\n",
      "iteration: 484, weight: [[-4.01825311e+107]], train loss: [[2.15125725e+215]]\n",
      "iteration: 485, weight: [[6.68917216e+107]], train loss: [[5.9615956e+215]]\n",
      "iteration: 486, weight: [[-1.11354419e+108]], train loss: [[1.65208611e+216]]\n",
      "iteration: 487, weight: [[1.85371319e+108]], train loss: [[4.57828523e+216]]\n",
      "iteration: 488, weight: [[-3.08586998e+108]], train loss: [[1.26874111e+217]]\n",
      "iteration: 489, weight: [[5.13703715e+108]], train loss: [[3.51595396e+217]]\n",
      "iteration: 490, weight: [[-8.5516081e+108]], train loss: [[9.74346313e+217]]\n",
      "iteration: 491, weight: [[1.42358326e+109]], train loss: [[2.70012278e+218]]\n",
      "iteration: 492, weight: [[-2.36983417e+109]], train loss: [[7.48261981e+218]]\n",
      "iteration: 493, weight: [[3.94505479e+109]], train loss: [[2.07359456e+219]]\n",
      "iteration: 494, weight: [[-6.56731916e+109]], train loss: [[5.74637564e+219]]\n",
      "iteration: 495, weight: [[1.09325937e+110]], train loss: [[1.59244404e+220]]\n",
      "iteration: 496, weight: [[-1.81994511e+110]], train loss: [[4.41300425e+220]]\n",
      "iteration: 497, weight: [[3.02965637e+110]], train loss: [[1.2229382e+221]]\n",
      "iteration: 498, weight: [[-5.04345855e+110]], train loss: [[3.38902427e+221]]\n",
      "iteration: 499, weight: [[8.39582813e+110]], train loss: [[9.39171371e+221]]\n",
      "iteration: 500, weight: [[-1.39765062e+111]], train loss: [[2.60264546e+222]]\n",
      "iteration: 501, weight: [[2.32666419e+111]], train loss: [[7.21248925e+222]]\n",
      "iteration: 502, weight: [[-3.87318989e+111]], train loss: [[1.99873559e+223]]\n",
      "iteration: 503, weight: [[6.4476859e+111]], train loss: [[5.53892535e+223]]\n",
      "iteration: 504, weight: [[-1.07334406e+112]], train loss: [[1.53495511e+224]]\n",
      "iteration: 505, weight: [[1.78679216e+112]], train loss: [[4.25369009e+224]]\n",
      "iteration: 506, weight: [[-2.97446678e+112]], train loss: [[1.17878883e+225]]\n",
      "iteration: 507, weight: [[4.95158463e+112]], train loss: [[3.26667688e+225]]\n",
      "iteration: 508, weight: [[-8.24288592e+112]], train loss: [[9.05266282e+225]]\n",
      "iteration: 509, weight: [[1.37219039e+113]], train loss: [[2.50868719e+226]]\n",
      "iteration: 510, weight: [[-2.28428062e+113]], train loss: [[6.95211069e+226]]\n",
      "iteration: 511, weight: [[3.80263411e+113]], train loss: [[1.9265791e+227]]\n",
      "iteration: 512, weight: [[-6.33023193e+113]], train loss: [[5.33896423e+227]]\n",
      "iteration: 513, weight: [[1.05379154e+114]], train loss: [[1.47954159e+228]]\n",
      "iteration: 514, weight: [[-1.75424315e+114]], train loss: [[4.10012733e+228]]\n",
      "iteration: 515, weight: [[2.92028254e+114]], train loss: [[1.13623329e+229]]\n",
      "iteration: 516, weight: [[-4.86138432e+114]], train loss: [[3.14874635e+229]]\n",
      "iteration: 517, weight: [[8.09272977e+114]], train loss: [[8.72585203e+229]]\n",
      "iteration: 518, weight: [[-1.34719395e+115]], train loss: [[2.4181209e+230]]\n",
      "iteration: 519, weight: [[2.24266913e+115]], train loss: [[6.70113207e+230]]\n",
      "iteration: 520, weight: [[-3.7333636e+115]], train loss: [[1.85702754e+231]]\n",
      "iteration: 521, weight: [[6.21491756e+115]], train loss: [[5.14622192e+231]]\n",
      "iteration: 522, weight: [[-1.03459519e+116]], train loss: [[1.42612856e+232]]\n",
      "iteration: 523, weight: [[1.72228706e+116]], train loss: [[3.95210836e+232]]\n",
      "iteration: 524, weight: [[-2.86708535e+116]], train loss: [[1.09521406e+233]]\n",
      "iteration: 525, weight: [[4.77282714e+116]], train loss: [[3.03507325e+233]]\n",
      "iteration: 526, weight: [[-7.94530894e+116]], train loss: [[8.41083946e+233]]\n",
      "iteration: 527, weight: [[1.32265285e+117]], train loss: [[2.33082416e+234]]\n",
      "iteration: 528, weight: [[-2.20181565e+117]], train loss: [[6.45921405e+234]]\n",
      "iteration: 529, weight: [[3.66535496e+117]], train loss: [[1.78998686e+235]]\n",
      "iteration: 530, weight: [[-6.1017038e+117]], train loss: [[4.96043782e+235]]\n",
      "iteration: 531, weight: [[1.01574854e+118]], train loss: [[1.3746438e+236]]\n",
      "iteration: 532, weight: [[-1.6909131e+118]], train loss: [[3.80943303e+236]]\n",
      "iteration: 533, weight: [[2.81485723e+118]], train loss: [[1.05567566e+237]]\n",
      "iteration: 534, weight: [[-4.68588316e+118]], train loss: [[2.92550387e+237]]\n",
      "iteration: 535, weight: [[7.80057359e+118]], train loss: [[8.10719918e+237]]\n",
      "iteration: 536, weight: [[-1.29855881e+119]], train loss: [[2.24667892e+238]]\n",
      "iteration: 537, weight: [[2.16170638e+119]], train loss: [[6.22602953e+238]]\n",
      "iteration: 538, weight: [[-3.59858519e+119]], train loss: [[1.72536643e+239]]\n",
      "iteration: 539, weight: [[5.9905524e+119]], train loss: [[4.78136072e+239]]\n",
      "iteration: 540, weight: [[-9.97245201e+119]], train loss: [[1.32501769e+240]]\n",
      "iteration: 541, weight: [[1.66011066e+120]], train loss: [[3.67190843e+240]]\n",
      "iteration: 542, weight: [[-2.76358051e+120]], train loss: [[1.01756465e+241]]\n",
      "iteration: 543, weight: [[4.60052298e+120]], train loss: [[2.81989006e+241]]\n",
      "iteration: 544, weight: [[-7.65847481e+120]], train loss: [[7.81452064e+241]]\n",
      "iteration: 545, weight: [[1.27490367e+121]], train loss: [[2.16557141e+242]]\n",
      "iteration: 546, weight: [[-2.12232776e+121]], train loss: [[6.00126322e+242]]\n",
      "iteration: 547, weight: [[3.53303173e+121]], train loss: [[1.66307886e+243]]\n",
      "iteration: 548, weight: [[-5.88142579e+121]], train loss: [[4.6087485e+243]]\n",
      "iteration: 549, weight: [[9.7907893e+121]], train loss: [[1.27718314e+244]]\n",
      "iteration: 550, weight: [[-1.62986933e+122]], train loss: [[3.53934862e+244]]\n",
      "iteration: 551, weight: [[2.71323788e+122]], train loss: [[9.80829476e+244]]\n",
      "iteration: 552, weight: [[-4.51671777e+122]], train loss: [[2.71808901e+245]]\n",
      "iteration: 553, weight: [[7.51896456e+122]], train loss: [[7.5324081e+245]]\n",
      "iteration: 554, weight: [[-1.25167945e+123]], train loss: [[2.08739197e+246]]\n",
      "iteration: 555, weight: [[2.08366648e+123]], train loss: [[5.78461121e+246]]\n",
      "iteration: 556, weight: [[-3.46867242e+123]], train loss: [[1.60303994e+247]]\n",
      "iteration: 557, weight: [[5.77428707e+123]], train loss: [[4.44236776e+247]]\n",
      "iteration: 558, weight: [[-9.61243585e+123]], train loss: [[1.23107546e+248]]\n",
      "iteration: 559, weight: [[1.60017889e+124]], train loss: [[3.41157436e+248]]\n",
      "iteration: 560, weight: [[-2.66381231e+124]], train loss: [[9.45420485e+248]]\n",
      "iteration: 561, weight: [[4.43443919e+124]], train loss: [[2.6199631e+249]]\n",
      "iteration: 562, weight: [[-7.38199569e+124]], train loss: [[7.26048013e+249]]\n",
      "iteration: 563, weight: [[1.22887829e+125]], train loss: [[2.01203489e+250]]\n",
      "iteration: 564, weight: [[-2.04570946e+125]], train loss: [[5.57578058e+250]]\n",
      "iteration: 565, weight: [[3.40548552e+125]], train loss: [[1.54516848e+251]]\n",
      "iteration: 566, weight: [[-5.66910004e+125]], train loss: [[4.28199354e+251]]\n",
      "iteration: 567, weight: [[9.43733136e+125]], train loss: [[1.18663232e+252]]\n",
      "iteration: 568, weight: [[-1.57102931e+126]], train loss: [[3.28841288e+252]]\n",
      "iteration: 569, weight: [[2.61528709e+126]], train loss: [[9.11289796e+252]]\n",
      "iteration: 570, weight: [[-4.35365944e+126]], train loss: [[2.52537964e+253]]\n",
      "iteration: 571, weight: [[7.24752191e+126]], train loss: [[6.99836905e+253]]\n",
      "iteration: 572, weight: [[-1.20649248e+127]], train loss: [[1.93939829e+254]]\n",
      "iteration: 573, weight: [[2.00844389e+127]], train loss: [[5.37448895e+254]]\n",
      "iteration: 574, weight: [[-3.34344965e+127]], train loss: [[1.48938625e+255]]\n",
      "iteration: 575, weight: [[5.56582914e+127]], train loss: [[4.127409e+255]]\n",
      "iteration: 576, weight: [[-9.26541665e+127]], train loss: [[1.14379363e+256]]\n",
      "iteration: 577, weight: [[1.54241073e+128]], train loss: [[3.16969767e+256]]\n",
      "iteration: 578, weight: [[-2.56764584e+128]], train loss: [[8.78391262e+256]]\n",
      "iteration: 579, weight: [[4.27435121e+128]], train loss: [[2.43421074e+257]]\n",
      "iteration: 580, weight: [[-7.11549777e+128]], train loss: [[6.74572046e+257]]\n",
      "iteration: 581, weight: [[1.18451447e+129]], train loss: [[1.86938394e+258]]\n",
      "iteration: 582, weight: [[-1.97185717e+129]], train loss: [[5.18046416e+258]]\n",
      "iteration: 583, weight: [[3.28254385e+129]], train loss: [[1.43561782e+259]]\n",
      "iteration: 584, weight: [[-5.46443948e+129]], train loss: [[3.97840513e+259]]\n",
      "iteration: 585, weight: [[9.09663362e+129]], train loss: [[1.10250146e+260]]\n",
      "iteration: 586, weight: [[-1.51431347e+130]], train loss: [[3.0552682e+260]]\n",
      "iteration: 587, weight: [[2.52087244e+130]], train loss: [[8.466804e+260]]\n",
      "iteration: 588, weight: [[-4.19648769e+130]], train loss: [[2.34633313e+261]]\n",
      "iteration: 589, weight: [[6.98587864e+130]], train loss: [[6.50219275e+261]]\n",
      "iteration: 590, weight: [[-1.16293682e+131]], train loss: [[1.80189718e+262]]\n",
      "iteration: 591, weight: [[1.93593692e+131]], train loss: [[4.99344387e+262]]\n",
      "iteration: 592, weight: [[-3.22274755e+131]], train loss: [[1.38379048e+263]]\n",
      "iteration: 593, weight: [[5.36489677e+131]], train loss: [[3.83478046e+263]]\n",
      "iteration: 594, weight: [[-8.93092522e+131]], train loss: [[1.06269998e+264]]\n",
      "iteration: 595, weight: [[1.48672805e+132]], train loss: [[2.94496976e+264]]\n",
      "iteration: 596, weight: [[-2.47495108e+132]], train loss: [[8.16114334e+264]]\n",
      "iteration: 597, weight: [[4.12004257e+132]], train loss: [[2.261628e+265]]\n",
      "iteration: 598, weight: [[-6.85862071e+132]], train loss: [[6.26745665e+265]]\n",
      "iteration: 599, weight: [[1.14175223e+133]], train loss: [[1.73684677e+266]]\n",
      "iteration: 600, weight: [[-1.90067102e+133]], train loss: [[4.81317522e+266]]\n",
      "iteration: 601, weight: [[3.16404052e+133]], train loss: [[1.33383417e+267]]\n",
      "iteration: 602, weight: [[-5.26716738e+133]], train loss: [[3.6963408e+267]]\n",
      "iteration: 603, weight: [[8.76823544e+133]], train loss: [[1.02433538e+268]]\n",
      "iteration: 604, weight: [[-1.45964514e+134]], train loss: [[2.83865321e+268]]\n",
      "iteration: 605, weight: [[2.42986625e+134]], train loss: [[7.86651736e+268]]\n",
      "iteration: 606, weight: [[-4.04499e+134]], train loss: [[2.17998082e+269]]\n",
      "iteration: 607, weight: [[6.73368096e+134]], train loss: [[6.04119478e+269]]\n",
      "iteration: 608, weight: [[-1.12095356e+135]], train loss: [[1.67414475e+270]]\n",
      "iteration: 609, weight: [[1.86604754e+135]], train loss: [[4.63941446e+270]]\n",
      "iteration: 610, weight: [[-3.10640292e+135]], train loss: [[1.28568133e+271]]\n",
      "iteration: 611, weight: [[5.17121827e+135]], train loss: [[3.56289895e+271]]\n",
      "iteration: 612, weight: [[-8.60850929e+135]], train loss: [[9.87355777e+271]]\n",
      "iteration: 613, weight: [[1.43305558e+136]], train loss: [[2.7361748e+272]]\n",
      "iteration: 614, weight: [[-2.38560271e+136]], train loss: [[7.58252769e+272]]\n",
      "iteration: 615, weight: [[3.97130463e+136]], train loss: [[2.10128118e+273]]\n",
      "iteration: 616, weight: [[-6.61101718e+136]], train loss: [[5.8231012e+273]]\n",
      "iteration: 617, weight: [[1.10053376e+137]], train loss: [[1.61370634e+274]]\n",
      "iteration: 618, weight: [[-1.83205477e+137]], train loss: [[4.47192665e+274]]\n",
      "iteration: 619, weight: [[3.04981528e+137]], train loss: [[1.23926686e+275]]\n",
      "iteration: 620, weight: [[-5.07701702e+137]], train loss: [[3.43427449e+275]]\n",
      "iteration: 621, weight: [[8.45169278e+137]], train loss: [[9.5171118e+275]]\n",
      "iteration: 622, weight: [[-1.40695039e+138]], train loss: [[2.63739597e+276]]\n",
      "iteration: 623, weight: [[2.34214549e+138]], train loss: [[7.30879034e+276]]\n",
      "iteration: 624, weight: [[-3.89896155e+138]], train loss: [[2.02542269e+277]]\n",
      "iteration: 625, weight: [[6.49058789e+138]], train loss: [[5.61288103e+277]]\n",
      "iteration: 626, weight: [[-1.08048594e+139]], train loss: [[1.55544982e+278]]\n",
      "iteration: 627, weight: [[1.79868123e+139]], train loss: [[4.31048532e+278]]\n",
      "iteration: 628, weight: [[-2.99425846e+139]], train loss: [[1.19452801e+279]]\n",
      "iteration: 629, weight: [[4.98453177e+139]], train loss: [[3.31029352e+279]]\n",
      "iteration: 630, weight: [[-8.29773292e+139]], train loss: [[9.1735339e+279]]\n",
      "iteration: 631, weight: [[1.38132075e+140]], train loss: [[2.54218316e+280]]\n",
      "iteration: 632, weight: [[-2.2994799e+140]], train loss: [[7.04493521e+280]]\n",
      "iteration: 633, weight: [[3.8279363e+140]], train loss: [[1.95230277e+281]]\n",
      "iteration: 634, weight: [[-6.3723524e+140]], train loss: [[5.41025004e+281]]\n",
      "iteration: 635, weight: [[1.06080332e+141]], train loss: [[1.49929642e+282]]\n",
      "iteration: 636, weight: [[-1.76591564e+141]], train loss: [[4.1548722e+282]]\n",
      "iteration: 637, weight: [[2.93971369e+141]], train loss: [[1.15140427e+283]]\n",
      "iteration: 638, weight: [[-4.89373128e+141]], train loss: [[3.19078839e+283]]\n",
      "iteration: 639, weight: [[8.14657766e+141]], train loss: [[8.84235954e+283]]\n",
      "iteration: 640, weight: [[-1.35615798e+142]], train loss: [[2.45040763e+284]]\n",
      "iteration: 641, weight: [[2.25759154e+142]], train loss: [[6.79060553e+284]]\n",
      "iteration: 642, weight: [[-3.75820488e+142]], train loss: [[1.88182255e+285]]\n",
      "iteration: 643, weight: [[6.25627075e+142]], train loss: [[5.21493424e+285]]\n",
      "iteration: 644, weight: [[-1.04147924e+143]], train loss: [[1.44517022e+286]]\n",
      "iteration: 645, weight: [[1.73374692e+143]], train loss: [[4.00487688e+286]]\n",
      "iteration: 646, weight: [[-2.88616253e+143]], train loss: [[1.10983735e+287]]\n",
      "iteration: 647, weight: [[4.80458486e+143]], train loss: [[3.07559752e+287]]\n",
      "iteration: 648, weight: [[-7.99817591e+143]], train loss: [[8.52314092e+287]]\n",
      "iteration: 649, weight: [[1.3314536e+144]], train loss: [[2.3619453e+288]]\n",
      "iteration: 650, weight: [[-2.21646623e+144]], train loss: [[6.54545743e+288]]\n",
      "iteration: 651, weight: [[3.68974371e+144]], train loss: [[1.81388675e+289]]\n",
      "iteration: 652, weight: [[-6.14230368e+144]], train loss: [[5.02666955e+289]]\n",
      "iteration: 653, weight: [[1.02250718e+145]], train loss: [[1.39299803e+290]]\n",
      "iteration: 654, weight: [[-1.7021642e+145]], train loss: [[3.86029655e+290]]\n",
      "iteration: 655, weight: [[2.83358689e+145]], train loss: [[1.06977104e+291]]\n",
      "iteration: 656, weight: [[-4.71706236e+145]], train loss: [[2.96456517e+291]]\n",
      "iteration: 657, weight: [[7.85247751e+145]], train loss: [[8.21544644e+291]]\n",
      "iteration: 658, weight: [[-1.30719923e+146]], train loss: [[2.27667655e+292]]\n",
      "iteration: 659, weight: [[2.17609007e+146]], train loss: [[6.30915943e+292]]\n",
      "iteration: 660, weight: [[-3.62252967e+146]], train loss: [[1.7484035e+293]]\n",
      "iteration: 661, weight: [[6.03041269e+146]], train loss: [[4.84520141e+293]]\n",
      "iteration: 662, weight: [[-1.00388073e+147]], train loss: [[1.34270931e+294]]\n",
      "iteration: 663, weight: [[1.6711568e+147]], train loss: [[3.72093573e+294]]\n",
      "iteration: 664, weight: [[-2.78196899e+147]], train loss: [[1.03115116e+295]]\n",
      "iteration: 665, weight: [[4.63113422e+147]], train loss: [[2.8575412e+295]]\n",
      "iteration: 666, weight: [[-7.70943322e+147]], train loss: [[7.91886005e+295]]\n",
      "iteration: 667, weight: [[1.2833867e+148]], train loss: [[2.1944861e+296]]\n",
      "iteration: 668, weight: [[-2.13644943e+148]], train loss: [[6.08139204e+296]]\n",
      "iteration: 669, weight: [[3.55654003e+148]], train loss: [[1.68528427e+297]]\n",
      "iteration: 670, weight: [[-5.92055996e+148]], train loss: [[4.67028447e+297]]\n",
      "iteration: 671, weight: [[9.85593583e+148]], train loss: [[1.29423608e+298]]\n",
      "iteration: 672, weight: [[-1.64071425e+149]], train loss: [[3.58660597e+298]]\n",
      "iteration: 673, weight: [[2.73129138e+149]], train loss: [[9.93925503e+298]]\n",
      "iteration: 674, weight: [[-4.54677138e+149]], train loss: [[2.75438092e+299]]\n",
      "iteration: 675, weight: [[7.56899469e+149]], train loss: [[7.63298076e+299]]\n",
      "iteration: 676, weight: [[-1.26000795e+150]], train loss: [[2.11526281e+300]]\n",
      "iteration: 677, weight: [[2.0975309e+150]], train loss: [[5.8618473e+300]]\n",
      "iteration: 678, weight: [[-3.49175248e+150]], train loss: [[1.62444371e+301]]\n",
      "iteration: 679, weight: [[5.81270836e+150]], train loss: [[4.50168222e+301]]\n",
      "iteration: 680, weight: [[-9.67639564e+150]], train loss: [[1.24751277e+302]]\n",
      "iteration: 681, weight: [[1.61082626e+151]], train loss: [[3.45712567e+302]]\n",
      "iteration: 682, weight: [[-2.68153694e+151]], train loss: [[9.58043732e+302]]\n",
      "iteration: 683, weight: [[4.46394533e+151]], train loss: [[2.65494483e+303]]\n",
      "iteration: 684, weight: [[-7.43111445e+151]], train loss: [[7.357422e+303]]\n",
      "iteration: 685, weight: [[1.23705507e+152]], train loss: [[inf]]\n",
      "iteration: 686, weight: [[-2.05932133e+152]], train loss: [[inf]]\n",
      "iteration: 687, weight: [[3.42814514e+152]], train loss: [[inf]]\n",
      "iteration: 688, weight: [[-5.70682143e+152]], train loss: [[inf]]\n",
      "iteration: 689, weight: [[9.50012603e+152]], train loss: [[inf]]\n",
      "iteration: 690, weight: [[-1.58148272e+153]], train loss: [[inf]]\n",
      "iteration: 691, weight: [[2.63268885e+153]], train loss: [[inf]]\n",
      "iteration: 692, weight: [[-4.38262808e+153]], train loss: [[inf]]\n",
      "iteration: 693, weight: [[7.2957459e+153]], train loss: [[inf]]\n",
      "iteration: 694, weight: [[-1.21452031e+154]], train loss: [[inf]]\n",
      "iteration: 695, weight: [[2.02180779e+154]], train loss: [[inf]]\n",
      "iteration: 696, weight: [[-3.36569649e+154]], train loss: [[inf]]\n",
      "iteration: 697, weight: [[5.60286338e+154]], train loss: [[inf]]\n",
      "iteration: 698, weight: [[-9.32706743e+154]], train loss: [[inf]]\n",
      "iteration: 699, weight: [[1.55267371e+155]], train loss: [[inf]]\n",
      "iteration: 700, weight: [[-2.58473059e+155]], train loss: [[inf]]\n",
      "iteration: 701, weight: [[4.30279214e+155]], train loss: [[inf]]\n",
      "iteration: 702, weight: [[-7.16284329e+155]], train loss: [[inf]]\n",
      "iteration: 703, weight: [[1.19239606e+156]], train loss: [[inf]]\n",
      "iteration: 704, weight: [[-1.98497763e+156]], train loss: [[inf]]\n",
      "iteration: 705, weight: [[3.30438544e+156]], train loss: [[inf]]\n",
      "iteration: 706, weight: [[-5.50079909e+156]], train loss: [[inf]]\n",
      "iteration: 707, weight: [[9.15716134e+156]], train loss: [[inf]]\n",
      "iteration: 708, weight: [[-1.5243895e+157]], train loss: [[inf]]\n",
      "iteration: 709, weight: [[2.53764597e+157]], train loss: [[inf]]\n",
      "iteration: 710, weight: [[-4.22441053e+157]], train loss: [[inf]]\n",
      "iteration: 711, weight: [[7.03236169e+157]], train loss: [[inf]]\n",
      "iteration: 712, weight: [[-1.17067483e+158]], train loss: [[inf]]\n",
      "iteration: 713, weight: [[1.94881838e+158]], train loss: [[inf]]\n",
      "iteration: 714, weight: [[-3.24419126e+158]], train loss: [[inf]]\n",
      "iteration: 715, weight: [[5.40059404e+158]], train loss: [[inf]]\n",
      "iteration: 716, weight: [[-8.99035034e+158]], train loss: [[inf]]\n",
      "iteration: 717, weight: [[1.49662053e+159]], train loss: [[inf]]\n",
      "iteration: 718, weight: [[-2.49141906e+159]], train loss: [[inf]]\n",
      "iteration: 719, weight: [[4.14745675e+159]], train loss: [[inf]]\n",
      "iteration: 720, weight: [[-6.904257e+159]], train loss: [[inf]]\n",
      "iteration: 721, weight: [[1.14934929e+160]], train loss: [[inf]]\n",
      "iteration: 722, weight: [[-1.91331782e+160]], train loss: [[inf]]\n",
      "iteration: 723, weight: [[3.1850936e+160]], train loss: [[inf]]\n",
      "iteration: 724, weight: [[-5.30221437e+160]], train loss: [[inf]]\n",
      "iteration: 725, weight: [[8.82657804e+160]], train loss: [[inf]]\n",
      "iteration: 726, weight: [[-1.46935741e+161]], train loss: [[inf]]\n",
      "iteration: 727, weight: [[2.44603424e+161]], train loss: [[inf]]\n",
      "iteration: 728, weight: [[-4.0719048e+161]], train loss: [[inf]]\n",
      "iteration: 729, weight: [[6.77848593e+161]], train loss: [[inf]]\n",
      "iteration: 730, weight: [[-1.12841222e+162]], train loss: [[inf]]\n",
      "iteration: 731, weight: [[1.87846395e+162]], train loss: [[inf]]\n",
      "iteration: 732, weight: [[-3.12707249e+162]], train loss: [[inf]]\n",
      "iteration: 733, weight: [[5.20562683e+162]], train loss: [[inf]]\n",
      "iteration: 734, weight: [[-8.6657891e+162]], train loss: [[inf]]\n",
      "iteration: 735, weight: [[1.44259093e+163]], train loss: [[inf]]\n",
      "iteration: 736, weight: [[-2.40147617e+163]], train loss: [[inf]]\n",
      "iteration: 737, weight: [[3.99772913e+163]], train loss: [[inf]]\n",
      "iteration: 738, weight: [[-6.65500596e+163]], train loss: [[inf]]\n",
      "iteration: 739, weight: [[1.10785656e+164]], train loss: [[inf]]\n",
      "iteration: 740, weight: [[-1.844245e+164]], train loss: [[inf]]\n",
      "iteration: 741, weight: [[3.07010832e+164]], train loss: [[inf]]\n",
      "iteration: 742, weight: [[-5.11079877e+164]], train loss: [[inf]]\n",
      "iteration: 743, weight: [[8.50792916e+164]], train loss: [[inf]]\n",
      "iteration: 744, weight: [[-1.41631204e+165]], train loss: [[inf]]\n",
      "iteration: 745, weight: [[2.35772979e+165]], train loss: [[inf]]\n",
      "iteration: 746, weight: [[-3.92490469e+165]], train loss: [[inf]]\n",
      "iteration: 747, weight: [[6.53377535e+165]], train loss: [[inf]]\n",
      "iteration: 748, weight: [[-1.08767534e+166]], train loss: [[inf]]\n",
      "iteration: 749, weight: [[1.8106494e+166]], train loss: [[inf]]\n",
      "iteration: 750, weight: [[-3.01418183e+166]], train loss: [[inf]]\n",
      "iteration: 751, weight: [[5.01769814e+166]], train loss: [[inf]]\n",
      "iteration: 752, weight: [[-8.35294486e+166]], train loss: [[inf]]\n",
      "iteration: 753, weight: [[1.39051186e+167]], train loss: [[inf]]\n",
      "iteration: 754, weight: [[-2.31478032e+167]], train loss: [[inf]]\n",
      "iteration: 755, weight: [[3.85340685e+167]], train loss: [[inf]]\n",
      "iteration: 756, weight: [[-6.41475314e+167]], train loss: [[inf]]\n",
      "iteration: 757, weight: [[1.06786175e+168]], train loss: [[inf]]\n",
      "iteration: 758, weight: [[-1.77766579e+168]], train loss: [[inf]]\n",
      "iteration: 759, weight: [[2.95927413e+168]], train loss: [[inf]]\n",
      "iteration: 760, weight: [[-4.92629348e+168]], train loss: [[inf]]\n",
      "iteration: 761, weight: [[8.20078383e+168]], train loss: [[inf]]\n",
      "iteration: 762, weight: [[-1.36518167e+169]], train loss: [[inf]]\n",
      "iteration: 763, weight: [[2.27261323e+169]], train loss: [[inf]]\n",
      "iteration: 764, weight: [[-3.78321144e+169]], train loss: [[inf]]\n",
      "iteration: 765, weight: [[6.29789909e+169]], train loss: [[inf]]\n",
      "iteration: 766, weight: [[-1.0484091e+170]], train loss: [[inf]]\n",
      "iteration: 767, weight: [[1.74528302e+170]], train loss: [[inf]]\n",
      "iteration: 768, weight: [[-2.90536665e+170]], train loss: [[inf]]\n",
      "iteration: 769, weight: [[4.83655389e+170]], train loss: [[inf]]\n",
      "iteration: 770, weight: [[-8.05139464e+170]], train loss: [[inf]]\n",
      "iteration: 771, weight: [[1.3403129e+171]], train loss: [[inf]]\n",
      "iteration: 772, weight: [[-2.23121428e+171]], train loss: [[inf]]\n",
      "iteration: 773, weight: [[3.71429475e+171]], train loss: [[inf]]\n",
      "iteration: 774, weight: [[-6.1831737e+171]], train loss: [[inf]]\n",
      "iteration: 775, weight: [[1.0293108e+172]], train loss: [[inf]]\n",
      "iteration: 776, weight: [[-1.71349016e+172]], train loss: [[inf]]\n",
      "iteration: 777, weight: [[2.85244118e+172]], train loss: [[inf]]\n",
      "iteration: 778, weight: [[-4.74844903e+172]], train loss: [[inf]]\n",
      "iteration: 779, weight: [[7.90472679e+172]], train loss: [[inf]]\n",
      "iteration: 780, weight: [[-1.31589715e+173]], train loss: [[inf]]\n",
      "iteration: 781, weight: [[2.19056947e+173]], train loss: [[inf]]\n",
      "iteration: 782, weight: [[-3.64663347e+173]], train loss: [[inf]]\n",
      "iteration: 783, weight: [[6.07053821e+173]], train loss: [[inf]]\n",
      "iteration: 784, weight: [[-1.01056041e+174]], train loss: [[inf]]\n",
      "iteration: 785, weight: [[1.68227644e+174]], train loss: [[inf]]\n",
      "iteration: 786, weight: [[-2.80047982e+174]], train loss: [[inf]]\n",
      "iteration: 787, weight: [[4.66194913e+174]], train loss: [[inf]]\n",
      "iteration: 788, weight: [[-7.7607307e+174]], train loss: [[inf]]\n",
      "iteration: 789, weight: [[1.29192617e+175]], train loss: [[inf]]\n",
      "iteration: 790, weight: [[-2.15066506e+175]], train loss: [[inf]]\n",
      "iteration: 791, weight: [[3.58020474e+175]], train loss: [[inf]]\n",
      "iteration: 792, weight: [[-5.95995454e+175]], train loss: [[inf]]\n",
      "iteration: 793, weight: [[9.92151584e+175]], train loss: [[inf]]\n",
      "iteration: 794, weight: [[-1.65163133e+176]], train loss: [[inf]]\n",
      "iteration: 795, weight: [[2.74946501e+176]], train loss: [[inf]]\n",
      "iteration: 796, weight: [[-4.57702495e+176]], train loss: [[inf]]\n",
      "iteration: 797, weight: [[7.61935771e+176]], train loss: [[inf]]\n",
      "iteration: 798, weight: [[-1.26839186e+177]], train loss: [[inf]]\n",
      "iteration: 799, weight: [[2.11148757e+177]], train loss: [[inf]]\n",
      "iteration: 800, weight: [[-3.51498611e+177]], train loss: [[inf]]\n",
      "iteration: 801, weight: [[5.8513853e+177]], train loss: [[inf]]\n",
      "iteration: 802, weight: [[-9.74078101e+177]], train loss: [[inf]]\n",
      "iteration: 803, weight: [[1.62154447e+178]], train loss: [[inf]]\n",
      "iteration: 804, weight: [[-2.69937951e+178]], train loss: [[inf]]\n",
      "iteration: 805, weight: [[4.49364779e+178]], train loss: [[inf]]\n",
      "iteration: 806, weight: [[-7.48056004e+178]], train loss: [[inf]]\n",
      "iteration: 807, weight: [[1.24528626e+179]], train loss: [[inf]]\n",
      "iteration: 808, weight: [[-2.07302376e+179]], train loss: [[inf]]\n",
      "iteration: 809, weight: [[3.45095553e+179]], train loss: [[inf]]\n",
      "iteration: 810, weight: [[-5.74479382e+179]], train loss: [[inf]]\n",
      "iteration: 811, weight: [[9.56333853e+179]], train loss: [[inf]]\n",
      "iteration: 812, weight: [[-1.59200568e+180]], train loss: [[inf]]\n",
      "iteration: 813, weight: [[2.65020639e+180]], train loss: [[inf]]\n",
      "iteration: 814, weight: [[-4.41178947e+180]], train loss: [[inf]]\n",
      "iteration: 815, weight: [[7.34429077e+180]], train loss: [[inf]]\n",
      "iteration: 816, weight: [[-1.22260156e+181]], train loss: [[inf]]\n",
      "iteration: 817, weight: [[2.03526062e+181]], train loss: [[inf]]\n",
      "iteration: 818, weight: [[-3.38809136e+181]], train loss: [[inf]]\n",
      "iteration: 819, weight: [[5.64014405e+181]], train loss: [[inf]]\n",
      "iteration: 820, weight: [[-9.38912842e+181]], train loss: [[inf]]\n",
      "iteration: 821, weight: [[1.56300498e+182]], train loss: [[inf]]\n",
      "iteration: 822, weight: [[-2.60192903e+182]], train loss: [[inf]]\n",
      "iteration: 823, weight: [[4.33142231e+182]], train loss: [[inf]]\n",
      "iteration: 824, weight: [[-7.21050384e+182]], train loss: [[inf]]\n",
      "iteration: 825, weight: [[1.2003301e+183]], train loss: [[inf]]\n",
      "iteration: 826, weight: [[-1.99818539e+183]], train loss: [[inf]]\n",
      "iteration: 827, weight: [[3.32637235e+183]], train loss: [[inf]]\n",
      "iteration: 828, weight: [[-5.53740063e+183]], train loss: [[inf]]\n",
      "iteration: 829, weight: [[9.2180918e+183]], train loss: [[inf]]\n",
      "iteration: 830, weight: [[-1.53453258e+184]], train loss: [[inf]]\n",
      "iteration: 831, weight: [[2.55453111e+184]], train loss: [[inf]]\n",
      "iteration: 832, weight: [[-4.25251916e+184]], train loss: [[inf]]\n",
      "iteration: 833, weight: [[7.07915403e+184]], train loss: [[inf]]\n",
      "iteration: 834, weight: [[-1.17846434e+185]], train loss: [[inf]]\n",
      "iteration: 835, weight: [[1.96178554e+185]], train loss: [[inf]]\n",
      "iteration: 836, weight: [[-3.26577765e+185]], train loss: [[inf]]\n",
      "iteration: 837, weight: [[5.43652883e+185]], train loss: [[inf]]\n",
      "iteration: 838, weight: [[-9.05017086e+185]], train loss: [[inf]]\n",
      "iteration: 839, weight: [[1.50657883e+186]], train loss: [[inf]]\n",
      "iteration: 840, weight: [[-2.50799661e+186]], train loss: [[inf]]\n",
      "iteration: 841, weight: [[4.17505334e+186]], train loss: [[inf]]\n",
      "iteration: 842, weight: [[-6.95019696e+186]], train loss: [[inf]]\n",
      "iteration: 843, weight: [[1.1569969e+187]], train loss: [[inf]]\n",
      "iteration: 844, weight: [[-1.92604876e+187]], train loss: [[inf]]\n",
      "iteration: 845, weight: [[3.20628676e+187]], train loss: [[inf]]\n",
      "iteration: 846, weight: [[-5.33749456e+187]], train loss: [[inf]]\n",
      "iteration: 847, weight: [[8.88530885e+187]], train loss: [[inf]]\n",
      "iteration: 848, weight: [[-1.47913431e+188]], train loss: [[inf]]\n",
      "iteration: 849, weight: [[2.46230981e+188]], train loss: [[inf]]\n",
      "iteration: 850, weight: [[-4.09899868e+188]], train loss: [[inf]]\n",
      "iteration: 851, weight: [[6.82358902e+188]], train loss: [[inf]]\n",
      "iteration: 852, weight: [[-1.13592052e+189]], train loss: [[inf]]\n",
      "iteration: 853, weight: [[1.89096299e+189]], train loss: [[inf]]\n",
      "iteration: 854, weight: [[-3.14787959e+189]], train loss: [[inf]]\n",
      "iteration: 855, weight: [[5.24026434e+189]], train loss: [[inf]]\n",
      "iteration: 856, weight: [[-8.72345004e+189]], train loss: [[inf]]\n",
      "iteration: 857, weight: [[1.45218973e+190]], train loss: [[inf]]\n",
      "iteration: 858, weight: [[-2.41745526e+190]], train loss: [[inf]]\n",
      "iteration: 859, weight: [[4.02432946e+190]], train loss: [[inf]]\n",
      "iteration: 860, weight: [[-6.69928743e+190]], train loss: [[inf]]\n",
      "iteration: 861, weight: [[1.11522808e+191]], train loss: [[inf]]\n",
      "iteration: 862, weight: [[-1.85651635e+191]], train loss: [[inf]]\n",
      "iteration: 863, weight: [[3.09053639e+191]], train loss: [[inf]]\n",
      "iteration: 864, weight: [[-5.14480531e+191]], train loss: [[inf]]\n",
      "iteration: 865, weight: [[8.56453972e+191]], train loss: [[inf]]\n",
      "iteration: 866, weight: [[-1.42573598e+192]], train loss: [[inf]]\n",
      "iteration: 867, weight: [[2.3734178e+192]], train loss: [[inf]]\n",
      "iteration: 868, weight: [[-3.95102045e+192]], train loss: [[inf]]\n",
      "iteration: 869, weight: [[6.57725017e+192]], train loss: [[inf]]\n",
      "iteration: 870, weight: [[-1.09491258e+193]], train loss: [[inf]]\n",
      "iteration: 871, weight: [[1.8226972e+193]], train loss: [[inf]]\n",
      "iteration: 872, weight: [[-3.03423778e+193]], train loss: [[inf]]\n",
      "iteration: 873, weight: [[5.0510852e+193]], train loss: [[inf]]\n",
      "iteration: 874, weight: [[-8.40852418e+193]], train loss: [[inf]]\n",
      "iteration: 875, weight: [[1.39976413e+194]], train loss: [[inf]]\n",
      "iteration: 876, weight: [[-2.33018254e+194]], train loss: [[inf]]\n",
      "iteration: 877, weight: [[3.87904687e+194]], train loss: [[inf]]\n",
      "iteration: 878, weight: [[-6.457436e+194]], train loss: [[inf]]\n",
      "iteration: 879, weight: [[1.07496715e+195]], train loss: [[inf]]\n",
      "iteration: 880, weight: [[-1.78949413e+195]], train loss: [[inf]]\n",
      "iteration: 881, weight: [[2.97896472e+195]], train loss: [[inf]]\n",
      "iteration: 882, weight: [[-4.95907234e+195]], train loss: [[inf]]\n",
      "iteration: 883, weight: [[8.25535069e+195]], train loss: [[inf]]\n",
      "iteration: 884, weight: [[-1.37426539e+196]], train loss: [[inf]]\n",
      "iteration: 885, weight: [[2.28773488e+196]], train loss: [[inf]]\n",
      "iteration: 886, weight: [[-3.8083844e+196]], train loss: [[inf]]\n",
      "iteration: 887, weight: [[6.33980442e+196]], train loss: [[inf]]\n",
      "iteration: 888, weight: [[-1.05538506e+197]], train loss: [[inf]]\n",
      "iteration: 889, weight: [[1.75689589e+197]], train loss: [[inf]]\n",
      "iteration: 890, weight: [[-2.92469855e+197]], train loss: [[inf]]\n",
      "iteration: 891, weight: [[4.86873563e+197]], train loss: [[inf]]\n",
      "iteration: 892, weight: [[-8.10496749e+197]], train loss: [[inf]]\n",
      "iteration: 893, weight: [[1.34923115e+198]], train loss: [[inf]]\n",
      "iteration: 894, weight: [[-2.24606047e+198]], train loss: [[inf]]\n",
      "iteration: 895, weight: [[3.73900914e+198]], train loss: [[inf]]\n",
      "iteration: 896, weight: [[-6.22431567e+198]], train loss: [[inf]]\n",
      "iteration: 897, weight: [[1.03615969e+199]], train loss: [[inf]]\n",
      "iteration: 898, weight: [[-1.72489148e+199]], train loss: [[inf]]\n",
      "iteration: 899, weight: [[2.87142092e+199]], train loss: [[inf]]\n",
      "iteration: 900, weight: [[-4.78004454e+199]], train loss: [[inf]]\n",
      "iteration: 901, weight: [[7.95732372e+199]], train loss: [[inf]]\n",
      "iteration: 902, weight: [[-1.32465295e+200]], train loss: [[inf]]\n",
      "iteration: 903, weight: [[2.20514521e+200]], train loss: [[inf]]\n",
      "iteration: 904, weight: [[-3.67089766e+200]], train loss: [[inf]]\n",
      "iteration: 905, weight: [[6.11093072e+200]], train loss: [[inf]]\n",
      "iteration: 906, weight: [[-1.01728454e+201]], train loss: [[inf]]\n",
      "iteration: 907, weight: [[1.69347007e+201]], train loss: [[inf]]\n",
      "iteration: 908, weight: [[-2.81911381e+201]], train loss: [[inf]]\n",
      "iteration: 909, weight: [[4.69296908e+201]], train loss: [[inf]]\n",
      "iteration: 910, weight: [[-7.81236951e+201]], train loss: [[inf]]\n",
      "iteration: 911, weight: [[1.30052247e+202]], train loss: [[inf]]\n",
      "iteration: 912, weight: [[-2.16497529e+202]], train loss: [[inf]]\n",
      "iteration: 913, weight: [[3.60402692e+202]], train loss: [[inf]]\n",
      "iteration: 914, weight: [[-5.99961123e+202]], train loss: [[inf]]\n",
      "iteration: 915, weight: [[9.98753221e+202]], train loss: [[inf]]\n",
      "iteration: 916, weight: [[-1.66262106e+203]], train loss: [[inf]]\n",
      "iteration: 917, weight: [[2.76775956e+203]], train loss: [[inf]]\n",
      "iteration: 918, weight: [[-4.60747983e+203]], train loss: [[inf]]\n",
      "iteration: 919, weight: [[7.67005585e+203]], train loss: [[inf]]\n",
      "iteration: 920, weight: [[-1.27683156e+204]], train loss: [[inf]]\n",
      "iteration: 921, weight: [[2.12553711e+204]], train loss: [[inf]]\n",
      "iteration: 922, weight: [[-3.53837433e+204]], train loss: [[inf]]\n",
      "iteration: 923, weight: [[5.8903196e+204]], train loss: [[inf]]\n",
      "iteration: 924, weight: [[-9.80559479e+204]], train loss: [[inf]]\n",
      "iteration: 925, weight: [[1.632334e+205]], train loss: [[inf]]\n",
      "iteration: 926, weight: [[-2.7173408e+205]], train loss: [[inf]]\n",
      "iteration: 927, weight: [[4.52354789e+205]], train loss: [[inf]]\n",
      "iteration: 928, weight: [[-7.53033463e+205]], train loss: [[inf]]\n",
      "iteration: 929, weight: [[1.25357222e+206]], train loss: [[inf]]\n",
      "iteration: 930, weight: [[-2.08681737e+206]], train loss: [[inf]]\n",
      "iteration: 931, weight: [[3.4739177e+206]], train loss: [[inf]]\n",
      "iteration: 932, weight: [[-5.78301886e+206]], train loss: [[inf]]\n",
      "iteration: 933, weight: [[9.62697164e+206]], train loss: [[inf]]\n",
      "iteration: 934, weight: [[-1.60259866e+207]], train loss: [[inf]]\n",
      "iteration: 935, weight: [[2.66784049e+207]], train loss: [[inf]]\n",
      "iteration: 936, weight: [[-4.44114489e+207]], train loss: [[inf]]\n",
      "iteration: 937, weight: [[7.39315864e+207]], train loss: [[inf]]\n",
      "iteration: 938, weight: [[-1.23073658e+208]], train loss: [[inf]]\n",
      "iteration: 939, weight: [[2.04880296e+208]], train loss: [[inf]]\n",
      "iteration: 940, weight: [[-3.41063524e+208]], train loss: [[inf]]\n",
      "iteration: 941, weight: [[5.67767277e+208]], train loss: [[inf]]\n",
      "iteration: 942, weight: [[-9.45160236e+208]], train loss: [[inf]]\n",
      "iteration: 943, weight: [[1.573405e+209]], train loss: [[inf]]\n",
      "iteration: 944, weight: [[-2.61924189e+209]], train loss: [[inf]]\n",
      "iteration: 945, weight: [[4.36024298e+209]], train loss: [[inf]]\n",
      "iteration: 946, weight: [[-7.25848152e+209]], train loss: [[inf]]\n",
      "iteration: 947, weight: [[1.20831692e+210]], train loss: [[inf]]\n",
      "iteration: 948, weight: [[-2.01148103e+210]], train loss: [[inf]]\n",
      "iteration: 949, weight: [[3.34850557e+210]], train loss: [[inf]]\n",
      "iteration: 950, weight: [[-5.57424571e+210]], train loss: [[inf]]\n",
      "iteration: 951, weight: [[9.27942769e+210]], train loss: [[inf]]\n",
      "iteration: 952, weight: [[-1.54474314e+211]], train loss: [[inf]]\n",
      "iteration: 953, weight: [[2.5715286e+211]], train loss: [[inf]]\n",
      "iteration: 954, weight: [[-4.28081482e+211]], train loss: [[inf]]\n",
      "iteration: 955, weight: [[7.12625773e+211]], train loss: [[inf]]\n",
      "iteration: 956, weight: [[-1.18630568e+212]], train loss: [[inf]]\n",
      "iteration: 957, weight: [[1.97483898e+212]], train loss: [[inf]]\n",
      "iteration: 958, weight: [[-3.28750767e+212]], train loss: [[inf]]\n",
      "iteration: 959, weight: [[5.47270273e+212]], train loss: [[inf]]\n",
      "iteration: 960, weight: [[-9.11038943e+212]], train loss: [[inf]]\n",
      "iteration: 961, weight: [[1.5166034e+213]], train loss: [[inf]]\n",
      "iteration: 962, weight: [[-2.52468447e+213]], train loss: [[inf]]\n",
      "iteration: 963, weight: [[4.20283356e+213]], train loss: [[inf]]\n",
      "iteration: 964, weight: [[-6.99644259e+213]], train loss: [[inf]]\n",
      "iteration: 965, weight: [[1.16469539e+214]], train loss: [[inf]]\n",
      "iteration: 966, weight: [[-1.93886442e+214]], train loss: [[inf]]\n",
      "iteration: 967, weight: [[3.22762094e+214]], train loss: [[inf]]\n",
      "iteration: 968, weight: [[-5.37300949e+214]], train loss: [[inf]]\n",
      "iteration: 969, weight: [[8.94443044e+214]], train loss: [[inf]]\n",
      "iteration: 970, weight: [[-1.48897626e+215]], train loss: [[inf]]\n",
      "iteration: 971, weight: [[2.47869367e+215]], train loss: [[inf]]\n",
      "iteration: 972, weight: [[-4.12627284e+215]], train loss: [[inf]]\n",
      "iteration: 973, weight: [[6.86899222e+215]], train loss: [[inf]]\n",
      "iteration: 974, weight: [[-1.14347877e+216]], train loss: [[inf]]\n",
      "iteration: 975, weight: [[1.90354519e+216]], train loss: [[inf]]\n",
      "iteration: 976, weight: [[-3.16882514e+216]], train loss: [[inf]]\n",
      "iteration: 977, weight: [[5.27513232e+216]], train loss: [[inf]]\n",
      "iteration: 978, weight: [[-8.78149464e+216]], train loss: [[inf]]\n",
      "iteration: 979, weight: [[1.4618524e+217]], train loss: [[inf]]\n",
      "iteration: 980, weight: [[-2.43354066e+217]], train loss: [[inf]]\n",
      "iteration: 981, weight: [[4.05110678e+217]], train loss: [[inf]]\n",
      "iteration: 982, weight: [[-6.74386354e+217]], train loss: [[inf]]\n",
      "iteration: 983, weight: [[1.12264865e+218]], train loss: [[inf]]\n",
      "iteration: 984, weight: [[-1.86886935e+218]], train loss: [[inf]]\n",
      "iteration: 985, weight: [[3.11110038e+218]], train loss: [[inf]]\n",
      "iteration: 986, weight: [[-5.17903812e+218]], train loss: [[inf]]\n",
      "iteration: 987, weight: [[8.62152696e+218]], train loss: [[inf]]\n",
      "iteration: 988, weight: [[-1.43522263e+219]], train loss: [[inf]]\n",
      "iteration: 989, weight: [[2.38921018e+219]], train loss: [[inf]]\n",
      "iteration: 990, weight: [[-3.97730998e+219]], train loss: [[inf]]\n",
      "iteration: 991, weight: [[6.62101427e+219]], train loss: [[inf]]\n",
      "iteration: 992, weight: [[-1.10219797e+220]], train loss: [[inf]]\n",
      "iteration: 993, weight: [[1.83482518e+220]], train loss: [[inf]]\n",
      "iteration: 994, weight: [[-3.05442717e+220]], train loss: [[inf]]\n",
      "iteration: 995, weight: [[5.08469441e+220]], train loss: [[inf]]\n",
      "iteration: 996, weight: [[-8.46447332e+220]], train loss: [[inf]]\n",
      "iteration: 997, weight: [[1.40907797e+221]], train loss: [[inf]]\n",
      "iteration: 998, weight: [[-2.34568725e+221]], train loss: [[inf]]\n",
      "iteration: 999, weight: [[3.9048575e+221]], train loss: [[inf]]\n",
      "iteration: 1000, weight: [[-6.50040287e+221]], train loss: [[inf]]\n",
      "iteration: 1, weight: [[1.17530448]], train loss: [[6.6198051]]\n",
      "iteration: 2, weight: [[1.76268943]], train loss: [[3.6292825]]\n",
      "iteration: 3, weight: [[2.19355412]], train loss: [[2.02018169]]\n",
      "iteration: 4, weight: [[2.50960644]], train loss: [[1.15437801]]\n",
      "iteration: 5, weight: [[2.74144041]], train loss: [[0.68851783]]\n",
      "iteration: 6, weight: [[2.91149766]], train loss: [[0.43785395]]\n",
      "iteration: 7, weight: [[3.03623981]], train loss: [[0.30298005]]\n",
      "iteration: 8, weight: [[3.12774197]], train loss: [[0.23040889]]\n",
      "iteration: 9, weight: [[3.19486157]], train loss: [[0.19136075]]\n",
      "iteration: 10, weight: [[3.24409584]], train loss: [[0.17035025]]\n",
      "iteration: 11, weight: [[3.28021067]], train loss: [[0.1590452]]\n",
      "iteration: 12, weight: [[3.306702]], train loss: [[0.15296233]]\n",
      "iteration: 13, weight: [[3.32613418]], train loss: [[0.14968933]]\n",
      "iteration: 14, weight: [[3.34038829]], train loss: [[0.14792825]]\n",
      "iteration: 15, weight: [[3.3508441]], train loss: [[0.14698066]]\n",
      "iteration: 16, weight: [[3.35851376]], train loss: [[0.1464708]]\n",
      "iteration: 17, weight: [[3.36413969]], train loss: [[0.14619646]]\n",
      "iteration: 18, weight: [[3.36826647]], train loss: [[0.14604885]]\n",
      "iteration: 19, weight: [[3.3712936]], train loss: [[0.14596942]]\n",
      "iteration: 20, weight: [[3.37351409]], train loss: [[0.14592668]]\n",
      "iteration: 21, weight: [[3.37514288]], train loss: [[0.14590369]]\n",
      "iteration: 22, weight: [[3.37633765]], train loss: [[0.14589132]]\n",
      "iteration: 23, weight: [[3.37721405]], train loss: [[0.14588466]]\n",
      "iteration: 24, weight: [[3.37785692]], train loss: [[0.14588108]]\n",
      "iteration: 25, weight: [[3.37832848]], train loss: [[0.14587915]]\n",
      "iteration: 26, weight: [[3.37867438]], train loss: [[0.14587811]]\n",
      "iteration: 27, weight: [[3.37892812]], train loss: [[0.14587755]]\n",
      "iteration: 28, weight: [[3.37911424]], train loss: [[0.14587725]]\n",
      "iteration: 29, weight: [[3.37925076]], train loss: [[0.14587709]]\n",
      "iteration: 30, weight: [[3.37935091]], train loss: [[0.145877]]\n",
      "iteration: 31, weight: [[3.37942437]], train loss: [[0.14587696]]\n",
      "iteration: 32, weight: [[3.37947825]], train loss: [[0.14587693]]\n",
      "iteration: 33, weight: [[3.37951778]], train loss: [[0.14587692]]\n",
      "iteration: 34, weight: [[3.37954677]], train loss: [[0.14587691]]\n",
      "iteration: 35, weight: [[3.37956804]], train loss: [[0.14587691]]\n",
      "iteration: 36, weight: [[3.37958364]], train loss: [[0.14587691]]\n",
      "iteration: 37, weight: [[3.37959508]], train loss: [[0.1458769]]\n",
      "iteration: 38, weight: [[3.37960348]], train loss: [[0.1458769]]\n",
      "iteration: 39, weight: [[3.37960963]], train loss: [[0.1458769]]\n",
      "iteration: 40, weight: [[3.37961415]], train loss: [[0.1458769]]\n",
      "iteration: 41, weight: [[3.37961746]], train loss: [[0.1458769]]\n",
      "iteration: 42, weight: [[3.37961989]], train loss: [[0.1458769]]\n",
      "iteration: 43, weight: [[3.37962167]], train loss: [[0.1458769]]\n",
      "iteration: 44, weight: [[3.37962298]], train loss: [[0.1458769]]\n",
      "iteration: 45, weight: [[3.37962394]], train loss: [[0.1458769]]\n",
      "iteration: 46, weight: [[3.37962465]], train loss: [[0.1458769]]\n",
      "iteration: 47, weight: [[3.37962516]], train loss: [[0.1458769]]\n",
      "iteration: 48, weight: [[3.37962554]], train loss: [[0.1458769]]\n",
      "iteration: 49, weight: [[3.37962582]], train loss: [[0.1458769]]\n",
      "iteration: 50, weight: [[3.37962602]], train loss: [[0.1458769]]\n",
      "iteration: 51, weight: [[3.37962617]], train loss: [[0.1458769]]\n",
      "iteration: 52, weight: [[3.37962628]], train loss: [[0.1458769]]\n",
      "iteration: 53, weight: [[3.37962636]], train loss: [[0.1458769]]\n",
      "iteration: 54, weight: [[3.37962642]], train loss: [[0.1458769]]\n",
      "iteration: 55, weight: [[3.37962646]], train loss: [[0.1458769]]\n",
      "iteration: 56, weight: [[3.37962649]], train loss: [[0.1458769]]\n",
      "iteration: 57, weight: [[3.37962652]], train loss: [[0.1458769]]\n",
      "iteration: 58, weight: [[3.37962654]], train loss: [[0.1458769]]\n",
      "iteration: 59, weight: [[3.37962655]], train loss: [[0.1458769]]\n",
      "iteration: 60, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 61, weight: [[3.37962656]], train loss: [[0.1458769]]\n",
      "iteration: 62, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 63, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 64, weight: [[3.37962657]], train loss: [[0.1458769]]\n",
      "iteration: 65, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 66, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 67, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 68, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 69, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 70, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 71, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 72, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 73, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 74, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 75, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 76, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 77, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 78, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 79, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 80, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 81, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 82, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 83, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 84, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 85, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 86, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 87, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 88, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 89, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 90, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 91, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 92, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 93, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 94, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 95, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 96, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 97, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 98, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 99, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 100, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 101, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 102, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 103, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 104, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 105, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 106, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 107, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 108, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 109, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 110, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 111, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 112, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 113, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 114, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 115, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 116, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 117, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 118, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 119, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 120, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 121, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 122, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 123, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 124, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 125, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 126, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 127, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 128, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 129, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 130, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 131, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 132, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 133, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 134, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 135, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 136, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 137, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 138, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 139, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 140, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 141, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 142, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 143, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 144, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 145, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 146, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 147, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 148, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 149, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 150, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 151, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 152, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 153, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 154, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 155, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 156, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 157, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 158, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 159, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 160, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 161, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 162, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 163, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 164, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 165, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 166, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 167, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 168, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 169, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 170, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 171, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 172, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 173, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 174, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 175, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 176, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 177, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 178, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 179, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 180, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 181, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 182, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 183, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 184, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 185, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 186, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 187, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 188, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 189, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 190, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 191, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 192, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 193, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 194, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 195, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 196, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 197, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 198, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 199, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 200, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 201, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 202, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 203, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 204, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 205, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 206, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 207, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 208, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 209, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 210, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 211, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 212, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 213, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 214, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 215, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 216, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 217, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 218, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 219, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 220, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 221, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 222, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 223, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 224, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 225, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 226, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 227, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 228, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 229, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 230, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 231, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 232, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 233, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 234, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 235, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 236, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 237, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 238, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 239, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 240, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 241, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 242, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 243, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 244, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 245, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 246, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 247, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 248, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 249, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 250, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 251, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 252, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 253, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 254, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 255, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 256, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 257, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 258, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 259, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 260, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 261, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 262, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 263, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 264, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 265, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 266, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 267, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 268, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 269, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 270, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 271, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 272, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 273, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 274, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 275, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 276, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 277, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 278, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 279, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 280, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 281, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 282, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 283, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 284, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 285, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 286, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 287, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 288, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 289, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 290, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 291, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 292, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 293, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 294, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 295, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 296, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 297, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 298, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 299, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 300, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 301, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 302, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 303, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 304, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 305, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 306, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 307, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 308, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 309, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 310, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 311, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 312, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 313, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 314, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 315, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 316, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 317, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 318, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 319, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 320, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 321, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 322, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 323, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 324, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 325, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 326, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 327, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 328, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 329, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 330, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 331, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 332, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 333, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 334, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 335, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 336, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 337, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 338, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 339, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 340, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 341, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 342, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 343, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 344, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 345, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 346, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 347, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 348, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 349, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 350, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 351, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 352, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 353, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 354, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 355, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 356, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 357, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 358, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 359, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 360, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 361, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 362, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 363, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 364, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 365, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 366, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 367, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 368, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 369, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 370, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 371, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 372, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 373, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 374, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 375, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 376, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 377, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 378, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 379, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 380, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 381, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 382, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 383, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 384, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 385, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 386, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 387, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 388, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 389, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 390, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 391, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 392, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 393, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 394, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 395, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 396, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 397, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 398, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 399, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 400, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 401, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 402, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 403, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 404, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 405, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 406, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 407, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 408, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 409, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 410, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 411, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 412, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 413, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 414, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 415, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 416, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 417, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 418, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 419, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 420, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 421, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 422, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 423, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 424, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 425, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 426, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 427, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 428, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 429, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 430, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 431, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 432, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 433, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 434, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 435, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 436, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 437, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 438, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 439, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 440, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 441, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 442, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 443, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 444, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 445, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 446, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 447, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 448, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 449, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 450, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 451, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 452, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 453, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 454, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 455, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 456, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 457, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 458, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 459, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 460, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 461, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 462, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 463, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 464, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 465, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 466, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 467, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 468, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 469, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 470, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 471, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 472, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 473, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 474, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 475, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 476, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 477, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 478, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 479, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 480, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 481, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 482, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 483, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 484, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 485, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 486, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 487, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 488, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 489, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 490, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 491, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 492, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 493, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 494, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 495, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 496, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 497, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 498, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 499, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 500, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 501, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 502, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 503, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 504, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 505, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 506, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 507, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 508, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 509, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 510, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 511, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 512, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 513, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 514, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 515, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 516, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 517, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 518, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 519, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 520, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 521, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 522, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 523, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 524, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 525, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 526, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 527, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 528, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 529, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 530, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 531, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 532, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 533, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 534, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 535, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 536, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 537, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 538, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 539, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 540, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 541, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 542, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 543, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 544, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 545, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 546, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 547, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 548, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 549, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 550, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 551, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 552, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 553, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 554, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 555, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 556, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 557, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 558, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 559, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 560, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 561, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 562, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 563, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 564, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 565, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 566, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 567, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 568, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 569, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 570, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 571, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 572, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 573, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 574, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 575, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 576, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 577, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 578, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 579, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 580, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 581, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 582, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 583, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 584, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 585, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 586, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 587, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 588, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 589, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 590, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 591, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 592, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 593, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 594, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 595, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 596, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 597, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 598, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 599, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 600, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 601, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 602, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 603, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 604, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 605, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 606, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 607, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 608, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 609, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 610, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 611, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 612, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 613, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 614, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 615, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 616, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 617, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 618, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 619, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 620, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 621, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 622, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 623, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 624, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 625, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 626, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 627, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 628, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 629, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 630, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 631, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 632, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 633, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 634, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 635, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 636, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 637, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 638, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 639, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 640, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 641, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 642, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 643, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 644, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 645, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 646, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 647, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 648, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 649, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 650, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 651, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 652, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 653, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 654, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 655, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 656, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 657, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 658, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 659, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 660, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 661, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 662, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 663, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 664, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 665, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 666, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 667, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 668, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 669, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 670, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 671, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 672, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 673, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 674, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 675, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 676, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 677, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 678, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 679, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 680, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 681, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 682, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 683, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 684, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 685, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 686, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 687, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 688, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 689, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 690, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 691, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 692, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 693, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 694, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 695, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 696, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 697, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 698, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 699, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 700, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 701, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 702, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 703, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 704, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 705, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 706, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 707, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 708, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 709, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 710, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 711, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 712, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 713, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 714, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 715, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 716, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 717, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 718, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 719, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 720, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 721, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 722, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 723, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 724, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 725, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 726, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 727, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 728, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 729, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 730, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 731, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 732, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 733, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 734, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 735, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 736, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 737, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 738, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 739, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 740, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 741, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 742, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 743, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 744, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 745, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 746, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 747, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 748, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 749, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 750, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 751, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 752, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 753, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 754, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 755, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 756, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 757, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 758, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 759, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 760, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 761, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 762, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 763, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 764, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 765, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 766, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 767, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 768, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 769, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 770, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 771, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 772, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 773, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 774, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 775, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 776, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 777, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 778, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 779, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 780, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 781, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 782, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 783, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 784, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 785, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 786, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 787, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 788, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 789, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 790, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 791, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 792, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 793, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 794, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 795, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 796, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 797, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 798, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 799, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 800, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 801, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 802, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 803, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 804, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 805, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 806, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 807, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 808, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 809, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 810, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 811, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 812, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 813, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 814, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 815, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 816, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 817, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 818, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 819, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 820, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 821, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 822, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 823, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 824, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 825, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 826, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 827, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 828, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 829, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 830, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 831, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 832, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 833, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 834, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 835, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 836, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 837, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 838, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 839, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 840, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 841, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 842, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 843, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 844, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 845, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 846, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 847, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 848, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 849, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 850, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 851, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 852, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 853, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 854, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 855, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 856, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 857, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 858, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 859, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 860, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 861, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 862, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 863, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 864, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 865, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 866, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 867, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 868, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 869, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 870, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 871, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 872, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 873, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 874, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 875, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 876, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 877, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 878, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 879, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 880, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 881, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 882, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 883, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 884, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 885, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 886, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 887, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 888, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 889, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 890, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 891, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 892, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 893, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 894, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 895, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 896, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 897, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 898, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 899, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 900, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 901, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 902, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 903, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 904, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 905, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 906, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 907, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 908, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 909, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 910, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 911, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 912, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 913, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 914, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 915, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 916, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 917, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 918, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 919, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 920, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 921, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 922, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 923, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 924, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 925, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 926, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 927, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 928, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 929, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 930, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 931, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 932, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 933, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 934, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 935, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 936, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 937, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 938, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 939, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 940, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 941, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 942, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 943, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 944, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 945, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 946, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 947, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 948, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 949, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 950, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 951, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 952, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 953, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 954, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 955, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 956, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 957, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 958, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 959, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 960, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 961, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 962, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 963, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 964, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 965, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 966, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 967, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 968, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 969, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 970, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 971, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 972, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 973, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 974, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 975, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 976, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 977, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 978, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 979, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 980, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 981, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 982, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 983, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 984, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 985, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 986, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 987, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 988, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 989, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 990, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 991, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 992, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 993, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 994, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 995, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 996, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 997, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 998, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 999, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "iteration: 1000, weight: [[3.37962658]], train loss: [[0.1458769]]\n",
      "The time used for learning rate = 0.1 is 1.8883910179138184\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "epochs = 1000 #Number of epochs for updating weights\n",
    "\n",
    "def compute_gradient_descent(w, eta):\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    d_train = X.size\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_predict = X.dot(w)\n",
    "        plt.plot(X, y_predict, \"r-\")\n",
    "        gradient = 2/d_train * (w.dot(X.T) - y.T).dot(X)\n",
    "        w = w - eta * gradient\n",
    "        train_loss = 1/d_train * (w.dot(X.T) - y.T).dot((w.dot(X.T) - y.T).T)\n",
    "        print('iteration: {}, weight: {}, train loss: {}'.format(i+1, w, train_loss))\n",
    "    \n",
    "    plt.xlabel(\"Feature\", fontsize=12)\n",
    "    plt.title(\"learning rate = {}\".format(eta), fontsize=12)\n",
    "    plt.axis([0, 2, 0, 10])\n",
    "np.random.seed(42)\n",
    "w = np.random.rand(1)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 4, 1)\n",
    "compute_gradient_descent(w, 0.02)\n",
    "plt.subplot(1, 4, 3)\n",
    "compute_gradient_descent(w, 0.5)\n",
    "plt.subplot(1, 4, 4)\n",
    "compute_gradient_descent(w, 1)\n",
    "plt.subplot(1, 4, 2)\n",
    "t1 = time.time()\n",
    "compute_gradient_descent(w, 0.1)\n",
    "t2 = time.time()\n",
    "print('The time used for learning rate = 0.1 is {}'.format(t2 - t1))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, weight: [3.28661603], train loss: [[0.157403]]\n",
      "epoch: 2, weight: [3.36361807], train loss: [[0.14621835]]\n",
      "epoch: 3, weight: [3.55675448], train loss: [[0.18767839]]\n",
      "epoch: 4, weight: [3.30512999], train loss: [[0.15327109]]\n",
      "epoch: 5, weight: [3.33394131], train loss: [[0.14865771]]\n",
      "epoch: 6, weight: [3.43642434], train loss: [[0.15017504]]\n",
      "epoch: 7, weight: [3.20381255], train loss: [[0.18706055]]\n",
      "epoch: 8, weight: [3.34384832], train loss: [[0.14758242]]\n",
      "epoch: 9, weight: [3.45324229], train loss: [[0.15309726]]\n",
      "epoch: 10, weight: [3.28204163], train loss: [[0.15856462]]\n",
      "epoch: 11, weight: [3.34402581], train loss: [[0.14756554]]\n",
      "epoch: 12, weight: [3.40952721], train loss: [[0.14706809]]\n",
      "epoch: 13, weight: [3.42360289], train loss: [[0.14845355]]\n",
      "epoch: 14, weight: [3.36338332], train loss: [[0.14622843]]\n",
      "epoch: 15, weight: [3.2695241], train loss: [[0.16202837]]\n",
      "epoch: 16, weight: [3.48618164], train loss: [[0.16100436]]\n",
      "epoch: 17, weight: [3.30925027], train loss: [[0.15247579]]\n",
      "epoch: 18, weight: [3.6466248], train loss: [[0.24085741]]\n",
      "epoch: 19, weight: [3.1692375], train loss: [[0.20485139]]\n",
      "epoch: 20, weight: [3.4377633], train loss: [[0.15038008]]\n",
      "epoch: 21, weight: [3.39560123], train loss: [[0.1462169]]\n",
      "epoch: 22, weight: [3.18603202], train loss: [[0.19581179]]\n",
      "epoch: 23, weight: [3.53163245], train loss: [[0.17666185]]\n",
      "epoch: 24, weight: [3.60878415], train loss: [[0.21584277]]\n",
      "epoch: 25, weight: [3.43518197], train loss: [[0.14998906]]\n",
      "epoch: 26, weight: [3.42125093], train loss: [[0.14818531]]\n",
      "epoch: 27, weight: [3.43670264], train loss: [[0.15021726]]\n",
      "epoch: 28, weight: [3.25707006], train loss: [[0.16588891]]\n",
      "epoch: 29, weight: [3.4834077], train loss: [[0.16022699]]\n",
      "epoch: 30, weight: [3.47018129], train loss: [[0.15680237]]\n",
      "epoch: 31, weight: [3.44870049], train loss: [[0.15223381]]\n",
      "epoch: 32, weight: [3.3884549], train loss: [[0.14598075]]\n",
      "epoch: 33, weight: [3.3324003], train loss: [[0.14884847]]\n",
      "epoch: 34, weight: [3.44174826], train loss: [[0.15101857]]\n",
      "epoch: 35, weight: [3.44971944], train loss: [[0.15242274]]\n",
      "epoch: 36, weight: [3.66356], train loss: [[0.25328841]]\n",
      "epoch: 37, weight: [3.44150269], train loss: [[0.150978]]\n",
      "epoch: 38, weight: [3.53530048], train loss: [[0.17816551]]\n",
      "epoch: 39, weight: [3.26908462], train loss: [[0.16215757]]\n",
      "epoch: 40, weight: [3.49356479], train loss: [[0.16317333]]\n",
      "epoch: 41, weight: [3.3431336], train loss: [[0.14765124]]\n",
      "epoch: 42, weight: [3.15530155], train loss: [[0.21292296]]\n",
      "epoch: 43, weight: [3.39763629], train loss: [[0.14630905]]\n",
      "epoch: 44, weight: [3.48577866], train loss: [[0.16089016]]\n",
      "epoch: 45, weight: [3.31922115], train loss: [[0.1507384]]\n",
      "epoch: 46, weight: [3.27865082], train loss: [[0.15946166]]\n",
      "epoch: 47, weight: [3.48185004], train loss: [[0.15979946]]\n",
      "epoch: 48, weight: [3.33071081], train loss: [[0.14906488]]\n",
      "epoch: 49, weight: [3.17230744], train loss: [[0.20314287]]\n",
      "epoch: 50, weight: [3.28039531], train loss: [[0.15899633]]\n",
      "epoch: 51, weight: [3.49861378], train loss: [[0.16474023]]\n",
      "epoch: 52, weight: [3.59138277], train loss: [[0.20562031]]\n",
      "epoch: 53, weight: [3.33719924], train loss: [[0.14827524]]\n",
      "epoch: 54, weight: [3.41555185], train loss: [[0.14759647]]\n",
      "epoch: 55, weight: [3.32464436], train loss: [[0.14990465]]\n",
      "epoch: 56, weight: [3.3468632], train loss: [[0.1473071]]\n",
      "epoch: 57, weight: [3.52073522], train loss: [[0.17240615]]\n",
      "epoch: 58, weight: [3.46087865], train loss: [[0.15467293]]\n",
      "epoch: 59, weight: [3.42335087], train loss: [[0.1484241]]\n",
      "epoch: 60, weight: [3.49309136], train loss: [[0.1630299]]\n",
      "epoch: 61, weight: [3.25215015], train loss: [[0.16752788]]\n",
      "epoch: 62, weight: [3.20313343], train loss: [[0.18737933]]\n",
      "epoch: 63, weight: [3.42008785], train loss: [[0.14805811]]\n",
      "epoch: 64, weight: [3.29618663], train loss: [[0.15515301]]\n",
      "epoch: 65, weight: [3.33938711], train loss: [[0.14803426]]\n",
      "epoch: 66, weight: [3.25294842], train loss: [[0.16725757]]\n",
      "epoch: 67, weight: [3.41795592], train loss: [[0.14783431]]\n",
      "epoch: 68, weight: [3.66580446], train loss: [[0.25499327]]\n",
      "epoch: 69, weight: [3.43605158], train loss: [[0.15011881]]\n",
      "epoch: 70, weight: [3.27012935], train loss: [[0.16185128]]\n",
      "epoch: 71, weight: [3.50580762], train loss: [[0.16709009]]\n",
      "epoch: 72, weight: [3.34579114], train loss: [[0.14740222]]\n",
      "epoch: 73, weight: [3.29694522], train loss: [[0.15498511]]\n",
      "epoch: 74, weight: [3.49370766], train loss: [[0.16321674]]\n",
      "epoch: 75, weight: [3.3082491], train loss: [[0.15266488]]\n",
      "epoch: 76, weight: [3.26824995], train loss: [[0.16240436]]\n",
      "epoch: 77, weight: [3.25794764], train loss: [[0.16560334]]\n",
      "epoch: 78, weight: [3.44742011], train loss: [[0.15200033]]\n",
      "epoch: 79, weight: [3.37513547], train loss: [[0.14590378]]\n",
      "epoch: 80, weight: [3.27279254], train loss: [[0.16108368]]\n",
      "epoch: 81, weight: [3.19175781], train loss: [[0.19290171]]\n",
      "epoch: 82, weight: [3.43479482], train loss: [[0.14993195]]\n",
      "epoch: 83, weight: [3.40221121], train loss: [[0.14655649]]\n",
      "epoch: 84, weight: [3.55944644], train loss: [[0.18895863]]\n",
      "epoch: 85, weight: [3.27299567], train loss: [[0.1610259]]\n",
      "epoch: 86, weight: [3.44738688], train loss: [[0.15199432]]\n",
      "epoch: 87, weight: [3.25295169], train loss: [[0.16725647]]\n",
      "epoch: 88, weight: [3.35691686], train loss: [[0.14656404]]\n",
      "epoch: 89, weight: [3.34836462], train loss: [[0.14717902]]\n",
      "epoch: 90, weight: [3.32366344], train loss: [[0.15004965]]\n",
      "epoch: 91, weight: [3.30390166], train loss: [[0.15351694]]\n",
      "epoch: 92, weight: [3.33927402], train loss: [[0.14804641]]\n",
      "epoch: 93, weight: [3.32895323], train loss: [[0.14929809]]\n",
      "epoch: 94, weight: [3.25918571], train loss: [[0.16520395]]\n",
      "epoch: 95, weight: [3.50884185], train loss: [[0.16812257]]\n",
      "epoch: 96, weight: [3.46447493], train loss: [[0.1554688]]\n",
      "epoch: 97, weight: [3.55238374], train loss: [[0.18564089]]\n",
      "epoch: 98, weight: [3.3032046], train loss: [[0.15365824]]\n",
      "epoch: 99, weight: [3.42040739], train loss: [[0.1480927]]\n",
      "epoch: 100, weight: [3.53023249], train loss: [[0.17609741]]\n",
      "epoch: 101, weight: [3.22461699], train loss: [[0.17789053]]\n",
      "epoch: 102, weight: [3.32554405], train loss: [[0.14977392]]\n",
      "epoch: 103, weight: [3.40269779], train loss: [[0.14658609]]\n",
      "epoch: 104, weight: [3.37429867], train loss: [[0.14591472]]\n",
      "epoch: 105, weight: [3.38905079], train loss: [[0.14599524]]\n",
      "epoch: 106, weight: [3.53471478], train loss: [[0.17792301]]\n",
      "epoch: 107, weight: [3.36426299], train loss: [[0.14619139]]\n",
      "epoch: 108, weight: [3.15416861], train loss: [[0.2136019]]\n",
      "epoch: 109, weight: [3.4738106], train loss: [[0.15769567]]\n",
      "epoch: 110, weight: [3.18224062], train loss: [[0.19778681]]\n",
      "epoch: 111, weight: [3.13819129], train loss: [[0.22354082]]\n",
      "epoch: 112, weight: [3.38648156], train loss: [[0.14593951]]\n",
      "epoch: 113, weight: [3.56272916], train loss: [[0.19054595]]\n",
      "epoch: 114, weight: [3.41936544], train loss: [[0.14798092]]\n",
      "epoch: 115, weight: [3.4154927], train loss: [[0.14759081]]\n",
      "epoch: 116, weight: [3.18740761], train loss: [[0.19510468]]\n",
      "epoch: 117, weight: [3.40922614], train loss: [[0.14704422]]\n",
      "epoch: 118, weight: [3.26785739], train loss: [[0.16252107]]\n",
      "epoch: 119, weight: [3.47299473], train loss: [[0.1574918]]\n",
      "epoch: 120, weight: [3.56290237], train loss: [[0.1906305]]\n",
      "epoch: 121, weight: [3.4990838], train loss: [[0.16488955]]\n",
      "epoch: 122, weight: [3.41233464], train loss: [[0.14730227]]\n",
      "epoch: 123, weight: [3.33586179], train loss: [[0.14842883]]\n",
      "epoch: 124, weight: [3.42225327], train loss: [[0.14829783]]\n",
      "epoch: 125, weight: [3.21798792], train loss: [[0.18068724]]\n",
      "epoch: 126, weight: [3.61484028], train loss: [[0.21958972]]\n",
      "epoch: 127, weight: [3.36124467], train loss: [[0.1463271]]\n",
      "epoch: 128, weight: [3.40770756], train loss: [[0.14692751]]\n",
      "epoch: 129, weight: [3.19313193], train loss: [[0.19221632]]\n",
      "epoch: 130, weight: [3.38972856], train loss: [[0.14601287]]\n",
      "epoch: 131, weight: [3.31982373], train loss: [[0.15064189]]\n",
      "epoch: 132, weight: [3.36188825], train loss: [[0.14629612]]\n",
      "epoch: 133, weight: [3.41392707], train loss: [[0.14744444]]\n",
      "epoch: 134, weight: [3.44191868], train loss: [[0.15104682]]\n",
      "epoch: 135, weight: [3.39835715], train loss: [[0.14634434]]\n",
      "epoch: 136, weight: [3.16979559], train loss: [[0.20453893]]\n",
      "epoch: 137, weight: [3.27710089], train loss: [[0.15988191]]\n",
      "epoch: 138, weight: [3.63427785], train loss: [[0.23227605]]\n",
      "epoch: 139, weight: [3.29241529], train loss: [[0.15601049]]\n",
      "epoch: 140, weight: [3.61634536], train loss: [[0.22053608]]\n",
      "epoch: 141, weight: [3.21058279], train loss: [[0.18394982]]\n",
      "epoch: 142, weight: [3.34247547], train loss: [[0.14771582]]\n",
      "epoch: 143, weight: [3.40289041], train loss: [[0.14659798]]\n",
      "epoch: 144, weight: [3.52732683], train loss: [[0.17494257]]\n",
      "epoch: 145, weight: [3.2490599], train loss: [[0.16859032]]\n",
      "epoch: 146, weight: [3.41346267], train loss: [[0.14740228]]\n",
      "epoch: 147, weight: [3.38244108], train loss: [[0.14588746]]\n",
      "epoch: 148, weight: [3.36296871], train loss: [[0.14624661]]\n",
      "epoch: 149, weight: [3.12243711], train loss: [[0.23400698]]\n",
      "epoch: 150, weight: [3.40987621], train loss: [[0.14709606]]\n",
      "epoch: 151, weight: [3.49589937], train loss: [[0.1638894]]\n",
      "epoch: 152, weight: [3.38118768], train loss: [[0.14588015]]\n",
      "epoch: 153, weight: [3.24439713], train loss: [[0.17024156]]\n",
      "epoch: 154, weight: [3.36862874], train loss: [[0.14603805]]\n",
      "epoch: 155, weight: [3.40823406], train loss: [[0.14696728]]\n",
      "epoch: 156, weight: [3.14153455], train loss: [[0.22140482]]\n",
      "epoch: 157, weight: [3.19674761], train loss: [[0.19043692]]\n",
      "epoch: 158, weight: [3.19282969], train loss: [[0.19236664]]\n",
      "epoch: 159, weight: [3.36267063], train loss: [[0.14625996]]\n",
      "epoch: 160, weight: [3.42257141], train loss: [[0.1483341]]\n",
      "epoch: 161, weight: [3.51351127], train loss: [[0.16975939]]\n",
      "epoch: 162, weight: [3.43172662], train loss: [[0.14949345]]\n",
      "epoch: 163, weight: [3.41704889], train loss: [[0.14774276]]\n",
      "epoch: 164, weight: [3.31128706], train loss: [[0.15209936]]\n",
      "epoch: 165, weight: [3.36052329], train loss: [[0.14636313]]\n",
      "epoch: 166, weight: [3.21310603], train loss: [[0.18282172]]\n",
      "epoch: 167, weight: [3.37488965], train loss: [[0.1459068]]\n",
      "epoch: 168, weight: [3.37414691], train loss: [[0.14591691]]\n",
      "epoch: 169, weight: [3.10448814], train loss: [[0.24673721]]\n",
      "epoch: 170, weight: [3.48784896], train loss: [[0.16148148]]\n",
      "epoch: 171, weight: [3.48376185], train loss: [[0.16032509]]\n",
      "epoch: 172, weight: [3.40636011], train loss: [[0.14682911]]\n",
      "epoch: 173, weight: [3.47855357], train loss: [[0.158916]]\n",
      "epoch: 174, weight: [3.2714502], train loss: [[0.16146821]]\n",
      "epoch: 175, weight: [3.45212321], train loss: [[0.15287941]]\n",
      "epoch: 176, weight: [3.13481976], train loss: [[0.22572504]]\n",
      "epoch: 177, weight: [3.24060372], train loss: [[0.17162767]]\n",
      "epoch: 178, weight: [3.3165393], train loss: [[0.15117966]]\n",
      "epoch: 179, weight: [3.33415821], train loss: [[0.14863136]]\n",
      "epoch: 180, weight: [3.20804635], train loss: [[0.18510094]]\n",
      "epoch: 181, weight: [3.21267744], train loss: [[0.18301214]]\n",
      "epoch: 182, weight: [3.52406038], train loss: [[0.17367118]]\n",
      "epoch: 183, weight: [3.25428201], train loss: [[0.16680978]]\n",
      "epoch: 184, weight: [3.24605616], train loss: [[0.16964741]]\n",
      "epoch: 185, weight: [3.43090047], train loss: [[0.14937966]]\n",
      "epoch: 186, weight: [3.37290515], train loss: [[0.1459371]]\n",
      "epoch: 187, weight: [3.63765674], train loss: [[0.23458407]]\n",
      "epoch: 188, weight: [3.43264033], train loss: [[0.14962141]]\n",
      "epoch: 189, weight: [3.24780775], train loss: [[0.16902806]]\n",
      "epoch: 190, weight: [3.12152468], train loss: [[0.23463341]]\n",
      "epoch: 191, weight: [3.43934361], train loss: [[0.15062822]]\n",
      "epoch: 192, weight: [3.41663126], train loss: [[0.14770135]]\n",
      "epoch: 193, weight: [3.26266289], train loss: [[0.1641041]]\n",
      "epoch: 194, weight: [3.38226919], train loss: [[0.14588621]]\n",
      "epoch: 195, weight: [3.19466902], train loss: [[0.1914556]]\n",
      "epoch: 196, weight: [3.3304938], train loss: [[0.14909323]]\n",
      "epoch: 197, weight: [3.2983681], train loss: [[0.15467432]]\n",
      "epoch: 198, weight: [3.34410597], train loss: [[0.14755795]]\n",
      "epoch: 199, weight: [3.19724203], train loss: [[0.1901963]]\n",
      "epoch: 200, weight: [3.44122356], train loss: [[0.15093208]]\n",
      "epoch: 201, weight: [3.4311284], train loss: [[0.14941087]]\n",
      "epoch: 202, weight: [3.04506652], train loss: [[0.29500723]]\n",
      "epoch: 203, weight: [3.44372815], train loss: [[0.15135154]]\n",
      "epoch: 204, weight: [3.41205211], train loss: [[0.14727775]]\n",
      "epoch: 205, weight: [3.48353683], train loss: [[0.16026272]]\n",
      "epoch: 206, weight: [3.18346423], train loss: [[0.19714522]]\n",
      "epoch: 207, weight: [3.12577429], train loss: [[0.23173474]]\n",
      "epoch: 208, weight: [3.27726143], train loss: [[0.15983808]]\n",
      "epoch: 209, weight: [3.33673868], train loss: [[0.14832759]]\n",
      "epoch: 210, weight: [3.43026804], train loss: [[0.14929379]]\n",
      "epoch: 211, weight: [3.42024897], train loss: [[0.14807552]]\n",
      "epoch: 212, weight: [3.59817206], train loss: [[0.20951269]]\n",
      "epoch: 213, weight: [3.47213691], train loss: [[0.15727935]]\n",
      "epoch: 214, weight: [3.48937646], train loss: [[0.16192509]]\n",
      "epoch: 215, weight: [3.33238599], train loss: [[0.14885027]]\n",
      "epoch: 216, weight: [3.26784859], train loss: [[0.16252369]]\n",
      "epoch: 217, weight: [3.3235167], train loss: [[0.15007156]]\n",
      "epoch: 218, weight: [3.1630803], train loss: [[0.20835377]]\n",
      "epoch: 219, weight: [3.55039777], train loss: [[0.18473191]]\n",
      "epoch: 220, weight: [3.2551113], train loss: [[0.16653371]]\n",
      "epoch: 221, weight: [3.29769613], train loss: [[0.15482042]]\n",
      "epoch: 222, weight: [3.41836798], train loss: [[0.14787662]]\n",
      "epoch: 223, weight: [3.50067357], train loss: [[0.16539897]]\n",
      "epoch: 224, weight: [3.40472964], train loss: [[0.1467165]]\n",
      "epoch: 225, weight: [3.39967564], train loss: [[0.14641246]]\n",
      "epoch: 226, weight: [3.59076122], train loss: [[0.2052701]]\n",
      "epoch: 227, weight: [3.3231733], train loss: [[0.15012306]]\n",
      "epoch: 228, weight: [3.18733321], train loss: [[0.1951428]]\n",
      "epoch: 229, weight: [3.46353476], train loss: [[0.15525741]]\n",
      "epoch: 230, weight: [3.42267008], train loss: [[0.1483454]]\n",
      "epoch: 231, weight: [3.4305686], train loss: [[0.14933447]]\n",
      "epoch: 232, weight: [3.35111181], train loss: [[0.14696023]]\n",
      "epoch: 233, weight: [3.45234806], train loss: [[0.15292291]]\n",
      "epoch: 234, weight: [3.1881654], train loss: [[0.19471731]]\n",
      "epoch: 235, weight: [3.25547389], train loss: [[0.16641357]]\n",
      "epoch: 236, weight: [3.36675522], train loss: [[0.14609764]]\n",
      "epoch: 237, weight: [3.56613869], train loss: [[0.19222499]]\n",
      "epoch: 238, weight: [3.50732739], train loss: [[0.16760417]]\n",
      "epoch: 239, weight: [3.48215784], train loss: [[0.15988343]]\n",
      "epoch: 240, weight: [3.14349571], train loss: [[0.2201657]]\n",
      "epoch: 241, weight: [3.35837147], train loss: [[0.14647883]]\n",
      "epoch: 242, weight: [3.36273437], train loss: [[0.14625708]]\n",
      "epoch: 243, weight: [3.37579219], train loss: [[0.14589649]]\n",
      "epoch: 244, weight: [3.36970618], train loss: [[0.14600803]]\n",
      "epoch: 245, weight: [3.42857268], train loss: [[0.14906884]]\n",
      "epoch: 246, weight: [3.57476053], train loss: [[0.19660907]]\n",
      "epoch: 247, weight: [3.19330937], train loss: [[0.19212818]]\n",
      "epoch: 248, weight: [3.29302494], train loss: [[0.15586931]]\n",
      "epoch: 249, weight: [3.46782984], train loss: [[0.15624233]]\n",
      "epoch: 250, weight: [3.27710985], train loss: [[0.15987946]]\n",
      "epoch: 251, weight: [3.47718514], train loss: [[0.15855776]]\n",
      "epoch: 252, weight: [3.51375402], train loss: [[0.16984607]]\n",
      "epoch: 253, weight: [3.42793411], train loss: [[0.14898609]]\n",
      "epoch: 254, weight: [3.42146009], train loss: [[0.14820857]]\n",
      "epoch: 255, weight: [3.33925351], train loss: [[0.14804861]]\n",
      "epoch: 256, weight: [3.32522225], train loss: [[0.14982043]]\n",
      "epoch: 257, weight: [3.48782651], train loss: [[0.161475]]\n",
      "epoch: 258, weight: [3.37722882], train loss: [[0.14588456]]\n",
      "epoch: 259, weight: [3.11865825], train loss: [[0.23661578]]\n",
      "epoch: 260, weight: [3.44882736], train loss: [[0.15225718]]\n",
      "epoch: 261, weight: [3.40543827], train loss: [[0.14676457]]\n",
      "epoch: 262, weight: [3.52968121], train loss: [[0.17587658]]\n",
      "epoch: 263, weight: [3.44153157], train loss: [[0.15098276]]\n",
      "epoch: 264, weight: [3.39313484], train loss: [[0.14612002]]\n",
      "epoch: 265, weight: [3.24885256], train loss: [[0.16866252]]\n",
      "epoch: 266, weight: [3.44808822], train loss: [[0.15212161]]\n",
      "epoch: 267, weight: [3.60215522], train loss: [[0.21185345]]\n",
      "epoch: 268, weight: [3.44817041], train loss: [[0.15213662]]\n",
      "epoch: 269, weight: [3.50467436], train loss: [[0.16671076]]\n",
      "epoch: 270, weight: [3.45235152], train loss: [[0.15292358]]\n",
      "epoch: 271, weight: [3.38772233], train loss: [[0.14596423]]\n",
      "epoch: 272, weight: [3.3774441], train loss: [[0.14588325]]\n",
      "epoch: 273, weight: [3.26002796], train loss: [[0.16493459]]\n",
      "epoch: 274, weight: [3.34956821], train loss: [[0.14708069]]\n",
      "epoch: 275, weight: [3.17697375], train loss: [[0.20059401]]\n",
      "epoch: 276, weight: [3.61045659], train loss: [[0.21686774]]\n",
      "epoch: 277, weight: [3.36169732], train loss: [[0.1463052]]\n",
      "epoch: 278, weight: [3.48660681], train loss: [[0.16112532]]\n",
      "epoch: 279, weight: [3.48917008], train loss: [[0.16186479]]\n",
      "epoch: 280, weight: [3.2547446], train loss: [[0.16665555]]\n",
      "epoch: 281, weight: [3.27973088], train loss: [[0.15917261]]\n",
      "epoch: 282, weight: [3.47452066], train loss: [[0.15787455]]\n",
      "epoch: 283, weight: [3.45498781], train loss: [[0.15344373]]\n",
      "epoch: 284, weight: [3.52537627], train loss: [[0.17417994]]\n",
      "epoch: 285, weight: [3.42857886], train loss: [[0.14906964]]\n",
      "epoch: 286, weight: [3.30760045], train loss: [[0.15278881]]\n",
      "epoch: 287, weight: [3.37840587], train loss: [[0.14587889]]\n",
      "epoch: 288, weight: [3.39507215], train loss: [[0.14619476]]\n",
      "epoch: 289, weight: [3.67279435], train loss: [[0.26038869]]\n",
      "epoch: 290, weight: [3.36244792], train loss: [[0.14627009]]\n",
      "epoch: 291, weight: [3.48065441], train loss: [[0.15947568]]\n",
      "epoch: 292, weight: [3.25452208], train loss: [[0.16672967]]\n",
      "epoch: 293, weight: [3.43813975], train loss: [[0.15043859]]\n",
      "epoch: 294, weight: [3.43321132], train loss: [[0.14970251]]\n",
      "epoch: 295, weight: [3.40060696], train loss: [[0.14646337]]\n",
      "epoch: 296, weight: [3.29639902], train loss: [[0.15510585]]\n",
      "epoch: 297, weight: [3.12714605], train loss: [[0.23080934]]\n",
      "epoch: 298, weight: [3.35559319], train loss: [[0.14664647]]\n",
      "epoch: 299, weight: [3.3789444], train loss: [[0.14587752]]\n",
      "epoch: 300, weight: [3.19522213], train loss: [[0.19118341]]\n",
      "epoch: 301, weight: [3.49412941], train loss: [[0.16334519]]\n",
      "epoch: 302, weight: [3.29237579], train loss: [[0.15601967]]\n",
      "epoch: 303, weight: [3.42218344], train loss: [[0.1482899]]\n",
      "epoch: 304, weight: [3.4664584], train loss: [[0.1559225]]\n",
      "epoch: 305, weight: [3.43161591], train loss: [[0.14947809]]\n",
      "epoch: 306, weight: [3.31761326], train loss: [[0.15100065]]\n",
      "epoch: 307, weight: [3.30119652], train loss: [[0.15407254]]\n",
      "epoch: 308, weight: [3.21435767], train loss: [[0.18226842]]\n",
      "epoch: 309, weight: [3.47183567], train loss: [[0.15720522]]\n",
      "epoch: 310, weight: [3.48359536], train loss: [[0.16027893]]\n",
      "epoch: 311, weight: [3.59231867], train loss: [[0.20614957]]\n",
      "epoch: 312, weight: [3.3319812], train loss: [[0.14890144]]\n",
      "epoch: 313, weight: [3.30966278], train loss: [[0.15239866]]\n",
      "epoch: 314, weight: [3.42045063], train loss: [[0.1480974]]\n",
      "epoch: 315, weight: [3.33184654], train loss: [[0.14891856]]\n",
      "epoch: 316, weight: [3.47299059], train loss: [[0.15749077]]\n",
      "epoch: 317, weight: [3.29106867], train loss: [[0.15632585]]\n",
      "epoch: 318, weight: [3.55207235], train loss: [[0.18549767]]\n",
      "epoch: 319, weight: [3.38352078], train loss: [[0.14589711]]\n",
      "epoch: 320, weight: [3.31826342], train loss: [[0.15089378]]\n",
      "epoch: 321, weight: [3.49345869], train loss: [[0.16314114]]\n",
      "epoch: 322, weight: [3.559606], train loss: [[0.18903512]]\n",
      "epoch: 323, weight: [3.36185884], train loss: [[0.14629752]]\n",
      "epoch: 324, weight: [3.21577875], train loss: [[0.18164527]]\n",
      "epoch: 325, weight: [3.21867864], train loss: [[0.18039037]]\n",
      "epoch: 326, weight: [3.3443282], train loss: [[0.14753698]]\n",
      "epoch: 327, weight: [3.46027134], train loss: [[0.15454193]]\n",
      "epoch: 328, weight: [3.55746185], train loss: [[0.18801293]]\n",
      "epoch: 329, weight: [3.4661573], train loss: [[0.15585295]]\n",
      "epoch: 330, weight: [3.30545111], train loss: [[0.15320748]]\n",
      "epoch: 331, weight: [3.28726126], train loss: [[0.15724364]]\n",
      "epoch: 332, weight: [3.39329571], train loss: [[0.14612585]]\n",
      "epoch: 333, weight: [3.26738367], train loss: [[0.16266245]]\n",
      "epoch: 334, weight: [3.35203196], train loss: [[0.14689144]]\n",
      "epoch: 335, weight: [3.31639959], train loss: [[0.15120317]]\n",
      "epoch: 336, weight: [3.48805411], train loss: [[0.1615407]]\n",
      "epoch: 337, weight: [3.2661133], train loss: [[0.16304456]]\n",
      "epoch: 338, weight: [3.48618736], train loss: [[0.16100598]]\n",
      "epoch: 339, weight: [3.39651391], train loss: [[0.14625686]]\n",
      "epoch: 340, weight: [3.38713789], train loss: [[0.14595207]]\n",
      "epoch: 341, weight: [3.40951642], train loss: [[0.14706723]]\n",
      "epoch: 342, weight: [3.18174211], train loss: [[0.19804935]]\n",
      "epoch: 343, weight: [3.25121802], train loss: [[0.16784567]]\n",
      "epoch: 344, weight: [3.09549589], train loss: [[0.25343772]]\n",
      "epoch: 345, weight: [3.34363371], train loss: [[0.14760294]]\n",
      "epoch: 346, weight: [3.52692437], train loss: [[0.17478438]]\n",
      "epoch: 347, weight: [3.29985866], train loss: [[0.15435453]]\n",
      "epoch: 348, weight: [3.30996468], train loss: [[0.1523425]]\n",
      "epoch: 349, weight: [3.47137251], train loss: [[0.1570917]]\n",
      "epoch: 350, weight: [3.5287116], train loss: [[0.17549013]]\n",
      "epoch: 351, weight: [3.31060606], train loss: [[0.15222399]]\n",
      "epoch: 352, weight: [3.29773528], train loss: [[0.15481188]]\n",
      "epoch: 353, weight: [3.40191626], train loss: [[0.14653885]]\n",
      "epoch: 354, weight: [3.34035832], train loss: [[0.14793138]]\n",
      "epoch: 355, weight: [3.27408032], train loss: [[0.16071928]]\n",
      "epoch: 356, weight: [3.17798476], train loss: [[0.20004942]]\n",
      "epoch: 357, weight: [3.30327713], train loss: [[0.15364348]]\n",
      "epoch: 358, weight: [3.07775036], train loss: [[0.26729281]]\n",
      "epoch: 359, weight: [3.33333736], train loss: [[0.14873172]]\n",
      "epoch: 360, weight: [3.54137204], train loss: [[0.18073326]]\n",
      "epoch: 361, weight: [3.34715934], train loss: [[0.14728136]]\n",
      "epoch: 362, weight: [3.28314624], train loss: [[0.15827901]]\n",
      "epoch: 363, weight: [3.40892911], train loss: [[0.14702091]]\n",
      "epoch: 364, weight: [3.4412979], train loss: [[0.15094429]]\n",
      "epoch: 365, weight: [3.39731817], train loss: [[0.14629392]]\n",
      "epoch: 366, weight: [3.42603801], train loss: [[0.14874681]]\n",
      "epoch: 367, weight: [3.28430715], train loss: [[0.15798235]]\n",
      "epoch: 368, weight: [3.31777547], train loss: [[0.15097388]]\n",
      "epoch: 369, weight: [3.3180149], train loss: [[0.1509345]]\n",
      "epoch: 370, weight: [3.21953311], train loss: [[0.18002488]]\n",
      "epoch: 371, weight: [3.23017202], train loss: [[0.17563712]]\n",
      "epoch: 372, weight: [3.43211204], train loss: [[0.14954715]]\n",
      "epoch: 373, weight: [3.32915513], train loss: [[0.14927088]]\n",
      "epoch: 374, weight: [3.36238219], train loss: [[0.1462731]]\n",
      "epoch: 375, weight: [3.32685109], train loss: [[0.14958783]]\n",
      "epoch: 376, weight: [3.59041645], train loss: [[0.20507629]]\n",
      "epoch: 377, weight: [3.33852013], train loss: [[0.14812823]]\n",
      "epoch: 378, weight: [3.22545546], train loss: [[0.17754513]]\n",
      "epoch: 379, weight: [3.38656431], train loss: [[0.14594103]]\n",
      "epoch: 380, weight: [3.18800252], train loss: [[0.19480044]]\n",
      "epoch: 381, weight: [3.527824], train loss: [[0.17513857]]\n",
      "epoch: 382, weight: [3.53465685], train loss: [[0.17789907]]\n",
      "epoch: 383, weight: [3.23484337], train loss: [[0.17380583]]\n",
      "epoch: 384, weight: [3.4447788], train loss: [[0.15153247]]\n",
      "epoch: 385, weight: [3.37742537], train loss: [[0.14588336]]\n",
      "epoch: 386, weight: [3.59351796], train loss: [[0.2068312]]\n",
      "epoch: 387, weight: [3.43608892], train loss: [[0.15012442]]\n",
      "epoch: 388, weight: [3.67557422], train loss: [[0.26257064]]\n",
      "epoch: 389, weight: [3.2961755], train loss: [[0.15515549]]\n",
      "epoch: 390, weight: [3.529697], train loss: [[0.17588289]]\n",
      "epoch: 391, weight: [3.54253853], train loss: [[0.18123783]]\n",
      "epoch: 392, weight: [3.24491857], train loss: [[0.17005403]]\n",
      "epoch: 393, weight: [3.4192959], train loss: [[0.14797356]]\n",
      "epoch: 394, weight: [3.45380955], train loss: [[0.15320897]]\n",
      "epoch: 395, weight: [3.37882819], train loss: [[0.14587775]]\n",
      "epoch: 396, weight: [3.21454487], train loss: [[0.18218602]]\n",
      "epoch: 397, weight: [3.2881305], train loss: [[0.1570307]]\n",
      "epoch: 398, weight: [3.22860232], train loss: [[0.17626554]]\n",
      "epoch: 399, weight: [3.3565], train loss: [[0.1465895]]\n",
      "epoch: 400, weight: [3.21190701], train loss: [[0.18335567]]\n",
      "epoch: 401, weight: [3.36569854], train loss: [[0.14613537]]\n",
      "epoch: 402, weight: [3.38767312], train loss: [[0.14596317]]\n",
      "epoch: 403, weight: [3.52919522], train loss: [[0.17568257]]\n",
      "epoch: 404, weight: [3.51047177], train loss: [[0.16868732]]\n",
      "epoch: 405, weight: [3.18104744], train loss: [[0.19841629]]\n",
      "epoch: 406, weight: [3.30907793], train loss: [[0.15250815]]\n",
      "epoch: 407, weight: [3.4355635], train loss: [[0.15004574]]\n",
      "epoch: 408, weight: [3.48230978], train loss: [[0.15992497]]\n",
      "epoch: 409, weight: [3.58195063], train loss: [[0.20041661]]\n",
      "epoch: 410, weight: [3.28727176], train loss: [[0.15724105]]\n",
      "epoch: 411, weight: [3.25115619], train loss: [[0.16786684]]\n",
      "epoch: 412, weight: [3.40294254], train loss: [[0.14660121]]\n",
      "epoch: 413, weight: [3.20371884], train loss: [[0.18710447]]\n",
      "epoch: 414, weight: [3.46787593], train loss: [[0.15625316]]\n",
      "epoch: 415, weight: [3.50460365], train loss: [[0.16668721]]\n",
      "epoch: 416, weight: [3.36392472], train loss: [[0.14620539]]\n",
      "epoch: 417, weight: [3.31664262], train loss: [[0.1511623]]\n",
      "epoch: 418, weight: [3.28916765], train loss: [[0.15677927]]\n",
      "epoch: 419, weight: [3.50974668], train loss: [[0.16843521]]\n",
      "epoch: 420, weight: [3.28872961], train loss: [[0.15688511]]\n",
      "epoch: 421, weight: [3.20313304], train loss: [[0.18737952]]\n",
      "epoch: 422, weight: [3.21855537], train loss: [[0.18044326]]\n",
      "epoch: 423, weight: [3.30486565], train loss: [[0.15332366]]\n",
      "epoch: 424, weight: [3.38892087], train loss: [[0.145992]]\n",
      "epoch: 425, weight: [3.39733305], train loss: [[0.14629462]]\n",
      "epoch: 426, weight: [3.31852048], train loss: [[0.15085183]]\n",
      "epoch: 427, weight: [3.39633643], train loss: [[0.14624892]]\n",
      "epoch: 428, weight: [3.36955603], train loss: [[0.14601202]]\n",
      "epoch: 429, weight: [3.13219506], train loss: [[0.22744641]]\n",
      "epoch: 430, weight: [3.32779691], train loss: [[0.14945601]]\n",
      "epoch: 431, weight: [3.4531388], train loss: [[0.15307698]]\n",
      "epoch: 432, weight: [3.27130742], train loss: [[0.1615094]]\n",
      "epoch: 433, weight: [3.36344024], train loss: [[0.14622598]]\n",
      "epoch: 434, weight: [3.2216378], train loss: [[0.17913292]]\n",
      "epoch: 435, weight: [3.24718068], train loss: [[0.16924885]]\n",
      "epoch: 436, weight: [3.29011472], train loss: [[0.15655218]]\n",
      "epoch: 437, weight: [3.66140806], train loss: [[0.25166643]]\n",
      "epoch: 438, weight: [3.47705082], train loss: [[0.15852286]]\n",
      "epoch: 439, weight: [3.53556], train loss: [[0.17827326]]\n",
      "epoch: 440, weight: [3.2377132], train loss: [[0.17270961]]\n",
      "epoch: 441, weight: [3.40238594], train loss: [[0.14656704]]\n",
      "epoch: 442, weight: [3.14622836], train loss: [[0.21845622]]\n",
      "epoch: 443, weight: [3.28555893], train loss: [[0.15766648]]\n",
      "epoch: 444, weight: [3.4154704], train loss: [[0.14758868]]\n",
      "epoch: 445, weight: [3.31725926], train loss: [[0.15105932]]\n",
      "epoch: 446, weight: [3.28001459], train loss: [[0.15909719]]\n",
      "epoch: 447, weight: [3.45644787], train loss: [[0.15373977]]\n",
      "epoch: 448, weight: [3.37434118], train loss: [[0.14591412]]\n",
      "epoch: 449, weight: [3.51746892], train loss: [[0.1711922]]\n",
      "epoch: 450, weight: [3.3751345], train loss: [[0.14590379]]\n",
      "epoch: 451, weight: [3.4749949], train loss: [[0.15799476]]\n",
      "epoch: 452, weight: [3.39765639], train loss: [[0.14631002]]\n",
      "epoch: 453, weight: [3.28971385], train loss: [[0.15664801]]\n",
      "epoch: 454, weight: [3.40795955], train loss: [[0.14694646]]\n",
      "epoch: 455, weight: [3.40557084], train loss: [[0.14677371]]\n",
      "epoch: 456, weight: [3.49219719], train loss: [[0.16276061]]\n",
      "epoch: 457, weight: [3.41567227], train loss: [[0.14760801]]\n",
      "epoch: 458, weight: [3.56790911], train loss: [[0.19310906]]\n",
      "epoch: 459, weight: [3.62792768], train loss: [[0.22802075]]\n",
      "epoch: 460, weight: [3.2524129], train loss: [[0.16743872]]\n",
      "epoch: 461, weight: [3.46384753], train loss: [[0.15532747]]\n",
      "epoch: 462, weight: [3.28547324], train loss: [[0.15768797]]\n",
      "epoch: 463, weight: [3.31985747], train loss: [[0.15063651]]\n",
      "epoch: 464, weight: [3.29896498], train loss: [[0.15454555]]\n",
      "epoch: 465, weight: [3.35250392], train loss: [[0.14685703]]\n",
      "epoch: 466, weight: [3.22884886], train loss: [[0.1761664]]\n",
      "epoch: 467, weight: [3.39702958], train loss: [[0.14628042]]\n",
      "epoch: 468, weight: [3.34982746], train loss: [[0.14706001]]\n",
      "epoch: 469, weight: [3.19783175], train loss: [[0.18991016]]\n",
      "epoch: 470, weight: [3.24374622], train loss: [[0.17047668]]\n",
      "epoch: 471, weight: [3.35903739], train loss: [[0.14644171]]\n",
      "epoch: 472, weight: [3.57305058], train loss: [[0.19572384]]\n",
      "epoch: 473, weight: [3.3103577], train loss: [[0.15226975]]\n",
      "epoch: 474, weight: [3.38927594], train loss: [[0.14600096]]\n",
      "epoch: 475, weight: [3.27100299], train loss: [[0.16159739]]\n",
      "epoch: 476, weight: [3.23800608], train loss: [[0.17259897]]\n",
      "epoch: 477, weight: [3.53147892], train loss: [[0.1765997]]\n",
      "epoch: 478, weight: [3.37840974], train loss: [[0.14587888]]\n",
      "epoch: 479, weight: [3.29938227], train loss: [[0.15445609]]\n",
      "epoch: 480, weight: [3.31182392], train loss: [[0.15200198]]\n",
      "epoch: 481, weight: [3.47824859], train loss: [[0.15883572]]\n",
      "epoch: 482, weight: [3.24704863], train loss: [[0.16929547]]\n",
      "epoch: 483, weight: [3.38576746], train loss: [[0.14592715]]\n",
      "epoch: 484, weight: [3.36896906], train loss: [[0.14602824]]\n",
      "epoch: 485, weight: [3.47213801], train loss: [[0.15727963]]\n",
      "epoch: 486, weight: [3.51063085], train loss: [[0.16874282]]\n",
      "epoch: 487, weight: [3.32809415], train loss: [[0.14941508]]\n",
      "epoch: 488, weight: [3.2993322], train loss: [[0.1544668]]\n",
      "epoch: 489, weight: [3.42739303], train loss: [[0.14891683]]\n",
      "epoch: 490, weight: [3.32570121], train loss: [[0.1497513]]\n",
      "epoch: 491, weight: [3.20054359], train loss: [[0.18860627]]\n",
      "epoch: 492, weight: [3.32327289], train loss: [[0.15010809]]\n",
      "epoch: 493, weight: [3.26814403], train loss: [[0.16243581]]\n",
      "epoch: 494, weight: [3.37872805], train loss: [[0.14587798]]\n",
      "epoch: 495, weight: [3.212341], train loss: [[0.18316196]]\n",
      "epoch: 496, weight: [3.4676393], train loss: [[0.15619759]]\n",
      "epoch: 497, weight: [3.5419381], train loss: [[0.18097766]]\n",
      "epoch: 498, weight: [3.37836963], train loss: [[0.14587901]]\n",
      "epoch: 499, weight: [3.33349394], train loss: [[0.14871243]]\n",
      "epoch: 500, weight: [3.37623236], train loss: [[0.14589225]]\n",
      "epoch: 501, weight: [3.34708475], train loss: [[0.14728782]]\n",
      "epoch: 502, weight: [3.46491688], train loss: [[0.15556898]]\n",
      "epoch: 503, weight: [3.55219359], train loss: [[0.1855534]]\n",
      "epoch: 504, weight: [3.48271919], train loss: [[0.16003722]]\n",
      "epoch: 505, weight: [3.38204351], train loss: [[0.14588469]]\n",
      "epoch: 506, weight: [3.58095071], train loss: [[0.19987885]]\n",
      "epoch: 507, weight: [3.54400883], train loss: [[0.18187898]]\n",
      "epoch: 508, weight: [3.33854354], train loss: [[0.14812566]]\n",
      "epoch: 509, weight: [3.60115752], train loss: [[0.21126317]]\n",
      "epoch: 510, weight: [3.44345594], train loss: [[0.15130514]]\n",
      "epoch: 511, weight: [3.48294105], train loss: [[0.16009823]]\n",
      "epoch: 512, weight: [3.18048357], train loss: [[0.19871509]]\n",
      "epoch: 513, weight: [3.35339343], train loss: [[0.1467938]]\n",
      "epoch: 514, weight: [3.49287427], train loss: [[0.16296432]]\n",
      "epoch: 515, weight: [3.39768583], train loss: [[0.14631143]]\n",
      "epoch: 516, weight: [3.32777224], train loss: [[0.14945942]]\n",
      "epoch: 517, weight: [3.42717861], train loss: [[0.1488896]]\n",
      "epoch: 518, weight: [3.42314593], train loss: [[0.14840028]]\n",
      "epoch: 519, weight: [3.29198924], train loss: [[0.15610974]]\n",
      "epoch: 520, weight: [3.19160859], train loss: [[0.19297644]]\n",
      "epoch: 521, weight: [3.25502053], train loss: [[0.16656383]]\n",
      "epoch: 522, weight: [3.29787976], train loss: [[0.15478038]]\n",
      "epoch: 523, weight: [3.43127387], train loss: [[0.14943087]]\n",
      "epoch: 524, weight: [3.44262904], train loss: [[0.15116541]]\n",
      "epoch: 525, weight: [3.25767296], train loss: [[0.1656925]]\n",
      "epoch: 526, weight: [3.17589321], train loss: [[0.20117907]]\n",
      "epoch: 527, weight: [3.41982552], train loss: [[0.14802992]]\n",
      "epoch: 528, weight: [3.42017671], train loss: [[0.1480677]]\n",
      "epoch: 529, weight: [3.56627653], train loss: [[0.19229353]]\n",
      "epoch: 530, weight: [3.37777441], train loss: [[0.14588147]]\n",
      "epoch: 531, weight: [3.3580892], train loss: [[0.14649493]]\n",
      "epoch: 532, weight: [3.48583736], train loss: [[0.16090676]]\n",
      "epoch: 533, weight: [3.22655086], train loss: [[0.17709672]]\n",
      "epoch: 534, weight: [3.32940134], train loss: [[0.14923785]]\n",
      "epoch: 535, weight: [3.23885791], train loss: [[0.17227847]]\n",
      "epoch: 536, weight: [3.22213475], train loss: [[0.17892404]]\n",
      "epoch: 537, weight: [3.29276682], train loss: [[0.15592896]]\n",
      "epoch: 538, weight: [3.4613222], train loss: [[0.15476923]]\n",
      "epoch: 539, weight: [3.4403706], train loss: [[0.15079305]]\n",
      "epoch: 540, weight: [3.41990653], train loss: [[0.1480386]]\n",
      "epoch: 541, weight: [3.42396986], train loss: [[0.14849673]]\n",
      "epoch: 542, weight: [3.19881283], train loss: [[0.18943618]]\n",
      "epoch: 543, weight: [3.13321722], train loss: [[0.22677386]]\n",
      "epoch: 544, weight: [3.4007096], train loss: [[0.14646912]]\n",
      "epoch: 545, weight: [3.3333737], train loss: [[0.14872724]]\n",
      "epoch: 546, weight: [3.49394562], train loss: [[0.16328915]]\n",
      "epoch: 547, weight: [3.5568551], train loss: [[0.18772589]]\n",
      "epoch: 548, weight: [3.41897365], train loss: [[0.14793963]]\n",
      "epoch: 549, weight: [3.38842737], train loss: [[0.1459801]]\n",
      "epoch: 550, weight: [3.33783858], train loss: [[0.1482035]]\n",
      "epoch: 551, weight: [3.50541446], train loss: [[0.1669581]]\n",
      "epoch: 552, weight: [3.55479069], train loss: [[0.18675663]]\n",
      "epoch: 553, weight: [3.56778084], train loss: [[0.19304473]]\n",
      "epoch: 554, weight: [3.46564603], train loss: [[0.15573541]]\n",
      "epoch: 555, weight: [3.39984424], train loss: [[0.14642151]]\n",
      "epoch: 556, weight: [3.48644242], train loss: [[0.1610785]]\n",
      "epoch: 557, weight: [3.29887089], train loss: [[0.15456579]]\n",
      "epoch: 558, weight: [3.50214493], train loss: [[0.16587644]]\n",
      "epoch: 559, weight: [3.22141088], train loss: [[0.17922852]]\n",
      "epoch: 560, weight: [3.22153871], train loss: [[0.17917465]]\n",
      "epoch: 561, weight: [3.33418592], train loss: [[0.14862801]]\n",
      "epoch: 562, weight: [3.26587127], train loss: [[0.16311785]]\n",
      "epoch: 563, weight: [3.53107131], train loss: [[0.17643498]]\n",
      "epoch: 564, weight: [3.18297818], train loss: [[0.1973996]]\n",
      "epoch: 565, weight: [3.09996867], train loss: [[0.25007793]]\n",
      "epoch: 566, weight: [3.30081685], train loss: [[0.15415208]]\n",
      "epoch: 567, weight: [3.23979653], train loss: [[0.17192757]]\n",
      "epoch: 568, weight: [3.37552597], train loss: [[0.14589931]]\n",
      "epoch: 569, weight: [3.37859693], train loss: [[0.14587832]]\n",
      "epoch: 570, weight: [3.51299643], train loss: [[0.16957607]]\n",
      "epoch: 571, weight: [3.3274664], train loss: [[0.1495018]]\n",
      "epoch: 572, weight: [3.57876935], train loss: [[0.19871496]]\n",
      "epoch: 573, weight: [3.4053653], train loss: [[0.14675956]]\n",
      "epoch: 574, weight: [3.38861556], train loss: [[0.14598456]]\n",
      "epoch: 575, weight: [3.60247855], train loss: [[0.21204531]]\n",
      "epoch: 576, weight: [3.53203839], train loss: [[0.1768265]]\n",
      "epoch: 577, weight: [3.13857945], train loss: [[0.22329129]]\n",
      "epoch: 578, weight: [3.40217878], train loss: [[0.14655454]]\n",
      "epoch: 579, weight: [3.33944325], train loss: [[0.14802825]]\n",
      "epoch: 580, weight: [3.27782636], train loss: [[0.15968441]]\n",
      "epoch: 581, weight: [3.33535035], train loss: [[0.14848882]]\n",
      "epoch: 582, weight: [3.53077129], train loss: [[0.17631403]]\n",
      "epoch: 583, weight: [3.24034044], train loss: [[0.1717253]]\n",
      "epoch: 584, weight: [3.41310216], train loss: [[0.14736995]]\n",
      "epoch: 585, weight: [3.36090266], train loss: [[0.14634401]]\n",
      "epoch: 586, weight: [3.54058863], train loss: [[0.18039642]]\n",
      "epoch: 587, weight: [3.66904807], train loss: [[0.25748079]]\n",
      "epoch: 588, weight: [3.40532181], train loss: [[0.14675658]]\n",
      "epoch: 589, weight: [3.45196316], train loss: [[0.15284852]]\n",
      "epoch: 590, weight: [3.25799719], train loss: [[0.16558728]]\n",
      "epoch: 591, weight: [3.0889066], train loss: [[0.25848446]]\n",
      "epoch: 592, weight: [3.28058974], train loss: [[0.15894497]]\n",
      "epoch: 593, weight: [3.58216143], train loss: [[0.20053032]]\n",
      "epoch: 594, weight: [3.44587679], train loss: [[0.1517247]]\n",
      "epoch: 595, weight: [3.61674181], train loss: [[0.22078636]]\n",
      "epoch: 596, weight: [3.42934121], train loss: [[0.14916986]]\n",
      "epoch: 597, weight: [3.33590644], train loss: [[0.14842362]]\n",
      "epoch: 598, weight: [3.288491], train loss: [[0.15694298]]\n",
      "epoch: 599, weight: [3.44085233], train loss: [[0.15087133]]\n",
      "epoch: 600, weight: [3.15411629], train loss: [[0.21363333]]\n",
      "epoch: 601, weight: [3.32560454], train loss: [[0.1497652]]\n",
      "epoch: 602, weight: [3.55682626], train loss: [[0.18771227]]\n",
      "epoch: 603, weight: [3.38118291], train loss: [[0.14588013]]\n",
      "epoch: 604, weight: [3.6524214], train loss: [[0.24502628]]\n",
      "epoch: 605, weight: [3.35832464], train loss: [[0.14648149]]\n",
      "epoch: 606, weight: [3.47859781], train loss: [[0.15892766]]\n",
      "epoch: 607, weight: [3.27571234], train loss: [[0.16026383]]\n",
      "epoch: 608, weight: [3.16498748], train loss: [[0.20725812]]\n",
      "epoch: 609, weight: [3.46877451], train loss: [[0.15646554]]\n",
      "epoch: 610, weight: [3.37571732], train loss: [[0.14589726]]\n",
      "epoch: 611, weight: [3.42508203], train loss: [[0.1486298]]\n",
      "epoch: 612, weight: [3.53025571], train loss: [[0.17610673]]\n",
      "epoch: 613, weight: [3.33590512], train loss: [[0.14842378]]\n",
      "epoch: 614, weight: [3.44737845], train loss: [[0.1519928]]\n",
      "epoch: 615, weight: [3.31027835], train loss: [[0.1522844]]\n",
      "epoch: 616, weight: [3.46249534], train loss: [[0.15502645]]\n",
      "epoch: 617, weight: [3.339436], train loss: [[0.14802902]]\n",
      "epoch: 618, weight: [3.54882934], train loss: [[0.18402147]]\n",
      "epoch: 619, weight: [3.4125261], train loss: [[0.14731901]]\n",
      "epoch: 620, weight: [3.3672497], train loss: [[0.146081]]\n",
      "epoch: 621, weight: [3.28522946], train loss: [[0.15774921]]\n",
      "epoch: 622, weight: [3.58134088], train loss: [[0.20008837]]\n",
      "epoch: 623, weight: [3.26157516], train loss: [[0.16444469]]\n",
      "epoch: 624, weight: [3.47898188], train loss: [[0.15902915]]\n",
      "epoch: 625, weight: [3.39206984], train loss: [[0.1460832]]\n",
      "epoch: 626, weight: [3.13367058], train loss: [[0.22647645]]\n",
      "epoch: 627, weight: [3.17989398], train loss: [[0.19902842]]\n",
      "epoch: 628, weight: [3.46520445], train loss: [[0.15563445]]\n",
      "epoch: 629, weight: [3.34882863], train loss: [[0.14714065]]\n",
      "epoch: 630, weight: [3.44265887], train loss: [[0.15117042]]\n",
      "epoch: 631, weight: [3.44779913], train loss: [[0.15206899]]\n",
      "epoch: 632, weight: [3.46082314], train loss: [[0.15466092]]\n",
      "epoch: 633, weight: [3.4853911], train loss: [[0.16078073]]\n",
      "epoch: 634, weight: [3.6424303], train loss: [[0.23789659]]\n",
      "epoch: 635, weight: [3.40751703], train loss: [[0.14691331]]\n",
      "epoch: 636, weight: [3.30447784], train loss: [[0.15340112]]\n",
      "epoch: 637, weight: [3.40673794], train loss: [[0.14685621]]\n",
      "epoch: 638, weight: [3.4479686], train loss: [[0.15209981]]\n",
      "epoch: 639, weight: [3.38933167], train loss: [[0.1460024]]\n",
      "epoch: 640, weight: [3.39049737], train loss: [[0.14603435]]\n",
      "epoch: 641, weight: [3.48358298], train loss: [[0.1602755]]\n",
      "epoch: 642, weight: [3.38169637], train loss: [[0.14588261]]\n",
      "epoch: 643, weight: [3.4027714], train loss: [[0.14659062]]\n",
      "epoch: 644, weight: [3.42163172], train loss: [[0.14822774]]\n",
      "epoch: 645, weight: [3.24908904], train loss: [[0.16858018]]\n",
      "epoch: 646, weight: [3.44223827], train loss: [[0.15110001]]\n",
      "epoch: 647, weight: [3.22608456], train loss: [[0.17728722]]\n",
      "epoch: 648, weight: [3.37056306], train loss: [[0.14598635]]\n",
      "epoch: 649, weight: [3.38409144], train loss: [[0.14590346]]\n",
      "epoch: 650, weight: [3.35231551], train loss: [[0.14687069]]\n",
      "epoch: 651, weight: [3.29264808], train loss: [[0.15595646]]\n",
      "epoch: 652, weight: [3.42827209], train loss: [[0.14902975]]\n",
      "epoch: 653, weight: [3.28076033], train loss: [[0.15889999]]\n",
      "epoch: 654, weight: [3.44120472], train loss: [[0.15092899]]\n",
      "epoch: 655, weight: [3.45916412], train loss: [[0.15430563]]\n",
      "epoch: 656, weight: [3.50291631], train loss: [[0.16612907]]\n",
      "epoch: 657, weight: [3.24248341], train loss: [[0.17093604]]\n",
      "epoch: 658, weight: [3.23723321], train loss: [[0.17289143]]\n",
      "epoch: 659, weight: [3.24666281], train loss: [[0.16943197]]\n",
      "epoch: 660, weight: [3.46487264], train loss: [[0.15555893]]\n",
      "epoch: 661, weight: [3.20803649], train loss: [[0.18510545]]\n",
      "epoch: 662, weight: [3.37663108], train loss: [[0.14588886]]\n",
      "epoch: 663, weight: [3.56426927], train loss: [[0.19130055]]\n",
      "epoch: 664, weight: [3.40886925], train loss: [[0.14701624]]\n",
      "epoch: 665, weight: [3.57672601], train loss: [[0.19763621]]\n",
      "epoch: 666, weight: [3.34406491], train loss: [[0.14756183]]\n",
      "epoch: 667, weight: [3.22050172], train loss: [[0.17961293]]\n",
      "epoch: 668, weight: [3.2348062], train loss: [[0.17382017]]\n",
      "epoch: 669, weight: [3.54523268], train loss: [[0.18241706]]\n",
      "epoch: 670, weight: [3.47832529], train loss: [[0.15885589]]\n",
      "epoch: 671, weight: [3.49365958], train loss: [[0.16320213]]\n",
      "epoch: 672, weight: [3.38151466], train loss: [[0.14588165]]\n",
      "epoch: 673, weight: [3.09701811], train loss: [[0.2522883]]\n",
      "epoch: 674, weight: [3.39108224], train loss: [[0.14605175]]\n",
      "epoch: 675, weight: [3.57085731], train loss: [[0.1945998]]\n",
      "epoch: 676, weight: [3.51568379], train loss: [[0.17054075]]\n",
      "epoch: 677, weight: [3.33119808], train loss: [[0.14900169]]\n",
      "epoch: 678, weight: [3.31232365], train loss: [[0.15191202]]\n",
      "epoch: 679, weight: [3.43174684], train loss: [[0.14949626]]\n",
      "epoch: 680, weight: [3.18347448], train loss: [[0.19713986]]\n",
      "epoch: 681, weight: [3.4459588], train loss: [[0.15173919]]\n",
      "epoch: 682, weight: [3.42030708], train loss: [[0.14808181]]\n",
      "epoch: 683, weight: [3.45313311], train loss: [[0.15307586]]\n",
      "epoch: 684, weight: [3.28246032], train loss: [[0.15845598]]\n",
      "epoch: 685, weight: [3.46219838], train loss: [[0.15496099]]\n",
      "epoch: 686, weight: [3.13868768], train loss: [[0.22322179]]\n",
      "epoch: 687, weight: [3.47974022], train loss: [[0.15923069]]\n",
      "epoch: 688, weight: [3.30976858], train loss: [[0.15237895]]\n",
      "epoch: 689, weight: [3.39409737], train loss: [[0.1461559]]\n",
      "epoch: 690, weight: [3.45167131], train loss: [[0.15279238]]\n",
      "epoch: 691, weight: [3.32589722], train loss: [[0.14972319]]\n",
      "epoch: 692, weight: [3.54140522], train loss: [[0.18074756]]\n",
      "epoch: 693, weight: [3.24829831], train loss: [[0.16885607]]\n",
      "epoch: 694, weight: [3.53199019], train loss: [[0.17680693]]\n",
      "epoch: 695, weight: [3.37054589], train loss: [[0.14598677]]\n",
      "epoch: 696, weight: [3.41716539], train loss: [[0.1477544]]\n",
      "epoch: 697, weight: [3.67011092], train loss: [[0.25830199]]\n",
      "epoch: 698, weight: [3.39436181], train loss: [[0.14616619]]\n",
      "epoch: 699, weight: [3.29802148], train loss: [[0.15474953]]\n",
      "epoch: 700, weight: [3.56214349], train loss: [[0.19026065]]\n",
      "epoch: 701, weight: [3.44526393], train loss: [[0.15161701]]\n",
      "epoch: 702, weight: [3.39557712], train loss: [[0.14621588]]\n",
      "epoch: 703, weight: [3.38512594], train loss: [[0.1459172]]\n",
      "epoch: 704, weight: [3.36361918], train loss: [[0.1462183]]\n",
      "epoch: 705, weight: [3.42629678], train loss: [[0.1487789]]\n",
      "epoch: 706, weight: [3.45126635], train loss: [[0.15271486]]\n",
      "epoch: 707, weight: [3.51526963], train loss: [[0.17039083]]\n",
      "epoch: 708, weight: [3.49730004], train loss: [[0.16432599]]\n",
      "epoch: 709, weight: [3.57219531], train loss: [[0.19528399]]\n",
      "epoch: 710, weight: [3.26424979], train loss: [[0.16361286]]\n",
      "epoch: 711, weight: [3.38319693], train loss: [[0.14589389]]\n",
      "epoch: 712, weight: [3.17167698], train loss: [[0.20349169]]\n",
      "epoch: 713, weight: [3.38559843], train loss: [[0.14592442]]\n",
      "epoch: 714, weight: [3.36665563], train loss: [[0.14610107]]\n",
      "epoch: 715, weight: [3.31397474], train loss: [[0.15161954]]\n",
      "epoch: 716, weight: [3.60087848], train loss: [[0.21109855]]\n",
      "epoch: 717, weight: [3.4345157], train loss: [[0.14989102]]\n",
      "epoch: 718, weight: [3.44486627], train loss: [[0.15154767]]\n",
      "epoch: 719, weight: [3.50160378], train loss: [[0.16570016]]\n",
      "epoch: 720, weight: [3.28046552], train loss: [[0.15897777]]\n",
      "epoch: 721, weight: [3.33644181], train loss: [[0.14836163]]\n",
      "epoch: 722, weight: [3.31556762], train loss: [[0.15134426]]\n",
      "epoch: 723, weight: [3.10660337], train loss: [[0.24519237]]\n",
      "epoch: 724, weight: [3.45660866], train loss: [[0.15377272]]\n",
      "epoch: 725, weight: [3.31192887], train loss: [[0.15198303]]\n",
      "epoch: 726, weight: [3.45607489], train loss: [[0.15366361]]\n",
      "epoch: 727, weight: [3.33988535], train loss: [[0.14798117]]\n",
      "epoch: 728, weight: [3.374516], train loss: [[0.1459117]]\n",
      "epoch: 729, weight: [3.66738006], train loss: [[0.25619809]]\n",
      "epoch: 730, weight: [3.58469472], train loss: [[0.20190607]]\n",
      "epoch: 731, weight: [3.17311052], train loss: [[0.20270007]]\n",
      "epoch: 732, weight: [3.19665062], train loss: [[0.19048419]]\n",
      "epoch: 733, weight: [3.75274274], train loss: [[0.33136066]]\n",
      "epoch: 734, weight: [3.21712065], train loss: [[0.18106179]]\n",
      "epoch: 735, weight: [3.41412106], train loss: [[0.14746222]]\n",
      "epoch: 736, weight: [3.32338986], train loss: [[0.15009055]]\n",
      "epoch: 737, weight: [3.27819825], train loss: [[0.15958371]]\n",
      "epoch: 738, weight: [3.50515255], train loss: [[0.16687041]]\n",
      "epoch: 739, weight: [3.40399235], train loss: [[0.14666791]]\n",
      "epoch: 740, weight: [3.46758216], train loss: [[0.15618419]]\n",
      "epoch: 741, weight: [3.44899293], train loss: [[0.15228775]]\n",
      "epoch: 742, weight: [3.39592978], train loss: [[0.14623103]]\n",
      "epoch: 743, weight: [3.15690946], train loss: [[0.21196527]]\n",
      "epoch: 744, weight: [3.35128565], train loss: [[0.14694706]]\n",
      "epoch: 745, weight: [3.40945464], train loss: [[0.14706231]]\n",
      "epoch: 746, weight: [3.57232946], train loss: [[0.19535286]]\n",
      "epoch: 747, weight: [3.36746475], train loss: [[0.14607397]]\n",
      "epoch: 748, weight: [3.13111265], train loss: [[0.22816163]]\n",
      "epoch: 749, weight: [3.16707145], train loss: [[0.20607198]]\n",
      "epoch: 750, weight: [3.44033779], train loss: [[0.15078774]]\n",
      "epoch: 751, weight: [3.18877206], train loss: [[0.19440829]]\n",
      "epoch: 752, weight: [3.4608262], train loss: [[0.15466158]]\n",
      "epoch: 753, weight: [3.52460884], train loss: [[0.17388267]]\n",
      "epoch: 754, weight: [3.527194], train loss: [[0.17489031]]\n",
      "epoch: 755, weight: [3.43043205], train loss: [[0.14931596]]\n",
      "epoch: 756, weight: [3.42017537], train loss: [[0.14806756]]\n",
      "epoch: 757, weight: [3.30802709], train loss: [[0.15270717]]\n",
      "epoch: 758, weight: [3.46092779], train loss: [[0.15468357]]\n",
      "epoch: 759, weight: [3.48530377], train loss: [[0.16075613]]\n",
      "epoch: 760, weight: [3.46778984], train loss: [[0.15623293]]\n",
      "epoch: 761, weight: [3.47903541], train loss: [[0.15904332]]\n",
      "epoch: 762, weight: [3.47666015], train loss: [[0.15842165]]\n",
      "epoch: 763, weight: [3.26837905], train loss: [[0.16236606]]\n",
      "epoch: 764, weight: [3.41193994], train loss: [[0.14726808]]\n",
      "epoch: 765, weight: [3.1244327], train loss: [[0.23264464]]\n",
      "epoch: 766, weight: [3.43108053], train loss: [[0.14940431]]\n",
      "epoch: 767, weight: [3.48868719], train loss: [[0.16172414]]\n",
      "epoch: 768, weight: [3.27565803], train loss: [[0.16027887]]\n",
      "epoch: 769, weight: [3.23594693], train loss: [[0.17338169]]\n",
      "epoch: 770, weight: [3.26050742], train loss: [[0.16478209]]\n",
      "epoch: 771, weight: [3.29648025], train loss: [[0.15508784]]\n",
      "epoch: 772, weight: [3.46186752], train loss: [[0.15488834]]\n",
      "epoch: 773, weight: [3.46468849], train loss: [[0.15551715]]\n",
      "epoch: 774, weight: [3.33114447], train loss: [[0.14900861]]\n",
      "epoch: 775, weight: [3.34704488], train loss: [[0.14729128]]\n",
      "epoch: 776, weight: [3.41039117], train loss: [[0.14713792]]\n",
      "epoch: 777, weight: [3.176937], train loss: [[0.20061386]]\n",
      "epoch: 778, weight: [3.34329253], train loss: [[0.14763582]]\n",
      "epoch: 779, weight: [3.30510908], train loss: [[0.15327524]]\n",
      "epoch: 780, weight: [3.36499771], train loss: [[0.14616203]]\n",
      "epoch: 781, weight: [3.2436477], train loss: [[0.17051237]]\n",
      "epoch: 782, weight: [3.36708182], train loss: [[0.14608658]]\n",
      "epoch: 783, weight: [3.39260832], train loss: [[0.14610144]]\n",
      "epoch: 784, weight: [3.26622005], train loss: [[0.16301229]]\n",
      "epoch: 785, weight: [3.15233125], train loss: [[0.21471024]]\n",
      "epoch: 786, weight: [3.46391478], train loss: [[0.15534257]]\n",
      "epoch: 787, weight: [3.55669675], train loss: [[0.18765114]]\n",
      "epoch: 788, weight: [3.40391939], train loss: [[0.14666318]]\n",
      "epoch: 789, weight: [3.45490311], train loss: [[0.15342673]]\n",
      "epoch: 790, weight: [3.56896904], train loss: [[0.19364235]]\n",
      "epoch: 791, weight: [3.38339584], train loss: [[0.14589583]]\n",
      "epoch: 792, weight: [3.6257739], train loss: [[0.22660189]]\n",
      "epoch: 793, weight: [3.27682691], train loss: [[0.15995686]]\n",
      "epoch: 794, weight: [3.52034846], train loss: [[0.17226092]]\n",
      "epoch: 795, weight: [3.65394656], train loss: [[0.24613804]]\n",
      "epoch: 796, weight: [3.35387279], train loss: [[0.14676059]]\n",
      "epoch: 797, weight: [3.22249934], train loss: [[0.17877121]]\n",
      "epoch: 798, weight: [3.41989623], train loss: [[0.1480375]]\n",
      "epoch: 799, weight: [3.37925036], train loss: [[0.14587709]]\n",
      "epoch: 800, weight: [3.1038439], train loss: [[0.2472101]]\n",
      "epoch: 801, weight: [3.42374223], train loss: [[0.14846991]]\n",
      "epoch: 802, weight: [3.53232589], train loss: [[0.17694337]]\n",
      "epoch: 803, weight: [3.40600813], train loss: [[0.1468042]]\n",
      "epoch: 804, weight: [3.1721007], train loss: [[0.20325714]]\n",
      "epoch: 805, weight: [3.37025178], train loss: [[0.145994]]\n",
      "epoch: 806, weight: [3.58272862], train loss: [[0.20083686]]\n",
      "epoch: 807, weight: [3.47856157], train loss: [[0.1589181]]\n",
      "epoch: 808, weight: [3.4280982], train loss: [[0.14900725]]\n",
      "epoch: 809, weight: [3.53655834], train loss: [[0.17868941]]\n",
      "epoch: 810, weight: [3.44742599], train loss: [[0.15200139]]\n",
      "epoch: 811, weight: [3.17162607], train loss: [[0.20351991]]\n",
      "epoch: 812, weight: [3.18401085], train loss: [[0.1968599]]\n",
      "epoch: 813, weight: [3.35021605], train loss: [[0.14702936]]\n",
      "epoch: 814, weight: [3.23625746], train loss: [[0.17326293]]\n",
      "epoch: 815, weight: [3.28308157], train loss: [[0.15829564]]\n",
      "epoch: 816, weight: [3.35473782], train loss: [[0.14670223]]\n",
      "epoch: 817, weight: [3.35980784], train loss: [[0.14640023]]\n",
      "epoch: 818, weight: [3.27683227], train loss: [[0.15995539]]\n",
      "epoch: 819, weight: [3.45551761], train loss: [[0.15355049]]\n",
      "epoch: 820, weight: [3.4496809], train loss: [[0.15241554]]\n",
      "epoch: 821, weight: [3.58733496], train loss: [[0.2033581]]\n",
      "epoch: 822, weight: [3.39362766], train loss: [[0.14613808]]\n",
      "epoch: 823, weight: [3.34693889], train loss: [[0.1473005]]\n",
      "epoch: 824, weight: [3.38106442], train loss: [[0.14587966]]\n",
      "epoch: 825, weight: [3.26998227], train loss: [[0.16189423]]\n",
      "epoch: 826, weight: [3.59624572], train loss: [[0.20839582]]\n",
      "epoch: 827, weight: [3.39007428], train loss: [[0.14602234]]\n",
      "epoch: 828, weight: [3.47711705], train loss: [[0.15854006]]\n",
      "epoch: 829, weight: [3.3481054], train loss: [[0.1472007]]\n",
      "epoch: 830, weight: [3.43738404], train loss: [[0.15032152]]\n",
      "epoch: 831, weight: [3.36330215], train loss: [[0.14623196]]\n",
      "epoch: 832, weight: [3.29931921], train loss: [[0.15446958]]\n",
      "epoch: 833, weight: [3.31806042], train loss: [[0.15092703]]\n",
      "epoch: 834, weight: [3.45975682], train loss: [[0.15443172]]\n",
      "epoch: 835, weight: [3.46510681], train loss: [[0.1556122]]\n",
      "epoch: 836, weight: [3.33275661], train loss: [[0.1488038]]\n",
      "epoch: 837, weight: [3.2746649], train loss: [[0.16055532]]\n",
      "epoch: 838, weight: [3.33290119], train loss: [[0.14878577]]\n",
      "epoch: 839, weight: [3.29423623], train loss: [[0.15559174]]\n",
      "epoch: 840, weight: [3.21212379], train loss: [[0.18325885]]\n",
      "epoch: 841, weight: [3.56273431], train loss: [[0.19054846]]\n",
      "epoch: 842, weight: [3.44623662], train loss: [[0.15178839]]\n",
      "epoch: 843, weight: [3.476598], train loss: [[0.15840558]]\n",
      "epoch: 844, weight: [3.371776], train loss: [[0.14595902]]\n",
      "epoch: 845, weight: [3.26885578], train loss: [[0.16222504]]\n",
      "epoch: 846, weight: [3.637313], train loss: [[0.23434788]]\n",
      "epoch: 847, weight: [3.30029304], train loss: [[0.15426245]]\n",
      "epoch: 848, weight: [3.31975758], train loss: [[0.15065244]]\n",
      "epoch: 849, weight: [3.24934715], train loss: [[0.16849049]]\n",
      "epoch: 850, weight: [3.21434314], train loss: [[0.18227482]]\n",
      "epoch: 851, weight: [3.46650056], train loss: [[0.15593225]]\n",
      "epoch: 852, weight: [3.48626002], train loss: [[0.16102662]]\n",
      "epoch: 853, weight: [3.30462893], train loss: [[0.15337089]]\n",
      "epoch: 854, weight: [3.48879072], train loss: [[0.16175424]]\n",
      "epoch: 855, weight: [3.27987776], train loss: [[0.15913354]]\n",
      "epoch: 856, weight: [3.44584256], train loss: [[0.15171866]]\n",
      "epoch: 857, weight: [3.34643166], train loss: [[0.14734502]]\n",
      "epoch: 858, weight: [3.51838447], train loss: [[0.17152961]]\n",
      "epoch: 859, weight: [3.32874497], train loss: [[0.14932627]]\n",
      "epoch: 860, weight: [3.4850855], train loss: [[0.16069473]]\n",
      "epoch: 861, weight: [3.39223572], train loss: [[0.14608873]]\n",
      "epoch: 862, weight: [3.56412638], train loss: [[0.19123027]]\n",
      "epoch: 863, weight: [3.54146471], train loss: [[0.18077321]]\n",
      "epoch: 864, weight: [3.23656854], train loss: [[0.17314422]]\n",
      "epoch: 865, weight: [3.57065946], train loss: [[0.19449904]]\n",
      "epoch: 866, weight: [3.31808047], train loss: [[0.15092374]]\n",
      "epoch: 867, weight: [3.40049408], train loss: [[0.14645708]]\n",
      "epoch: 868, weight: [3.38191397], train loss: [[0.14588387]]\n",
      "epoch: 869, weight: [3.61233907], train loss: [[0.21803036]]\n",
      "epoch: 870, weight: [3.29283151], train loss: [[0.15591399]]\n",
      "epoch: 871, weight: [3.46918037], train loss: [[0.15656218]]\n",
      "epoch: 872, weight: [3.6255206], train loss: [[0.22643583]]\n",
      "epoch: 873, weight: [3.41098181], train loss: [[0.1471868]]\n",
      "epoch: 874, weight: [3.50755038], train loss: [[0.16768011]]\n",
      "epoch: 875, weight: [3.27681307], train loss: [[0.15996065]]\n",
      "epoch: 876, weight: [3.42788279], train loss: [[0.14897949]]\n",
      "epoch: 877, weight: [3.44938032], train loss: [[0.15235956]]\n",
      "epoch: 878, weight: [3.59403675], train loss: [[0.20712725]]\n",
      "epoch: 879, weight: [3.21435406], train loss: [[0.18227001]]\n",
      "epoch: 880, weight: [3.28543436], train loss: [[0.15769773]]\n",
      "epoch: 881, weight: [3.28105556], train loss: [[0.15882233]]\n",
      "epoch: 882, weight: [3.25101056], train loss: [[0.16791672]]\n",
      "epoch: 883, weight: [3.30957769], train loss: [[0.15241453]]\n",
      "epoch: 884, weight: [3.47090236], train loss: [[0.15697705]]\n",
      "epoch: 885, weight: [3.49352245], train loss: [[0.16316048]]\n",
      "epoch: 886, weight: [3.21117209], train loss: [[0.18368484]]\n",
      "epoch: 887, weight: [3.64950913], train loss: [[0.24292061]]\n",
      "epoch: 888, weight: [3.47994226], train loss: [[0.15928464]]\n",
      "epoch: 889, weight: [3.43440236], train loss: [[0.14987446]]\n",
      "epoch: 890, weight: [3.56045756], train loss: [[0.18944448]]\n",
      "epoch: 891, weight: [3.43745543], train loss: [[0.15033251]]\n",
      "epoch: 892, weight: [3.22170907], train loss: [[0.17910292]]\n",
      "epoch: 893, weight: [3.56517152], train loss: [[0.19174556]]\n",
      "epoch: 894, weight: [3.6134684], train loss: [[0.21873237]]\n",
      "epoch: 895, weight: [3.62593299], train loss: [[0.22670627]]\n",
      "epoch: 896, weight: [3.21562809], train loss: [[0.18171108]]\n",
      "epoch: 897, weight: [3.46118494], train loss: [[0.15473937]]\n",
      "epoch: 898, weight: [3.48863522], train loss: [[0.16170904]]\n",
      "epoch: 899, weight: [3.45052074], train loss: [[0.15257326]]\n",
      "epoch: 900, weight: [3.40309696], train loss: [[0.14661084]]\n",
      "epoch: 901, weight: [3.30970575], train loss: [[0.15239065]]\n",
      "epoch: 902, weight: [3.48123883], train loss: [[0.15963346]]\n",
      "epoch: 903, weight: [3.4609744], train loss: [[0.15469368]]\n",
      "epoch: 904, weight: [3.27029131], train loss: [[0.16180406]]\n",
      "epoch: 905, weight: [3.57340387], train loss: [[0.1959061]]\n",
      "epoch: 906, weight: [3.4084415], train loss: [[0.14698315]]\n",
      "epoch: 907, weight: [3.46341767], train loss: [[0.15523125]]\n",
      "epoch: 908, weight: [3.41695045], train loss: [[0.14773296]]\n",
      "epoch: 909, weight: [3.40001084], train loss: [[0.14643052]]\n",
      "epoch: 910, weight: [3.68426919], train loss: [[0.26952831]]\n",
      "epoch: 911, weight: [3.22922389], train loss: [[0.17601591]]\n",
      "epoch: 912, weight: [3.38848488], train loss: [[0.14598145]]\n",
      "epoch: 913, weight: [3.38222314], train loss: [[0.14588589]]\n",
      "epoch: 914, weight: [3.53706704], train loss: [[0.17890248]]\n",
      "epoch: 915, weight: [3.47575376], train loss: [[0.15818838]]\n",
      "epoch: 916, weight: [3.18955373], train loss: [[0.19401157]]\n",
      "epoch: 917, weight: [3.24109087], train loss: [[0.17144752]]\n",
      "epoch: 918, weight: [3.39269279], train loss: [[0.14610437]]\n",
      "epoch: 919, weight: [3.36571453], train loss: [[0.14613477]]\n",
      "epoch: 920, weight: [3.20536849], train loss: [[0.18633483]]\n",
      "epoch: 921, weight: [3.53398081], train loss: [[0.1776204]]\n",
      "epoch: 922, weight: [3.44399007], train loss: [[0.15139637]]\n",
      "epoch: 923, weight: [3.27790639], train loss: [[0.15966271]]\n",
      "epoch: 924, weight: [3.27121652], train loss: [[0.16153565]]\n",
      "epoch: 925, weight: [3.17154844], train loss: [[0.20356294]]\n",
      "epoch: 926, weight: [3.34993183], train loss: [[0.14705174]]\n",
      "epoch: 927, weight: [3.36349386], train loss: [[0.14622367]]\n",
      "epoch: 928, weight: [3.1548675], train loss: [[0.21318267]]\n",
      "epoch: 929, weight: [3.48024864], train loss: [[0.15936666]]\n",
      "epoch: 930, weight: [3.37062027], train loss: [[0.14598497]]\n",
      "epoch: 931, weight: [3.52716829], train loss: [[0.1748802]]\n",
      "epoch: 932, weight: [3.55482304], train loss: [[0.18677174]]\n",
      "epoch: 933, weight: [3.29243543], train loss: [[0.15600581]]\n",
      "epoch: 934, weight: [3.50501218], train loss: [[0.16682348]]\n",
      "epoch: 935, weight: [3.39000461], train loss: [[0.1460204]]\n",
      "epoch: 936, weight: [3.4090464], train loss: [[0.14703008]]\n",
      "epoch: 937, weight: [3.4511079], train loss: [[0.15268464]]\n",
      "epoch: 938, weight: [3.44297259], train loss: [[0.15122324]]\n",
      "epoch: 939, weight: [3.26053383], train loss: [[0.16477371]]\n",
      "epoch: 940, weight: [3.4618196], train loss: [[0.15487784]]\n",
      "epoch: 941, weight: [3.40724049], train loss: [[0.14689286]]\n",
      "epoch: 942, weight: [3.30987788], train loss: [[0.15235862]]\n",
      "epoch: 943, weight: [3.40784913], train loss: [[0.14693814]]\n",
      "epoch: 944, weight: [3.30802323], train loss: [[0.15270791]]\n",
      "epoch: 945, weight: [3.45001365], train loss: [[0.15247781]]\n",
      "epoch: 946, weight: [3.42778658], train loss: [[0.14896713]]\n",
      "epoch: 947, weight: [3.46391319], train loss: [[0.15534221]]\n",
      "epoch: 948, weight: [3.41467987], train loss: [[0.147514]]\n",
      "epoch: 949, weight: [3.38241656], train loss: [[0.14588727]]\n",
      "epoch: 950, weight: [3.44462159], train loss: [[0.15150521]]\n",
      "epoch: 951, weight: [3.37445679], train loss: [[0.14591251]]\n",
      "epoch: 952, weight: [3.45737754], train loss: [[0.15393123]]\n",
      "epoch: 953, weight: [3.39325068], train loss: [[0.14612421]]\n",
      "epoch: 954, weight: [3.17469394], train loss: [[0.20183205]]\n",
      "epoch: 955, weight: [3.39486051], train loss: [[0.1461861]]\n",
      "epoch: 956, weight: [3.37983761], train loss: [[0.14587696]]\n",
      "epoch: 957, weight: [3.5152849], train loss: [[0.17039635]]\n",
      "epoch: 958, weight: [3.36478707], train loss: [[0.1461703]]\n",
      "epoch: 959, weight: [3.48178541], train loss: [[0.15978186]]\n",
      "epoch: 960, weight: [3.45468594], train loss: [[0.15338323]]\n",
      "epoch: 961, weight: [3.17743497], train loss: [[0.20034523]]\n",
      "epoch: 962, weight: [3.09586661], train loss: [[0.25315722]]\n",
      "epoch: 963, weight: [3.48024821], train loss: [[0.15936655]]\n",
      "epoch: 964, weight: [3.5067897], train loss: [[0.16742159]]\n",
      "epoch: 965, weight: [3.40353889], train loss: [[0.14663874]]\n",
      "epoch: 966, weight: [3.37576317], train loss: [[0.14589679]]\n",
      "epoch: 967, weight: [3.45207314], train loss: [[0.15286974]]\n",
      "epoch: 968, weight: [3.66322968], train loss: [[0.25303863]]\n",
      "epoch: 969, weight: [3.52067414], train loss: [[0.17238319]]\n",
      "epoch: 970, weight: [3.72849348], train loss: [[0.3080345]]\n",
      "epoch: 971, weight: [3.33156237], train loss: [[0.14895485]]\n",
      "epoch: 972, weight: [3.38876323], train loss: [[0.14598813]]\n",
      "epoch: 973, weight: [3.34253945], train loss: [[0.14770949]]\n",
      "epoch: 974, weight: [3.25197967], train loss: [[0.16758583]]\n",
      "epoch: 975, weight: [3.52311474], train loss: [[0.17330843]]\n",
      "epoch: 976, weight: [3.62880282], train loss: [[0.22860081]]\n",
      "epoch: 977, weight: [3.53437682], train loss: [[0.1777835]]\n",
      "epoch: 978, weight: [3.41173829], train loss: [[0.14725077]]\n",
      "epoch: 979, weight: [3.46729547], train loss: [[0.15611711]]\n",
      "epoch: 980, weight: [3.41640433], train loss: [[0.14767904]]\n",
      "epoch: 981, weight: [3.122222], train loss: [[0.23415446]]\n",
      "epoch: 982, weight: [3.29744392], train loss: [[0.15487557]]\n",
      "epoch: 983, weight: [3.34257724], train loss: [[0.14770576]]\n",
      "epoch: 984, weight: [3.47913841], train loss: [[0.15907062]]\n",
      "epoch: 985, weight: [3.2381392], train loss: [[0.17254876]]\n",
      "epoch: 986, weight: [3.30577252], train loss: [[0.15314409]]\n",
      "epoch: 987, weight: [3.41749213], train loss: [[0.14778722]]\n",
      "epoch: 988, weight: [3.49460146], train loss: [[0.16348951]]\n",
      "epoch: 989, weight: [3.2700172], train loss: [[0.16188402]]\n",
      "epoch: 990, weight: [3.46992599], train loss: [[0.15674085]]\n",
      "epoch: 991, weight: [3.15876054], train loss: [[0.21087126]]\n",
      "epoch: 992, weight: [3.36221528], train loss: [[0.14628081]]\n",
      "epoch: 993, weight: [3.35400891], train loss: [[0.14675128]]\n",
      "epoch: 994, weight: [3.37817233], train loss: [[0.14587972]]\n",
      "epoch: 995, weight: [3.25554556], train loss: [[0.16638987]]\n",
      "epoch: 996, weight: [3.29192752], train loss: [[0.15612416]]\n",
      "epoch: 997, weight: [3.40368982], train loss: [[0.14664839]]\n",
      "epoch: 998, weight: [3.36519326], train loss: [[0.14615446]]\n",
      "epoch: 999, weight: [3.52457969], train loss: [[0.17387141]]\n",
      "epoch: 1000, weight: [3.34641296], train loss: [[0.14734668]]\n",
      "The time used: 36.083597898483276\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.rand(1)\n",
    "eta = 0.1\n",
    "d_train = X.size\n",
    "t1 = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(d_train):\n",
    "        xi = X[i]\n",
    "        yi = y[i]\n",
    "        gradient = 2 * (w.dot(xi.T) - yi.T).dot(xi)\n",
    "        w = w - eta * gradient\n",
    "    train_loss = 1/d_train * (w.dot(X.T) - y.T).dot((w.dot(X.T) - y.T).T)\n",
    "    print('epoch: {}, weight: {}, train loss: {}'.format(epoch + 1, w, train_loss))\n",
    "\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices] #Suffle the input X\n",
    "    y = y[indices] #Suffle the input y\n",
    "\n",
    "t2 = time.time()\n",
    "print('The time used: {}'.format(t2 - t1))\n",
    "\n",
    "y_predict = X.dot(w.T)\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, y_predict, 'r-')\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, weight: [[3.35937523]], train loss: [[22.87864412]]\n",
      "epoch: 2, weight: [[3.37769404]], train loss: [[22.79404364]]\n",
      "epoch: 3, weight: [[3.38832355]], train loss: [[22.80901227]]\n",
      "epoch: 4, weight: [[3.38026799]], train loss: [[22.79335179]]\n",
      "epoch: 5, weight: [[3.38968358]], train loss: [[22.81432209]]\n",
      "epoch: 6, weight: [[3.37111611]], train loss: [[22.80834419]]\n",
      "epoch: 7, weight: [[3.37679865]], train loss: [[22.794931]]\n",
      "epoch: 8, weight: [[3.38348853]], train loss: [[22.79637107]]\n",
      "epoch: 9, weight: [[3.37441636]], train loss: [[22.79891747]]\n",
      "epoch: 10, weight: [[3.3729189]], train loss: [[22.80263277]]\n",
      "epoch: 11, weight: [[3.37095472]], train loss: [[22.80892149]]\n",
      "epoch: 12, weight: [[3.38581797]], train loss: [[22.80124634]]\n",
      "epoch: 13, weight: [[3.36720775]], train loss: [[22.8253731]]\n",
      "epoch: 14, weight: [[3.41173247]], train loss: [[23.00785507]]\n",
      "epoch: 15, weight: [[3.38183901]], train loss: [[22.79428515]]\n",
      "epoch: 16, weight: [[3.39617924]], train loss: [[22.85030533]]\n",
      "epoch: 17, weight: [[3.37058805]], train loss: [[22.81027336]]\n",
      "epoch: 18, weight: [[3.38535932]], train loss: [[22.80010781]]\n",
      "epoch: 19, weight: [[3.39563191]], train loss: [[22.84659558]]\n",
      "epoch: 20, weight: [[3.38660672]], train loss: [[22.80340913]]\n",
      "epoch: 21, weight: [[3.37582317]], train loss: [[22.79627766]]\n",
      "epoch: 22, weight: [[3.38036862]], train loss: [[22.79338078]]\n",
      "epoch: 23, weight: [[3.38442979]], train loss: [[22.79806901]]\n",
      "epoch: 24, weight: [[3.37640402]], train loss: [[22.79542808]]\n",
      "epoch: 25, weight: [[3.3860947]], train loss: [[22.80197566]]\n",
      "epoch: 26, weight: [[3.36589366]], train loss: [[22.83252738]]\n",
      "epoch: 27, weight: [[3.3814116]], train loss: [[22.79392947]]\n",
      "epoch: 28, weight: [[3.38132061]], train loss: [[22.79386357]]\n",
      "epoch: 29, weight: [[3.39261904]], train loss: [[22.82840768]]\n",
      "epoch: 30, weight: [[3.37397421]], train loss: [[22.79991734]]\n",
      "epoch: 31, weight: [[3.38910384]], train loss: [[22.81196451]]\n",
      "epoch: 32, weight: [[3.3620937]], train loss: [[22.85726094]]\n",
      "epoch: 33, weight: [[3.38834679]], train loss: [[22.80909652]]\n",
      "epoch: 34, weight: [[3.393876]], train loss: [[22.83553615]]\n",
      "epoch: 35, weight: [[3.38062977]], train loss: [[22.79347566]]\n",
      "epoch: 36, weight: [[3.37516257]], train loss: [[22.79741462]]\n",
      "epoch: 37, weight: [[3.38314104]], train loss: [[22.79583746]]\n",
      "epoch: 38, weight: [[3.39015879]], train loss: [[22.81635894]]\n",
      "epoch: 39, weight: [[3.35820916]], train loss: [[22.88875932]]\n",
      "epoch: 40, weight: [[3.36829024]], train loss: [[22.82001984]]\n",
      "epoch: 41, weight: [[3.37865715]], train loss: [[22.7934618]]\n",
      "epoch: 42, weight: [[3.36100774]], train loss: [[22.86543391]]\n",
      "epoch: 43, weight: [[3.37877629]], train loss: [[22.79341666]]\n",
      "epoch: 44, weight: [[3.38231077]], train loss: [[22.79476605]]\n",
      "epoch: 45, weight: [[3.37689088]], train loss: [[22.79482417]]\n",
      "epoch: 46, weight: [[3.39025268]], train loss: [[22.81677253]]\n",
      "epoch: 47, weight: [[3.3721933]], train loss: [[22.80476882]]\n",
      "epoch: 48, weight: [[3.37889948]], train loss: [[22.79337621]]\n",
      "epoch: 49, weight: [[3.40510149]], train loss: [[22.92836849]]\n",
      "epoch: 50, weight: [[3.38484332]], train loss: [[22.79893161]]\n",
      "epoch: 51, weight: [[3.3954133]], train loss: [[22.84514869]]\n",
      "epoch: 52, weight: [[3.39857975]], train loss: [[22.86804886]]\n",
      "epoch: 53, weight: [[3.38698097]], train loss: [[22.80452596]]\n",
      "epoch: 54, weight: [[3.35681922]], train loss: [[22.901556]]\n",
      "epoch: 55, weight: [[3.37292167]], train loss: [[22.80262504]]\n",
      "epoch: 56, weight: [[3.3448327]], train loss: [[23.0452911]]\n",
      "epoch: 57, weight: [[3.38434408]], train loss: [[22.79789915]]\n",
      "epoch: 58, weight: [[3.38345968]], train loss: [[22.79632485]]\n",
      "epoch: 59, weight: [[3.39066166]], train loss: [[22.81861679]]\n",
      "epoch: 60, weight: [[3.40149229]], train loss: [[22.89279868]]\n",
      "epoch: 61, weight: [[3.37845031]], train loss: [[22.79355419]]\n",
      "epoch: 62, weight: [[3.38639465]], train loss: [[22.80280217]]\n",
      "epoch: 63, weight: [[3.37056449]], train loss: [[22.81036215]]\n",
      "epoch: 64, weight: [[3.37985756]], train loss: [[22.79327726]]\n",
      "epoch: 65, weight: [[3.37815108]], train loss: [[22.79371938]]\n",
      "epoch: 66, weight: [[3.39021902]], train loss: [[22.81662382]]\n",
      "epoch: 67, weight: [[3.37932406]], train loss: [[22.7932852]]\n",
      "epoch: 68, weight: [[3.3500087]], train loss: [[22.9758851]]\n",
      "epoch: 69, weight: [[3.38612309]], train loss: [[22.80205228]]\n",
      "epoch: 70, weight: [[3.36587418]], train loss: [[22.83263884]]\n",
      "epoch: 71, weight: [[3.35829023]], train loss: [[22.88803778]]\n",
      "epoch: 72, weight: [[3.38277159]], train loss: [[22.79532527]]\n",
      "epoch: 73, weight: [[3.36401631]], train loss: [[22.84399541]]\n",
      "epoch: 74, weight: [[3.38851236]], train loss: [[22.80970339]]\n",
      "epoch: 75, weight: [[3.39007337]], train loss: [[22.81598588]]\n",
      "epoch: 76, weight: [[3.38611252]], train loss: [[22.80202372]]\n",
      "epoch: 77, weight: [[3.35643807]], train loss: [[22.90520569]]\n",
      "epoch: 78, weight: [[3.37628459]], train loss: [[22.79559129]]\n",
      "epoch: 79, weight: [[3.38513134]], train loss: [[22.79957448]]\n",
      "epoch: 80, weight: [[3.36593108]], train loss: [[22.83231372]]\n",
      "epoch: 81, weight: [[3.38235419]], train loss: [[22.79481497]]\n",
      "epoch: 82, weight: [[3.38516695]], train loss: [[22.79965636]]\n",
      "epoch: 83, weight: [[3.39198829]], train loss: [[22.82507844]]\n",
      "epoch: 84, weight: [[3.38250721]], train loss: [[22.79499363]]\n",
      "epoch: 85, weight: [[3.36116381]], train loss: [[22.86422911]]\n",
      "epoch: 86, weight: [[3.36767584]], train loss: [[22.82299837]]\n",
      "epoch: 87, weight: [[3.40371379]], train loss: [[22.91405053]]\n",
      "epoch: 88, weight: [[3.38546984]], train loss: [[22.80037417]]\n",
      "epoch: 89, weight: [[3.37638599]], train loss: [[22.79545234]]\n",
      "epoch: 90, weight: [[3.36654179]], train loss: [[22.82890893]]\n",
      "epoch: 91, weight: [[3.37042019]], train loss: [[22.81091093]]\n",
      "epoch: 92, weight: [[3.37894685]], train loss: [[22.79336233]]\n",
      "epoch: 93, weight: [[3.38252973]], train loss: [[22.79502075]]\n",
      "epoch: 94, weight: [[3.39338918]], train loss: [[22.8326972]]\n",
      "epoch: 95, weight: [[3.40107009]], train loss: [[22.88899209]]\n",
      "epoch: 96, weight: [[3.36821889]], train loss: [[22.82035767]]\n",
      "epoch: 97, weight: [[3.38098988]], train loss: [[22.79365307]]\n",
      "epoch: 98, weight: [[3.35889399]], train loss: [[22.88275005]]\n",
      "epoch: 99, weight: [[3.36146921]], train loss: [[22.86190084]]\n",
      "epoch: 100, weight: [[3.37195766]], train loss: [[22.80550967]]\n",
      "epoch: 101, weight: [[3.39222181]], train loss: [[22.82629168]]\n",
      "epoch: 102, weight: [[3.39952697]], train loss: [[22.87571051]]\n",
      "epoch: 103, weight: [[3.36573549]], train loss: [[22.83343696]]\n",
      "epoch: 104, weight: [[3.36314481]], train loss: [[22.84981785]]\n",
      "epoch: 105, weight: [[3.38925949]], train loss: [[22.81258371]]\n",
      "epoch: 106, weight: [[3.36694262]], train loss: [[22.82675863]]\n",
      "epoch: 107, weight: [[3.36541105]], train loss: [[22.83533533]]\n",
      "epoch: 108, weight: [[3.39139087]], train loss: [[22.82207788]]\n",
      "epoch: 109, weight: [[3.38078431]], train loss: [[22.79354518]]\n",
      "epoch: 110, weight: [[3.37552905]], train loss: [[22.79676144]]\n",
      "epoch: 111, weight: [[3.35465747]], train loss: [[22.92305693]]\n",
      "epoch: 112, weight: [[3.3441873]], train loss: [[23.05472758]]\n",
      "epoch: 113, weight: [[3.36669529]], train loss: [[22.82807755]]\n",
      "epoch: 114, weight: [[3.38835172]], train loss: [[22.80911444]]\n",
      "epoch: 115, weight: [[3.3577303]], train loss: [[22.89307721]]\n",
      "epoch: 116, weight: [[3.37289331]], train loss: [[22.80270438]]\n",
      "epoch: 117, weight: [[3.39437353]], train loss: [[22.83853944]]\n",
      "epoch: 118, weight: [[3.39298116]], train loss: [[22.83039384]]\n",
      "epoch: 119, weight: [[3.38319983]], train loss: [[22.79592421]]\n",
      "epoch: 120, weight: [[3.3579787]], train loss: [[22.89082548]]\n",
      "epoch: 121, weight: [[3.39009889]], train loss: [[22.81609702]]\n",
      "epoch: 122, weight: [[3.40425407]], train loss: [[22.91952967]]\n",
      "epoch: 123, weight: [[3.38060864]], train loss: [[22.79346692]]\n",
      "epoch: 124, weight: [[3.37041452]], train loss: [[22.81093269]]\n",
      "epoch: 125, weight: [[3.38066985]], train loss: [[22.79349273]]\n",
      "epoch: 126, weight: [[3.377476]], train loss: [[22.79422898]]\n",
      "epoch: 127, weight: [[3.36897997]], train loss: [[22.81686336]]\n",
      "epoch: 128, weight: [[3.39427996]], train loss: [[22.83796675]]\n",
      "epoch: 129, weight: [[3.40612213]], train loss: [[22.93941104]]\n",
      "epoch: 130, weight: [[3.3891475]], train loss: [[22.81213716]]\n",
      "epoch: 131, weight: [[3.36775737]], train loss: [[22.82259409]]\n",
      "epoch: 132, weight: [[3.39525441]], train loss: [[22.84410958]]\n",
      "epoch: 133, weight: [[3.39377388]], train loss: [[22.83493244]]\n",
      "epoch: 134, weight: [[3.35553952]], train loss: [[22.91404909]]\n",
      "epoch: 135, weight: [[3.3678713]], train loss: [[22.82203375]]\n",
      "epoch: 136, weight: [[3.37069546]], train loss: [[22.80987158]]\n",
      "epoch: 137, weight: [[3.3762473]], train loss: [[22.79564346]]\n",
      "epoch: 138, weight: [[3.36976467]], train loss: [[22.81351313]]\n",
      "epoch: 139, weight: [[3.38894826]], train loss: [[22.81135562]]\n",
      "epoch: 140, weight: [[3.37396958]], train loss: [[22.79992823]]\n",
      "epoch: 141, weight: [[3.35755914]], train loss: [[22.89464367]]\n",
      "epoch: 142, weight: [[3.38363099]], train loss: [[22.79660436]]\n",
      "epoch: 143, weight: [[3.38905437]], train loss: [[22.81176979]]\n",
      "epoch: 144, weight: [[3.37585313]], train loss: [[22.79623041]]\n",
      "epoch: 145, weight: [[3.39147992]], train loss: [[22.8225157]]\n",
      "epoch: 146, weight: [[3.41622826]], train loss: [[23.07216051]]\n",
      "epoch: 147, weight: [[3.38348013]], train loss: [[22.79635758]]\n",
      "epoch: 148, weight: [[3.39464232]], train loss: [[22.84020484]]\n",
      "epoch: 149, weight: [[3.39111128]], train loss: [[22.82072468]]\n",
      "epoch: 150, weight: [[3.39179746]], train loss: [[22.82410381]]\n",
      "epoch: 151, weight: [[3.37018778]], train loss: [[22.81181305]]\n",
      "epoch: 152, weight: [[3.37279297]], train loss: [[22.80298775]]\n",
      "epoch: 153, weight: [[3.37905541]], train loss: [[22.79333407]]\n",
      "epoch: 154, weight: [[3.37099155]], train loss: [[22.80878879]]\n",
      "epoch: 155, weight: [[3.39463828]], train loss: [[22.84017964]]\n",
      "epoch: 156, weight: [[3.36264399]], train loss: [[22.85330683]]\n",
      "epoch: 157, weight: [[3.37827011]], train loss: [[22.7936492]]\n",
      "epoch: 158, weight: [[3.37939172]], train loss: [[22.79327763]]\n",
      "epoch: 159, weight: [[3.35994117]], train loss: [[22.87393887]]\n",
      "epoch: 160, weight: [[3.38510026]], train loss: [[22.79950344]]\n",
      "epoch: 161, weight: [[3.39485145]], train loss: [[22.84152142]]\n",
      "epoch: 162, weight: [[3.38727916]], train loss: [[22.80545756]]\n",
      "epoch: 163, weight: [[3.37388869]], train loss: [[22.80012012]]\n",
      "epoch: 164, weight: [[3.36523573]], train loss: [[22.83637943]]\n",
      "epoch: 165, weight: [[3.37144917]], train loss: [[22.80718712]]\n",
      "epoch: 166, weight: [[3.37747368]], train loss: [[22.79423106]]\n",
      "epoch: 167, weight: [[3.35386905]], train loss: [[22.93138293]]\n",
      "epoch: 168, weight: [[3.39792235]], train loss: [[22.86295114]]\n",
      "epoch: 169, weight: [[3.38531895]], train loss: [[22.8000118]]\n",
      "epoch: 170, weight: [[3.39622924]], train loss: [[22.85065046]]\n",
      "epoch: 171, weight: [[3.37540938]], train loss: [[22.79696858]]\n",
      "epoch: 172, weight: [[3.38458163]], train loss: [[22.79837747]]\n",
      "epoch: 173, weight: [[3.38799358]], train loss: [[22.80784008]]\n",
      "epoch: 174, weight: [[3.35781778]], train loss: [[22.89228123]]\n",
      "epoch: 175, weight: [[3.35724532]], train loss: [[22.8975476]]\n",
      "epoch: 176, weight: [[3.396743]], train loss: [[22.85425683]]\n",
      "epoch: 177, weight: [[3.3902321]], train loss: [[22.81668156]]\n",
      "epoch: 178, weight: [[3.35331547]], train loss: [[22.93738345]]\n",
      "epoch: 179, weight: [[3.38749172]], train loss: [[22.80614422]]\n",
      "epoch: 180, weight: [[3.35943676]], train loss: [[22.87812606]]\n",
      "epoch: 181, weight: [[3.39302178]], train loss: [[22.83062003]]\n",
      "epoch: 182, weight: [[3.38619122]], train loss: [[22.80223753]]\n",
      "epoch: 183, weight: [[3.36745381]], train loss: [[22.82411345]]\n",
      "epoch: 184, weight: [[3.39572382]], train loss: [[22.84720981]]\n",
      "epoch: 185, weight: [[3.38069726]], train loss: [[22.7935048]]\n",
      "epoch: 186, weight: [[3.37872116]], train loss: [[22.79343681]]\n",
      "epoch: 187, weight: [[3.38037817]], train loss: [[22.79338375]]\n",
      "epoch: 188, weight: [[3.36291887]], train loss: [[22.85137895]]\n",
      "epoch: 189, weight: [[3.36689659]], train loss: [[22.8270022]]\n",
      "epoch: 190, weight: [[3.36595929]], train loss: [[22.83215298]]\n",
      "epoch: 191, weight: [[3.40148787]], train loss: [[22.89275838]]\n",
      "epoch: 192, weight: [[3.35039167]], train loss: [[22.9711929]]\n",
      "epoch: 193, weight: [[3.36762653]], train loss: [[22.82324426]]\n",
      "epoch: 194, weight: [[3.3796604]], train loss: [[22.79326639]]\n",
      "epoch: 195, weight: [[3.36414037]], train loss: [[22.84319229]]\n",
      "epoch: 196, weight: [[3.40193373]], train loss: [[22.89685808]]\n",
      "epoch: 197, weight: [[3.35285172]], train loss: [[22.94250857]]\n",
      "epoch: 198, weight: [[3.38923368]], train loss: [[22.81248036]]\n",
      "epoch: 199, weight: [[3.36282642]], train loss: [[22.85202386]]\n",
      "epoch: 200, weight: [[3.39101813]], train loss: [[22.82028106]]\n",
      "epoch: 201, weight: [[3.37188633]], train loss: [[22.80573849]]\n",
      "epoch: 202, weight: [[3.37218839]], train loss: [[22.80478402]]\n",
      "epoch: 203, weight: [[3.37308656]], train loss: [[22.80217038]]\n",
      "epoch: 204, weight: [[3.38475143]], train loss: [[22.79873378]]\n",
      "epoch: 205, weight: [[3.3728141]], train loss: [[22.80292773]]\n",
      "epoch: 206, weight: [[3.3603156]], train loss: [[22.87089919]]\n",
      "epoch: 207, weight: [[3.35301757]], train loss: [[22.94066545]]\n",
      "epoch: 208, weight: [[3.37954047]], train loss: [[22.79326769]]\n",
      "epoch: 209, weight: [[3.3601337]], train loss: [[22.87236859]]\n",
      "epoch: 210, weight: [[3.38869904]], train loss: [[22.8104013]]\n",
      "epoch: 211, weight: [[3.38052466]], train loss: [[22.79343406]]\n",
      "epoch: 212, weight: [[3.39187081]], train loss: [[22.82447664]]\n",
      "epoch: 213, weight: [[3.38140464]], train loss: [[22.79392431]]\n",
      "epoch: 214, weight: [[3.34748544]], train loss: [[23.00832659]]\n",
      "epoch: 215, weight: [[3.35797647]], train loss: [[22.8908455]]\n",
      "epoch: 216, weight: [[3.3765943]], train loss: [[22.7951803]]\n",
      "epoch: 217, weight: [[3.38590462]], train loss: [[22.80147128]]\n",
      "epoch: 218, weight: [[3.34855976]], train loss: [[22.99418998]]\n",
      "epoch: 219, weight: [[3.4063787]], train loss: [[22.94225513]]\n",
      "epoch: 220, weight: [[3.37575095]], train loss: [[22.79639312]]\n",
      "epoch: 221, weight: [[3.36291898]], train loss: [[22.8513782]]\n",
      "epoch: 222, weight: [[3.36324389]], train loss: [[22.84913998]]\n",
      "epoch: 223, weight: [[3.3958714]], train loss: [[22.84820349]]\n",
      "epoch: 224, weight: [[3.41288978]], train loss: [[23.02360426]]\n",
      "epoch: 225, weight: [[3.37376283]], train loss: [[22.80042411]]\n",
      "epoch: 226, weight: [[3.35800623]], train loss: [[22.89057745]]\n",
      "epoch: 227, weight: [[3.36126759]], train loss: [[22.86343357]]\n",
      "epoch: 228, weight: [[3.37819817]], train loss: [[22.79369091]]\n",
      "epoch: 229, weight: [[3.37187418]], train loss: [[22.80577767]]\n",
      "epoch: 230, weight: [[3.37479986]], train loss: [[22.79811616]]\n",
      "epoch: 231, weight: [[3.40435731]], train loss: [[22.92059052]]\n",
      "epoch: 232, weight: [[3.37639976]], train loss: [[22.7954338]]\n",
      "epoch: 233, weight: [[3.37120162]], train loss: [[22.80804273]]\n",
      "epoch: 234, weight: [[3.38467176]], train loss: [[22.79856511]]\n",
      "epoch: 235, weight: [[3.35357602]], train loss: [[22.93454328]]\n",
      "epoch: 236, weight: [[3.37797621]], train loss: [[22.79383317]]\n",
      "epoch: 237, weight: [[3.3751752]], train loss: [[22.79739119]]\n",
      "epoch: 238, weight: [[3.3785239]], train loss: [[22.79351928]]\n",
      "epoch: 239, weight: [[3.37771286]], train loss: [[22.79402857]]\n",
      "epoch: 240, weight: [[3.39151565]], train loss: [[22.82269231]]\n",
      "epoch: 241, weight: [[3.39849554]], train loss: [[22.86738588]]\n",
      "epoch: 242, weight: [[3.38352306]], train loss: [[22.79642685]]\n",
      "epoch: 243, weight: [[3.3855823]], train loss: [[22.80065039]]\n",
      "epoch: 244, weight: [[3.3549569]], train loss: [[22.91996275]]\n",
      "epoch: 245, weight: [[3.35472871]], train loss: [[22.92231739]]\n",
      "epoch: 246, weight: [[3.3843404]], train loss: [[22.79789191]]\n",
      "epoch: 247, weight: [[3.38578789]], train loss: [[22.801169]]\n",
      "epoch: 248, weight: [[3.36421705]], train loss: [[22.84269911]]\n",
      "epoch: 249, weight: [[3.37713087]], train loss: [[22.79456281]]\n",
      "epoch: 250, weight: [[3.39869145]], train loss: [[22.86893293]]\n",
      "epoch: 251, weight: [[3.37714473]], train loss: [[22.79454845]]\n",
      "epoch: 252, weight: [[3.37567564]], train loss: [[22.79651581]]\n",
      "epoch: 253, weight: [[3.37144315]], train loss: [[22.80720763]]\n",
      "epoch: 254, weight: [[3.40102233]], train loss: [[22.88856608]]\n",
      "epoch: 255, weight: [[3.3378516]], train loss: [[23.15657032]]\n",
      "epoch: 256, weight: [[3.37164029]], train loss: [[22.80654402]]\n",
      "epoch: 257, weight: [[3.37327049]], train loss: [[22.80167658]]\n",
      "epoch: 258, weight: [[3.36409387]], train loss: [[22.84349256]]\n",
      "epoch: 259, weight: [[3.38067425]], train loss: [[22.79349465]]\n",
      "epoch: 260, weight: [[3.36728511]], train loss: [[22.82497435]]\n",
      "epoch: 261, weight: [[3.36475481]], train loss: [[22.83930913]]\n",
      "epoch: 262, weight: [[3.38524555]], train loss: [[22.79983896]]\n",
      "epoch: 263, weight: [[3.38743288]], train loss: [[22.80595224]]\n",
      "epoch: 264, weight: [[3.39585011]], train loss: [[22.84805959]]\n",
      "epoch: 265, weight: [[3.40430191]], train loss: [[22.92002071]]\n",
      "epoch: 266, weight: [[3.3482437]], train loss: [[22.99829904]]\n",
      "epoch: 267, weight: [[3.39413085]], train loss: [[22.83706164]]\n",
      "epoch: 268, weight: [[3.38057714]], train loss: [[22.79345425]]\n",
      "epoch: 269, weight: [[3.38764401]], train loss: [[22.80664773]]\n",
      "epoch: 270, weight: [[3.3679627]], train loss: [[22.82158815]]\n",
      "epoch: 271, weight: [[3.35633897]], train loss: [[22.90616456]]\n",
      "epoch: 272, weight: [[3.36626924]], train loss: [[22.8304092]]\n",
      "epoch: 273, weight: [[3.36581135]], train loss: [[22.83299939]]\n",
      "epoch: 274, weight: [[3.36819644]], train loss: [[22.82046441]]\n",
      "epoch: 275, weight: [[3.36365881]], train loss: [[22.84634562]]\n",
      "epoch: 276, weight: [[3.37009193]], train loss: [[22.81219166]]\n",
      "epoch: 277, weight: [[3.39693948]], train loss: [[22.85566514]]\n",
      "epoch: 278, weight: [[3.38822785]], train loss: [[22.80866764]]\n",
      "epoch: 279, weight: [[3.38149779]], train loss: [[22.79399507]]\n",
      "epoch: 280, weight: [[3.36274926]], train loss: [[22.8525648]]\n",
      "epoch: 281, weight: [[3.37475554]], train loss: [[22.79820564]]\n",
      "epoch: 282, weight: [[3.37075133]], train loss: [[22.80966447]]\n",
      "epoch: 283, weight: [[3.38544777]], train loss: [[22.80032056]]\n",
      "epoch: 284, weight: [[3.40938613]], train loss: [[22.97763622]]\n",
      "epoch: 285, weight: [[3.38966471]], train loss: [[22.81424313]]\n",
      "epoch: 286, weight: [[3.34554451]], train loss: [[23.03508478]]\n",
      "epoch: 287, weight: [[3.37971855]], train loss: [[22.79326791]]\n",
      "epoch: 288, weight: [[3.3848766]], train loss: [[22.79900413]]\n",
      "epoch: 289, weight: [[3.3742595]], train loss: [[22.79926288]]\n",
      "epoch: 290, weight: [[3.36855578]], train loss: [[22.81878117]]\n",
      "epoch: 291, weight: [[3.3841314]], train loss: [[22.79749081]]\n",
      "epoch: 292, weight: [[3.38104817]], train loss: [[22.79368686]]\n",
      "epoch: 293, weight: [[3.40215896]], train loss: [[22.89896053]]\n",
      "epoch: 294, weight: [[3.35397136]], train loss: [[22.93028781]]\n",
      "epoch: 295, weight: [[3.37481778]], train loss: [[22.7980802]]\n",
      "epoch: 296, weight: [[3.37125452]], train loss: [[22.80785773]]\n",
      "epoch: 297, weight: [[3.35984066]], train loss: [[22.87476475]]\n",
      "epoch: 298, weight: [[3.38041349]], train loss: [[22.79339506]]\n",
      "epoch: 299, weight: [[3.34221785]], train loss: [[23.08459516]]\n",
      "epoch: 300, weight: [[3.38308943]], train loss: [[22.79576249]]\n",
      "epoch: 301, weight: [[3.39502065]], train loss: [[22.84259992]]\n",
      "epoch: 302, weight: [[3.36503562]], train loss: [[22.83758676]]\n",
      "epoch: 303, weight: [[3.38358312]], train loss: [[22.79652503]]\n",
      "epoch: 304, weight: [[3.38918623]], train loss: [[22.812291]]\n",
      "epoch: 305, weight: [[3.38503173]], train loss: [[22.79934824]]\n",
      "epoch: 306, weight: [[3.37669935]], train loss: [[22.79504997]]\n",
      "epoch: 307, weight: [[3.36003]], train loss: [[22.87321246]]\n",
      "epoch: 308, weight: [[3.41076668]], train loss: [[22.99513889]]\n",
      "epoch: 309, weight: [[3.3788737]], train loss: [[22.79338415]]\n",
      "epoch: 310, weight: [[3.36778498]], train loss: [[22.8224578]]\n",
      "epoch: 311, weight: [[3.39775654]], train loss: [[22.86169376]]\n",
      "epoch: 312, weight: [[3.3794176]], train loss: [[22.79327524]]\n",
      "epoch: 313, weight: [[3.38886249]], train loss: [[22.81102429]]\n",
      "epoch: 314, weight: [[3.39472389]], train loss: [[22.84071623]]\n",
      "epoch: 315, weight: [[3.37962731]], train loss: [[22.79326615]]\n",
      "epoch: 316, weight: [[3.35080711]], train loss: [[22.96617206]]\n",
      "epoch: 317, weight: [[3.40826593]], train loss: [[22.96401747]]\n",
      "epoch: 318, weight: [[3.36775605]], train loss: [[22.82260063]]\n",
      "epoch: 319, weight: [[3.3794936]], train loss: [[22.79326983]]\n",
      "epoch: 320, weight: [[3.38617146]], train loss: [[22.8021836]]\n",
      "epoch: 321, weight: [[3.38643655]], train loss: [[22.8029206]]\n",
      "epoch: 322, weight: [[3.39422303]], train loss: [[22.83762011]]\n",
      "epoch: 323, weight: [[3.38619034]], train loss: [[22.80223512]]\n",
      "epoch: 324, weight: [[3.38139609]], train loss: [[22.79391799]]\n",
      "epoch: 325, weight: [[3.39117462]], train loss: [[22.82102835]]\n",
      "epoch: 326, weight: [[3.39265369]], train loss: [[22.82859536]]\n",
      "epoch: 327, weight: [[3.38190075]], train loss: [[22.79434282]]\n",
      "epoch: 328, weight: [[3.38574048]], train loss: [[22.80104784]]\n",
      "epoch: 329, weight: [[3.36206907]], train loss: [[22.85744082]]\n",
      "epoch: 330, weight: [[3.35123344]], train loss: [[22.96109426]]\n",
      "epoch: 331, weight: [[3.3859711]], train loss: [[22.80164597]]\n",
      "epoch: 332, weight: [[3.3844956]], train loss: [[22.79820152]]\n",
      "epoch: 333, weight: [[3.39257279]], train loss: [[22.82815791]]\n",
      "epoch: 334, weight: [[3.38426913]], train loss: [[22.79775309]]\n",
      "epoch: 335, weight: [[3.38276297]], train loss: [[22.795314]]\n",
      "epoch: 336, weight: [[3.3681751]], train loss: [[22.82056607]]\n",
      "epoch: 337, weight: [[3.38862825]], train loss: [[22.81013495]]\n",
      "epoch: 338, weight: [[3.39046434]], train loss: [[22.81771829]]\n",
      "epoch: 339, weight: [[3.38457982]], train loss: [[22.79837375]]\n",
      "epoch: 340, weight: [[3.36209758]], train loss: [[22.8572326]]\n",
      "epoch: 341, weight: [[3.39995522]], train loss: [[22.879297]]\n",
      "epoch: 342, weight: [[3.38078497]], train loss: [[22.7935455]]\n",
      "epoch: 343, weight: [[3.39230262]], train loss: [[22.82671683]]\n",
      "epoch: 344, weight: [[3.37749054]], train loss: [[22.79421601]]\n",
      "epoch: 345, weight: [[3.3785669]], train loss: [[22.79349992]]\n",
      "epoch: 346, weight: [[3.35814884]], train loss: [[22.88929796]]\n",
      "epoch: 347, weight: [[3.3682768]], train loss: [[22.8200833]]\n",
      "epoch: 348, weight: [[3.39094296]], train loss: [[22.81992569]]\n",
      "epoch: 349, weight: [[3.38933157]], train loss: [[22.81287389]]\n",
      "epoch: 350, weight: [[3.40195633]], train loss: [[22.89706808]]\n",
      "epoch: 351, weight: [[3.36446332]], train loss: [[22.84113171]]\n",
      "epoch: 352, weight: [[3.38190294]], train loss: [[22.7943449]]\n",
      "epoch: 353, weight: [[3.36497181]], train loss: [[22.83797524]]\n",
      "epoch: 354, weight: [[3.37732409]], train loss: [[22.7943698]]\n",
      "epoch: 355, weight: [[3.36718459]], train loss: [[22.82549298]]\n",
      "epoch: 356, weight: [[3.36411088]], train loss: [[22.84338265]]\n",
      "epoch: 357, weight: [[3.40996069]], train loss: [[22.98482407]]\n",
      "epoch: 358, weight: [[3.37588024]], train loss: [[22.79618796]]\n",
      "epoch: 359, weight: [[3.39452496]], train loss: [[22.83947401]]\n",
      "epoch: 360, weight: [[3.40710928]], train loss: [[22.95050377]]\n",
      "epoch: 361, weight: [[3.36738326]], train loss: [[22.82447202]]\n",
      "epoch: 362, weight: [[3.34614291]], train loss: [[23.02666786]]\n",
      "epoch: 363, weight: [[3.37886985]], train loss: [[22.79338536]]\n",
      "epoch: 364, weight: [[3.38497677]], train loss: [[22.79922519]]\n",
      "epoch: 365, weight: [[3.396017]], train loss: [[22.84919264]]\n",
      "epoch: 366, weight: [[3.38505172]], train loss: [[22.7993933]]\n",
      "epoch: 367, weight: [[3.36571598]], train loss: [[22.8335499]]\n",
      "epoch: 368, weight: [[3.37281038]], train loss: [[22.8029383]]\n",
      "epoch: 369, weight: [[3.3575218]], train loss: [[22.8949871]]\n",
      "epoch: 370, weight: [[3.36868517]], train loss: [[22.81818825]]\n",
      "epoch: 371, weight: [[3.37438212]], train loss: [[22.79899201]]\n",
      "epoch: 372, weight: [[3.38298672]], train loss: [[22.79561661]]\n",
      "epoch: 373, weight: [[3.38642706]], train loss: [[22.80289373]]\n",
      "epoch: 374, weight: [[3.37528954]], train loss: [[22.79718198]]\n",
      "epoch: 375, weight: [[3.38034163]], train loss: [[22.79337259]]\n",
      "epoch: 376, weight: [[3.3650665]], train loss: [[22.83739937]]\n",
      "epoch: 377, weight: [[3.35149367]], train loss: [[22.95803196]]\n",
      "epoch: 378, weight: [[3.37669077]], train loss: [[22.79506045]]\n",
      "epoch: 379, weight: [[3.38669724]], train loss: [[22.80367391]]\n",
      "epoch: 380, weight: [[3.37328102]], train loss: [[22.80164873]]\n",
      "epoch: 381, weight: [[3.37399777]], train loss: [[22.79986201]]\n",
      "epoch: 382, weight: [[3.37173798]], train loss: [[22.80622115]]\n",
      "epoch: 383, weight: [[3.35101741]], train loss: [[22.96365777]]\n",
      "epoch: 384, weight: [[3.38855238]], train loss: [[22.80985178]]\n",
      "epoch: 385, weight: [[3.36790929]], train loss: [[22.82184815]]\n",
      "epoch: 386, weight: [[3.38893607]], train loss: [[22.81130835]]\n",
      "epoch: 387, weight: [[3.36715708]], train loss: [[22.82563563]]\n",
      "epoch: 388, weight: [[3.37566948]], train loss: [[22.79652596]]\n",
      "epoch: 389, weight: [[3.38813451]], train loss: [[22.80833518]]\n",
      "epoch: 390, weight: [[3.40549859]], train loss: [[22.93261328]]\n",
      "epoch: 391, weight: [[3.3999261]], train loss: [[22.87905074]]\n",
      "epoch: 392, weight: [[3.3660278]], train loss: [[22.83176414]]\n",
      "epoch: 393, weight: [[3.39126056]], train loss: [[22.82144314]]\n",
      "epoch: 394, weight: [[3.38455761]], train loss: [[22.79832804]]\n",
      "epoch: 395, weight: [[3.39603188]], train loss: [[22.84929427]]\n",
      "epoch: 396, weight: [[3.38526507]], train loss: [[22.7998847]]\n",
      "epoch: 397, weight: [[3.36073069]], train loss: [[22.8675976]]\n",
      "epoch: 398, weight: [[3.38756054]], train loss: [[22.80637056]]\n",
      "epoch: 399, weight: [[3.38202737]], train loss: [[22.79446605]]\n",
      "epoch: 400, weight: [[3.40687252]], train loss: [[22.94780635]]\n",
      "epoch: 401, weight: [[3.38218538]], train loss: [[22.79462919]]\n",
      "epoch: 402, weight: [[3.36796668]], train loss: [[22.82156883]]\n",
      "epoch: 403, weight: [[3.36945803]], train loss: [[22.81479179]]\n",
      "epoch: 404, weight: [[3.36618081]], train loss: [[22.83090264]]\n",
      "epoch: 405, weight: [[3.38283391]], train loss: [[22.79540767]]\n",
      "epoch: 406, weight: [[3.37197819]], train loss: [[22.80544422]]\n",
      "epoch: 407, weight: [[3.34364117]], train loss: [[23.06284801]]\n",
      "epoch: 408, weight: [[3.38956432]], train loss: [[22.81382565]]\n",
      "epoch: 409, weight: [[3.35248282]], train loss: [[22.94664942]]\n",
      "epoch: 410, weight: [[3.39141872]], train loss: [[22.82221446]]\n",
      "epoch: 411, weight: [[3.38938786]], train loss: [[22.813102]]\n",
      "epoch: 412, weight: [[3.38393302]], train loss: [[22.79712692]]\n",
      "epoch: 413, weight: [[3.37797218]], train loss: [[22.79383595]]\n",
      "epoch: 414, weight: [[3.38576646]], train loss: [[22.80111411]]\n",
      "epoch: 415, weight: [[3.36962705]], train loss: [[22.81408213]]\n",
      "epoch: 416, weight: [[3.36863068]], train loss: [[22.81843708]]\n",
      "epoch: 417, weight: [[3.36405364]], train loss: [[22.84375313]]\n",
      "epoch: 418, weight: [[3.37396853]], train loss: [[22.79993071]]\n",
      "epoch: 419, weight: [[3.36965796]], train loss: [[22.81395366]]\n",
      "epoch: 420, weight: [[3.38255887]], train loss: [[22.79505615]]\n",
      "epoch: 421, weight: [[3.34835778]], train loss: [[22.99681103]]\n",
      "epoch: 422, weight: [[3.37400135]], train loss: [[22.79985363]]\n",
      "epoch: 423, weight: [[3.39967668]], train loss: [[22.87695563]]\n",
      "epoch: 424, weight: [[3.39590545]], train loss: [[22.84843404]]\n",
      "epoch: 425, weight: [[3.38250946]], train loss: [[22.79499632]]\n",
      "epoch: 426, weight: [[3.36804864]], train loss: [[22.82117234]]\n",
      "epoch: 427, weight: [[3.37526913]], train loss: [[22.79721893]]\n",
      "epoch: 428, weight: [[3.36576591]], train loss: [[22.83326119]]\n",
      "epoch: 429, weight: [[3.38662414]], train loss: [[22.80345981]]\n",
      "epoch: 430, weight: [[3.3778388]], train loss: [[22.79393153]]\n",
      "epoch: 431, weight: [[3.381246]], train loss: [[22.7938121]]\n",
      "epoch: 432, weight: [[3.37971251]], train loss: [[22.79326769]]\n",
      "epoch: 433, weight: [[3.38010087]], train loss: [[22.79331298]]\n",
      "epoch: 434, weight: [[3.36469559]], train loss: [[22.83967656]]\n",
      "epoch: 435, weight: [[3.39841418]], train loss: [[22.86674806]]\n",
      "epoch: 436, weight: [[3.38485665]], train loss: [[22.79896062]]\n",
      "epoch: 437, weight: [[3.41258545]], train loss: [[23.0194088]]\n",
      "epoch: 438, weight: [[3.37554618]], train loss: [[22.79673227]]\n",
      "epoch: 439, weight: [[3.36670619]], train loss: [[22.82801889]]\n",
      "epoch: 440, weight: [[3.36742359]], train loss: [[22.82426679]]\n",
      "epoch: 441, weight: [[3.36697685]], train loss: [[22.82657814]]\n",
      "epoch: 442, weight: [[3.37111241]], train loss: [[22.8083573]]\n",
      "epoch: 443, weight: [[3.40439957]], train loss: [[22.92102601]]\n",
      "epoch: 444, weight: [[3.37265688]], train loss: [[22.80337882]]\n",
      "epoch: 445, weight: [[3.37029497]], train loss: [[22.81139422]]\n",
      "epoch: 446, weight: [[3.38127186]], train loss: [[22.79382968]]\n",
      "epoch: 447, weight: [[3.40152099]], train loss: [[22.89306013]]\n",
      "epoch: 448, weight: [[3.37852165]], train loss: [[22.79352031]]\n",
      "epoch: 449, weight: [[3.35043675]], train loss: [[22.97064469]]\n",
      "epoch: 450, weight: [[3.37581077]], train loss: [[22.79629733]]\n",
      "epoch: 451, weight: [[3.37610252]], train loss: [[22.79585153]]\n",
      "epoch: 452, weight: [[3.36892205]], train loss: [[22.81712079]]\n",
      "epoch: 453, weight: [[3.38388041]], train loss: [[22.79703316]]\n",
      "epoch: 454, weight: [[3.38624302]], train loss: [[22.80237968]]\n",
      "epoch: 455, weight: [[3.37297156]], train loss: [[22.80248629]]\n",
      "epoch: 456, weight: [[3.36901567]], train loss: [[22.81670538]]\n",
      "epoch: 457, weight: [[3.36129401]], train loss: [[22.86323178]]\n",
      "epoch: 458, weight: [[3.37024398]], train loss: [[22.81159284]]\n",
      "epoch: 459, weight: [[3.39951939]], train loss: [[22.87564771]]\n",
      "epoch: 460, weight: [[3.36619747]], train loss: [[22.83080945]]\n",
      "epoch: 461, weight: [[3.39077475]], train loss: [[22.81913905]]\n",
      "epoch: 462, weight: [[3.37659948]], train loss: [[22.79517377]]\n",
      "epoch: 463, weight: [[3.35762952]], train loss: [[22.89399812]]\n",
      "epoch: 464, weight: [[3.36425398]], train loss: [[22.84246245]]\n",
      "epoch: 465, weight: [[3.35178252]], train loss: [[22.95466596]]\n",
      "epoch: 466, weight: [[3.37810754]], train loss: [[22.79374652]]\n",
      "epoch: 467, weight: [[3.38226153]], train loss: [[22.79471153]]\n",
      "epoch: 468, weight: [[3.40116534]], train loss: [[22.88984434]]\n",
      "epoch: 469, weight: [[3.38094758]], train loss: [[22.79362943]]\n",
      "epoch: 470, weight: [[3.38595375]], train loss: [[22.8016002]]\n",
      "epoch: 471, weight: [[3.38768514]], train loss: [[22.80678539]]\n",
      "epoch: 472, weight: [[3.38198764]], train loss: [[22.79442667]]\n",
      "epoch: 473, weight: [[3.38247715]], train loss: [[22.79495776]]\n",
      "epoch: 474, weight: [[3.38985468]], train loss: [[22.81504462]]\n",
      "epoch: 475, weight: [[3.37950087]], train loss: [[22.79326944]]\n",
      "epoch: 476, weight: [[3.38632577]], train loss: [[22.80260907]]\n",
      "epoch: 477, weight: [[3.38861926]], train loss: [[22.81010126]]\n",
      "epoch: 478, weight: [[3.36618477]], train loss: [[22.83088047]]\n",
      "epoch: 479, weight: [[3.38706872]], train loss: [[22.80479626]]\n",
      "epoch: 480, weight: [[3.35108114]], train loss: [[22.96289953]]\n",
      "epoch: 481, weight: [[3.36697388]], train loss: [[22.82659379]]\n",
      "epoch: 482, weight: [[3.38543895]], train loss: [[22.80029921]]\n",
      "epoch: 483, weight: [[3.37929939]], train loss: [[22.79328844]]\n",
      "epoch: 484, weight: [[3.37305458]], train loss: [[22.80225766]]\n",
      "epoch: 485, weight: [[3.40530155]], train loss: [[22.93049882]]\n",
      "epoch: 486, weight: [[3.37373155]], train loss: [[22.80050067]]\n",
      "epoch: 487, weight: [[3.39243354]], train loss: [[22.82741134]]\n",
      "epoch: 488, weight: [[3.3652896]], train loss: [[22.83605724]]\n",
      "epoch: 489, weight: [[3.37121556]], train loss: [[22.80799385]]\n",
      "epoch: 490, weight: [[3.37650459]], train loss: [[22.79529524]]\n",
      "epoch: 491, weight: [[3.36164627]], train loss: [[22.86056884]]\n",
      "epoch: 492, weight: [[3.37101567]], train loss: [[22.80870218]]\n",
      "epoch: 493, weight: [[3.39128288]], train loss: [[22.82155133]]\n",
      "epoch: 494, weight: [[3.3959898]], train loss: [[22.84900721]]\n",
      "epoch: 495, weight: [[3.38732323]], train loss: [[22.80559837]]\n",
      "epoch: 496, weight: [[3.37833961]], train loss: [[22.79361096]]\n",
      "epoch: 497, weight: [[3.363636]], train loss: [[22.84649738]]\n",
      "epoch: 498, weight: [[3.38862029]], train loss: [[22.81010513]]\n",
      "epoch: 499, weight: [[3.39598636]], train loss: [[22.84898377]]\n",
      "epoch: 500, weight: [[3.36026131]], train loss: [[22.87133631]]\n",
      "epoch: 501, weight: [[3.37275801]], train loss: [[22.8030875]]\n",
      "epoch: 502, weight: [[3.37924186]], train loss: [[22.79329696]]\n",
      "epoch: 503, weight: [[3.37701469]], train loss: [[22.79468634]]\n",
      "epoch: 504, weight: [[3.40488902]], train loss: [[22.9261243]]\n",
      "epoch: 505, weight: [[3.37420371]], train loss: [[22.7993882]]\n",
      "epoch: 506, weight: [[3.36690724]], train loss: [[22.82694574]]\n",
      "epoch: 507, weight: [[3.36895863]], train loss: [[22.81695804]]\n",
      "epoch: 508, weight: [[3.36074599]], train loss: [[22.86747725]]\n",
      "epoch: 509, weight: [[3.39306696]], train loss: [[22.83087248]]\n",
      "epoch: 510, weight: [[3.36463042]], train loss: [[22.84008257]]\n",
      "epoch: 511, weight: [[3.36867068]], train loss: [[22.81825428]]\n",
      "epoch: 512, weight: [[3.37258387]], train loss: [[22.8035918]]\n",
      "epoch: 513, weight: [[3.37670265]], train loss: [[22.79504596]]\n",
      "epoch: 514, weight: [[3.3663397]], train loss: [[22.83001842]]\n",
      "epoch: 515, weight: [[3.38941614]], train loss: [[22.8132171]]\n",
      "epoch: 516, weight: [[3.36355465]], train loss: [[22.84704033]]\n",
      "epoch: 517, weight: [[3.38902341]], train loss: [[22.81164846]]\n",
      "epoch: 518, weight: [[3.37262978]], train loss: [[22.80345763]]\n",
      "epoch: 519, weight: [[3.36653066]], train loss: [[22.8289696]]\n",
      "epoch: 520, weight: [[3.37158728]], train loss: [[22.80672087]]\n",
      "epoch: 521, weight: [[3.3931104]], train loss: [[22.83111597]]\n",
      "epoch: 522, weight: [[3.38601453]], train loss: [[22.80176108]]\n",
      "epoch: 523, weight: [[3.38201381]], train loss: [[22.79445253]]\n",
      "epoch: 524, weight: [[3.37096946]], train loss: [[22.8088683]]\n",
      "epoch: 525, weight: [[3.40086407]], train loss: [[22.88716148]]\n",
      "epoch: 526, weight: [[3.36632339]], train loss: [[22.83010871]]\n",
      "epoch: 527, weight: [[3.36840682]], train loss: [[22.81947241]]\n",
      "epoch: 528, weight: [[3.37278907]], train loss: [[22.80299886]]\n",
      "epoch: 529, weight: [[3.41040864]], train loss: [[22.9905235]]\n",
      "epoch: 530, weight: [[3.3830731]], train loss: [[22.795739]]\n",
      "epoch: 531, weight: [[3.38130429]], train loss: [[22.79385211]]\n",
      "epoch: 532, weight: [[3.40180799]], train loss: [[22.89569352]]\n",
      "epoch: 533, weight: [[3.35528325]], train loss: [[22.91663286]]\n",
      "epoch: 534, weight: [[3.39037588]], train loss: [[22.81732076]]\n",
      "epoch: 535, weight: [[3.38154676]], train loss: [[22.79403372]]\n",
      "epoch: 536, weight: [[3.38331542]], train loss: [[22.79609896]]\n",
      "epoch: 537, weight: [[3.36620301]], train loss: [[22.83077845]]\n",
      "epoch: 538, weight: [[3.37062333]], train loss: [[22.81014088]]\n",
      "epoch: 539, weight: [[3.38001168]], train loss: [[22.79329702]]\n",
      "epoch: 540, weight: [[3.38835341]], train loss: [[22.80912056]]\n",
      "epoch: 541, weight: [[3.385163]], train loss: [[22.79964725]]\n",
      "epoch: 542, weight: [[3.38170568]], train loss: [[22.79416604]]\n",
      "epoch: 543, weight: [[3.37661942]], train loss: [[22.79514872]]\n",
      "epoch: 544, weight: [[3.38699696]], train loss: [[22.80457498]]\n",
      "epoch: 545, weight: [[3.39149455]], train loss: [[22.82258792]]\n",
      "epoch: 546, weight: [[3.36636748]], train loss: [[22.82986489]]\n",
      "epoch: 547, weight: [[3.36991635]], train loss: [[22.81289508]]\n",
      "epoch: 548, weight: [[3.38959249]], train loss: [[22.81394241]]\n",
      "epoch: 549, weight: [[3.36754398]], train loss: [[22.82365813]]\n",
      "epoch: 550, weight: [[3.39129532]], train loss: [[22.82161176]]\n",
      "epoch: 551, weight: [[3.3864045]], train loss: [[22.80282995]]\n",
      "epoch: 552, weight: [[3.38213653]], train loss: [[22.79457764]]\n",
      "epoch: 553, weight: [[3.35278266]], train loss: [[22.9432794]]\n",
      "epoch: 554, weight: [[3.38944433]], train loss: [[22.81333217]]\n",
      "epoch: 555, weight: [[3.39382757]], train loss: [[22.83524932]]\n",
      "epoch: 556, weight: [[3.37424897]], train loss: [[22.79928644]]\n",
      "epoch: 557, weight: [[3.39055322]], train loss: [[22.81812101]]\n",
      "epoch: 558, weight: [[3.39546712]], train loss: [[22.84550305]]\n",
      "epoch: 559, weight: [[3.36937242]], train loss: [[22.81515578]]\n",
      "epoch: 560, weight: [[3.37388379]], train loss: [[22.80013185]]\n",
      "epoch: 561, weight: [[3.37649555]], train loss: [[22.79530701]]\n",
      "epoch: 562, weight: [[3.36482909]], train loss: [[22.83885035]]\n",
      "epoch: 563, weight: [[3.37454243]], train loss: [[22.7986473]]\n",
      "epoch: 564, weight: [[3.39115946]], train loss: [[22.82095552]]\n",
      "epoch: 565, weight: [[3.37295463]], train loss: [[22.80253324]]\n",
      "epoch: 566, weight: [[3.38969823]], train loss: [[22.81438348]]\n",
      "epoch: 567, weight: [[3.40016392]], train loss: [[22.8810725]]\n",
      "epoch: 568, weight: [[3.37736959]], train loss: [[22.79432662]]\n",
      "epoch: 569, weight: [[3.37601821]], train loss: [[22.79597671]]\n",
      "epoch: 570, weight: [[3.39351342]], train loss: [[22.83341237]]\n",
      "epoch: 571, weight: [[3.39952217]], train loss: [[22.87567072]]\n",
      "epoch: 572, weight: [[3.3512752]], train loss: [[22.96060099]]\n",
      "epoch: 573, weight: [[3.38688456]], train loss: [[22.80423268]]\n",
      "epoch: 574, weight: [[3.35762255]], train loss: [[22.89406195]]\n",
      "epoch: 575, weight: [[3.38891851]], train loss: [[22.81124033]]\n",
      "epoch: 576, weight: [[3.40046833]], train loss: [[22.8836948]]\n",
      "epoch: 577, weight: [[3.39693265]], train loss: [[22.85561588]]\n",
      "epoch: 578, weight: [[3.37531467]], train loss: [[22.79713674]]\n",
      "epoch: 579, weight: [[3.38520853]], train loss: [[22.79975264]]\n",
      "epoch: 580, weight: [[3.38017862]], train loss: [[22.79332959]]\n",
      "epoch: 581, weight: [[3.38964015]], train loss: [[22.81414063]]\n",
      "epoch: 582, weight: [[3.37732241]], train loss: [[22.79437142]]\n",
      "epoch: 583, weight: [[3.37149668]], train loss: [[22.80702582]]\n",
      "epoch: 584, weight: [[3.38323686]], train loss: [[22.79597958]]\n",
      "epoch: 585, weight: [[3.36797093]], train loss: [[22.82154821]]\n",
      "epoch: 586, weight: [[3.38668761]], train loss: [[22.80364557]]\n",
      "epoch: 587, weight: [[3.38271884]], train loss: [[22.79525677]]\n",
      "epoch: 588, weight: [[3.39749076]], train loss: [[22.85970221]]\n",
      "epoch: 589, weight: [[3.39365585]], train loss: [[22.83424012]]\n",
      "epoch: 590, weight: [[3.37137361]], train loss: [[22.80744556]]\n",
      "epoch: 591, weight: [[3.38655987]], train loss: [[22.80327342]]\n",
      "epoch: 592, weight: [[3.38682834]], train loss: [[22.80406344]]\n",
      "epoch: 593, weight: [[3.36465284]], train loss: [[22.83994265]]\n",
      "epoch: 594, weight: [[3.38587145]], train loss: [[22.80138481]]\n",
      "epoch: 595, weight: [[3.3566813]], train loss: [[22.90286973]]\n",
      "epoch: 596, weight: [[3.38891789]], train loss: [[22.81123793]]\n",
      "epoch: 597, weight: [[3.3766476]], train loss: [[22.79511361]]\n",
      "epoch: 598, weight: [[3.36351885]], train loss: [[22.84728017]]\n",
      "epoch: 599, weight: [[3.3568598]], train loss: [[22.901171]]\n",
      "epoch: 600, weight: [[3.37356755]], train loss: [[22.8009088]]\n",
      "epoch: 601, weight: [[3.38061386]], train loss: [[22.79346907]]\n",
      "epoch: 602, weight: [[3.40360844]], train loss: [[22.91299629]]\n",
      "epoch: 603, weight: [[3.37779206]], train loss: [[22.79396677]]\n",
      "epoch: 604, weight: [[3.37997908]], train loss: [[22.79329202]]\n",
      "epoch: 605, weight: [[3.37952843]], train loss: [[22.79326815]]\n",
      "epoch: 606, weight: [[3.37576829]], train loss: [[22.7963652]]\n",
      "epoch: 607, weight: [[3.42436858]], train loss: [[23.21000944]]\n",
      "epoch: 608, weight: [[3.37529535]], train loss: [[22.79717151]]\n",
      "epoch: 609, weight: [[3.39854309]], train loss: [[22.86775988]]\n",
      "epoch: 610, weight: [[3.37210109]], train loss: [[22.80505597]]\n",
      "epoch: 611, weight: [[3.37513941]], train loss: [[22.79745778]]\n",
      "epoch: 612, weight: [[3.36528202]], train loss: [[22.83610249]]\n",
      "epoch: 613, weight: [[3.35191823]], train loss: [[22.9530965]]\n",
      "epoch: 614, weight: [[3.38941711]], train loss: [[22.81322106]]\n",
      "epoch: 615, weight: [[3.39567275]], train loss: [[22.84686805]]\n",
      "epoch: 616, weight: [[3.38189879]], train loss: [[22.79434096]]\n",
      "epoch: 617, weight: [[3.38007316]], train loss: [[22.79330767]]\n",
      "epoch: 618, weight: [[3.3845895]], train loss: [[22.79839372]]\n",
      "epoch: 619, weight: [[3.35966228]], train loss: [[22.87624092]]\n",
      "epoch: 620, weight: [[3.38218272]], train loss: [[22.79462636]]\n",
      "epoch: 621, weight: [[3.38241985]], train loss: [[22.79489044]]\n",
      "epoch: 622, weight: [[3.39309476]], train loss: [[22.83102819]]\n",
      "epoch: 623, weight: [[3.3923745]], train loss: [[22.82709724]]\n",
      "epoch: 624, weight: [[3.37999638]], train loss: [[22.79329462]]\n",
      "epoch: 625, weight: [[3.37184168]], train loss: [[22.80588279]]\n",
      "epoch: 626, weight: [[3.38104502]], train loss: [[22.793685]]\n",
      "epoch: 627, weight: [[3.38073314]], train loss: [[22.79352106]]\n",
      "epoch: 628, weight: [[3.39064212]], train loss: [[22.81852707]]\n",
      "epoch: 629, weight: [[3.37812392]], train loss: [[22.79373622]]\n",
      "epoch: 630, weight: [[3.3476405]], train loss: [[23.00625651]]\n",
      "epoch: 631, weight: [[3.37184916]], train loss: [[22.80585855]]\n",
      "epoch: 632, weight: [[3.39276402]], train loss: [[22.82919632]]\n",
      "epoch: 633, weight: [[3.38369984]], train loss: [[22.79672014]]\n",
      "epoch: 634, weight: [[3.38312937]], train loss: [[22.79582042]]\n",
      "epoch: 635, weight: [[3.39130543]], train loss: [[22.82166086]]\n",
      "epoch: 636, weight: [[3.40054302]], train loss: [[22.88434408]]\n",
      "epoch: 637, weight: [[3.38588948]], train loss: [[22.80143176]]\n",
      "epoch: 638, weight: [[3.35564533]], train loss: [[22.91299026]]\n",
      "epoch: 639, weight: [[3.35502535]], train loss: [[22.9192606]]\n",
      "epoch: 640, weight: [[3.37001453]], train loss: [[22.81250016]]\n",
      "epoch: 641, weight: [[3.38787974]], train loss: [[22.80744622]]\n",
      "epoch: 642, weight: [[3.38379865]], train loss: [[22.79688975]]\n",
      "epoch: 643, weight: [[3.3863377]], train loss: [[22.80264237]]\n",
      "epoch: 644, weight: [[3.38779677]], train loss: [[22.80716253]]\n",
      "epoch: 645, weight: [[3.39220992]], train loss: [[22.82622938]]\n",
      "epoch: 646, weight: [[3.36361956]], train loss: [[22.84660687]]\n",
      "epoch: 647, weight: [[3.37716187]], train loss: [[22.7945308]]\n",
      "epoch: 648, weight: [[3.377668]], train loss: [[22.79406474]]\n",
      "epoch: 649, weight: [[3.3889262]], train loss: [[22.81127012]]\n",
      "epoch: 650, weight: [[3.39041086]], train loss: [[22.81747755]]\n",
      "epoch: 651, weight: [[3.37937992]], train loss: [[22.79327882]]\n",
      "epoch: 652, weight: [[3.36131773]], train loss: [[22.86305079]]\n",
      "epoch: 653, weight: [[3.35959001]], train loss: [[22.87684272]]\n",
      "epoch: 654, weight: [[3.37849647]], train loss: [[22.79353203]]\n",
      "epoch: 655, weight: [[3.38832781]], train loss: [[22.8090277]]\n",
      "epoch: 656, weight: [[3.3955134]], train loss: [[22.84580874]]\n",
      "epoch: 657, weight: [[3.38699617]], train loss: [[22.80457254]]\n",
      "epoch: 658, weight: [[3.38986866]], train loss: [[22.81510422]]\n",
      "epoch: 659, weight: [[3.36717078]], train loss: [[22.82556458]]\n",
      "epoch: 660, weight: [[3.38007576]], train loss: [[22.79330815]]\n",
      "epoch: 661, weight: [[3.38827015]], train loss: [[22.80881949]]\n",
      "epoch: 662, weight: [[3.37739041]], train loss: [[22.79430714]]\n",
      "epoch: 663, weight: [[3.37324981]], train loss: [[22.8017314]]\n",
      "epoch: 664, weight: [[3.37935823]], train loss: [[22.79328114]]\n",
      "epoch: 665, weight: [[3.40883385]], train loss: [[22.97085665]]\n",
      "epoch: 666, weight: [[3.38478334]], train loss: [[22.79880208]]\n",
      "epoch: 667, weight: [[3.38271694]], train loss: [[22.79525433]]\n",
      "epoch: 668, weight: [[3.37985907]], train loss: [[22.7932774]]\n",
      "epoch: 669, weight: [[3.38639156]], train loss: [[22.80279346]]\n",
      "epoch: 670, weight: [[3.38393265]], train loss: [[22.79712625]]\n",
      "epoch: 671, weight: [[3.3832597]], train loss: [[22.79601403]]\n",
      "epoch: 672, weight: [[3.39510137]], train loss: [[22.8431187]]\n",
      "epoch: 673, weight: [[3.37723785]], train loss: [[22.79445403]]\n",
      "epoch: 674, weight: [[3.37949488]], train loss: [[22.79326976]]\n",
      "epoch: 675, weight: [[3.38320865]], train loss: [[22.79593735]]\n",
      "epoch: 676, weight: [[3.35060128]], train loss: [[22.96865065]]\n",
      "epoch: 677, weight: [[3.3652722]], train loss: [[22.83616115]]\n",
      "epoch: 678, weight: [[3.38163349]], train loss: [[22.79410463]]\n",
      "epoch: 679, weight: [[3.37680366]], train loss: [[22.7949251]]\n",
      "epoch: 680, weight: [[3.35674825]], train loss: [[22.90223099]]\n",
      "epoch: 681, weight: [[3.38345766]], train loss: [[22.79632163]]\n",
      "epoch: 682, weight: [[3.37353711]], train loss: [[22.80098578]]\n",
      "epoch: 683, weight: [[3.35332104]], train loss: [[22.93732249]]\n",
      "epoch: 684, weight: [[3.39734661]], train loss: [[22.85863433]]\n",
      "epoch: 685, weight: [[3.37383192]], train loss: [[22.80025643]]\n",
      "epoch: 686, weight: [[3.39750164]], train loss: [[22.85978313]]\n",
      "epoch: 687, weight: [[3.35629595]], train loss: [[22.90658203]]\n",
      "epoch: 688, weight: [[3.37635286]], train loss: [[22.79549726]]\n",
      "epoch: 689, weight: [[3.36564664]], train loss: [[22.8339525]]\n",
      "epoch: 690, weight: [[3.39420539]], train loss: [[22.83751296]]\n",
      "epoch: 691, weight: [[3.39695111]], train loss: [[22.85574898]]\n",
      "epoch: 692, weight: [[3.38820011]], train loss: [[22.80856847]]\n",
      "epoch: 693, weight: [[3.40814807]], train loss: [[22.96261496]]\n",
      "epoch: 694, weight: [[3.38197074]], train loss: [[22.79441011]]\n",
      "epoch: 695, weight: [[3.36982838]], train loss: [[22.81325235]]\n",
      "epoch: 696, weight: [[3.35760403]], train loss: [[22.89423166]]\n",
      "epoch: 697, weight: [[3.36747643]], train loss: [[22.82399888]]\n",
      "epoch: 698, weight: [[3.35359479]], train loss: [[22.93433985]]\n",
      "epoch: 699, weight: [[3.37795806]], train loss: [[22.79384571]]\n",
      "epoch: 700, weight: [[3.3923685]], train loss: [[22.82706541]]\n",
      "epoch: 701, weight: [[3.39779402]], train loss: [[22.86197701]]\n",
      "epoch: 702, weight: [[3.35630716]], train loss: [[22.90647317]]\n",
      "epoch: 703, weight: [[3.39416095]], train loss: [[22.83724363]]\n",
      "epoch: 704, weight: [[3.35150181]], train loss: [[22.95793671]]\n",
      "epoch: 705, weight: [[3.38462195]], train loss: [[22.79846099]]\n",
      "epoch: 706, weight: [[3.38878794]], train loss: [[22.81073874]]\n",
      "epoch: 707, weight: [[3.38409414]], train loss: [[22.79742122]]\n",
      "epoch: 708, weight: [[3.37403696]], train loss: [[22.79977048]]\n",
      "epoch: 709, weight: [[3.37616696]], train loss: [[22.79575785]]\n",
      "epoch: 710, weight: [[3.3841085]], train loss: [[22.79744798]]\n",
      "epoch: 711, weight: [[3.38980997]], train loss: [[22.81485466]]\n",
      "epoch: 712, weight: [[3.38708981]], train loss: [[22.8048617]]\n",
      "epoch: 713, weight: [[3.36806045]], train loss: [[22.82111542]]\n",
      "epoch: 714, weight: [[3.38490348]], train loss: [[22.79906305]]\n",
      "epoch: 715, weight: [[3.38515501]], train loss: [[22.79962885]]\n",
      "epoch: 716, weight: [[3.37352527]], train loss: [[22.80101583]]\n",
      "epoch: 717, weight: [[3.38426494]], train loss: [[22.79774499]]\n",
      "epoch: 718, weight: [[3.39398452]], train loss: [[22.8361824]]\n",
      "epoch: 719, weight: [[3.3931944]], train loss: [[22.83158898]]\n",
      "epoch: 720, weight: [[3.37456967]], train loss: [[22.79858979]]\n",
      "epoch: 721, weight: [[3.36956021]], train loss: [[22.81436135]]\n",
      "epoch: 722, weight: [[3.36666755]], train loss: [[22.82822707]]\n",
      "epoch: 723, weight: [[3.39321651]], train loss: [[22.831714]]\n",
      "epoch: 724, weight: [[3.38566385]], train loss: [[22.800854]]\n",
      "epoch: 725, weight: [[3.38680928]], train loss: [[22.80400636]]\n",
      "epoch: 726, weight: [[3.36955405]], train loss: [[22.8143872]]\n",
      "epoch: 727, weight: [[3.38167358]], train loss: [[22.79413846]]\n",
      "epoch: 728, weight: [[3.38144605]], train loss: [[22.79395532]]\n",
      "epoch: 729, weight: [[3.36481143]], train loss: [[22.83895917]]\n",
      "epoch: 730, weight: [[3.38357059]], train loss: [[22.79650442]]\n",
      "epoch: 731, weight: [[3.34462666]], train loss: [[23.04828479]]\n",
      "epoch: 732, weight: [[3.38026121]], train loss: [[22.79334999]]\n",
      "epoch: 733, weight: [[3.37751686]], train loss: [[22.79419274]]\n",
      "epoch: 734, weight: [[3.39021557]], train loss: [[22.81660862]]\n",
      "epoch: 735, weight: [[3.37901461]], train loss: [[22.79334411]]\n",
      "epoch: 736, weight: [[3.35834057]], train loss: [[22.88759108]]\n",
      "epoch: 737, weight: [[3.37463114]], train loss: [[22.79846115]]\n",
      "epoch: 738, weight: [[3.37574926]], train loss: [[22.79639584]]\n",
      "epoch: 739, weight: [[3.37852628]], train loss: [[22.79351818]]\n",
      "epoch: 740, weight: [[3.38014939]], train loss: [[22.79332305]]\n",
      "epoch: 741, weight: [[3.36430439]], train loss: [[22.84214032]]\n",
      "epoch: 742, weight: [[3.37072404]], train loss: [[22.80976545]]\n",
      "epoch: 743, weight: [[3.39800376]], train loss: [[22.86357265]]\n",
      "epoch: 744, weight: [[3.38249262]], train loss: [[22.79497617]]\n",
      "epoch: 745, weight: [[3.38424016]], train loss: [[22.79769726]]\n",
      "epoch: 746, weight: [[3.38347492]], train loss: [[22.79634922]]\n",
      "epoch: 747, weight: [[3.38949831]], train loss: [[22.81355344]]\n",
      "epoch: 748, weight: [[3.37437542]], train loss: [[22.79900664]]\n",
      "epoch: 749, weight: [[3.3662184]], train loss: [[22.83069253]]\n",
      "epoch: 750, weight: [[3.411999]], train loss: [[23.01143268]]\n",
      "epoch: 751, weight: [[3.36404345]], train loss: [[22.84381922]]\n",
      "epoch: 752, weight: [[3.35974823]], train loss: [[22.87552803]]\n",
      "epoch: 753, weight: [[3.37181481]], train loss: [[22.80597005]]\n",
      "epoch: 754, weight: [[3.37095877]], train loss: [[22.80890687]]\n",
      "epoch: 755, weight: [[3.37514029]], train loss: [[22.79745614]]\n",
      "epoch: 756, weight: [[3.3852774]], train loss: [[22.79991368]]\n",
      "epoch: 757, weight: [[3.36064646]], train loss: [[22.86826178]]\n",
      "epoch: 758, weight: [[3.39707249]], train loss: [[22.85662759]]\n",
      "epoch: 759, weight: [[3.37258997]], train loss: [[22.80357392]]\n",
      "epoch: 760, weight: [[3.37089318]], train loss: [[22.80914448]]\n",
      "epoch: 761, weight: [[3.38335016]], train loss: [[22.79615256]]\n",
      "epoch: 762, weight: [[3.35882555]], train loss: [[22.88334186]]\n",
      "epoch: 763, weight: [[3.36746381]], train loss: [[22.82406277]]\n",
      "epoch: 764, weight: [[3.38752079]], train loss: [[22.80623958]]\n",
      "epoch: 765, weight: [[3.40767943]], train loss: [[22.95709546]]\n",
      "epoch: 766, weight: [[3.37532924]], train loss: [[22.79711064]]\n",
      "epoch: 767, weight: [[3.3900786]], train loss: [[22.81600863]]\n",
      "epoch: 768, weight: [[3.36936153]], train loss: [[22.81520229]]\n",
      "epoch: 769, weight: [[3.39337572]], train loss: [[22.83262016]]\n",
      "epoch: 770, weight: [[3.38475254]], train loss: [[22.79873616]]\n",
      "epoch: 771, weight: [[3.39974835]], train loss: [[22.87755498]]\n",
      "epoch: 772, weight: [[3.39016451]], train loss: [[22.81638404]]\n",
      "epoch: 773, weight: [[3.37411622]], train loss: [[22.79958732]]\n",
      "epoch: 774, weight: [[3.39723883]], train loss: [[22.8578416]]\n",
      "epoch: 775, weight: [[3.3679134]], train loss: [[22.82182807]]\n",
      "epoch: 776, weight: [[3.37230422]], train loss: [[22.8044281]]\n",
      "epoch: 777, weight: [[3.37921792]], train loss: [[22.79330092]]\n",
      "epoch: 778, weight: [[3.36998582]], train loss: [[22.81261524]]\n",
      "epoch: 779, weight: [[3.36662891]], train loss: [[22.82843588]]\n",
      "epoch: 780, weight: [[3.38116953]], train loss: [[22.79376176]]\n",
      "epoch: 781, weight: [[3.37299858]], train loss: [[22.80241156]]\n",
      "epoch: 782, weight: [[3.36400773]], train loss: [[22.84405119]]\n",
      "epoch: 783, weight: [[3.36233446]], train loss: [[22.85551544]]\n",
      "epoch: 784, weight: [[3.39849413]], train loss: [[22.86737478]]\n",
      "epoch: 785, weight: [[3.3651133]], train loss: [[22.83711612]]\n",
      "epoch: 786, weight: [[3.40393318]], train loss: [[22.91626079]]\n",
      "epoch: 787, weight: [[3.37549304]], train loss: [[22.79682313]]\n",
      "epoch: 788, weight: [[3.37979246]], train loss: [[22.79327188]]\n",
      "epoch: 789, weight: [[3.37932056]], train loss: [[22.79328565]]\n",
      "epoch: 790, weight: [[3.40631175]], train loss: [[22.94151035]]\n",
      "epoch: 791, weight: [[3.36897674]], train loss: [[22.81687769]]\n",
      "epoch: 792, weight: [[3.39843343]], train loss: [[22.86689866]]\n",
      "epoch: 793, weight: [[3.35569533]], train loss: [[22.91249147]]\n",
      "epoch: 794, weight: [[3.38209108]], train loss: [[22.79453058]]\n",
      "epoch: 795, weight: [[3.38309352]], train loss: [[22.79576839]]\n",
      "epoch: 796, weight: [[3.37826366]], train loss: [[22.79365286]]\n",
      "epoch: 797, weight: [[3.37359402]], train loss: [[22.80084216]]\n",
      "epoch: 798, weight: [[3.38085166]], train loss: [[22.79357859]]\n",
      "epoch: 799, weight: [[3.3718986]], train loss: [[22.80569897]]\n",
      "epoch: 800, weight: [[3.3916302]], train loss: [[22.82326204]]\n",
      "epoch: 801, weight: [[3.3952043]], train loss: [[22.84378407]]\n",
      "epoch: 802, weight: [[3.40219838]], train loss: [[22.89933068]]\n",
      "epoch: 803, weight: [[3.37406673]], train loss: [[22.79970138]]\n",
      "epoch: 804, weight: [[3.37216829]], train loss: [[22.80484635]]\n",
      "epoch: 805, weight: [[3.37732227]], train loss: [[22.79437155]]\n",
      "epoch: 806, weight: [[3.38503867]], train loss: [[22.79936386]]\n",
      "epoch: 807, weight: [[3.37896703]], train loss: [[22.79335671]]\n",
      "epoch: 808, weight: [[3.39240098]], train loss: [[22.82723794]]\n",
      "epoch: 809, weight: [[3.37334843]], train loss: [[22.80147159]]\n",
      "epoch: 810, weight: [[3.35947894]], train loss: [[22.87777192]]\n",
      "epoch: 811, weight: [[3.38876664]], train loss: [[22.81065758]]\n",
      "epoch: 812, weight: [[3.40016732]], train loss: [[22.88110158]]\n",
      "epoch: 813, weight: [[3.38123555]], train loss: [[22.79380508]]\n",
      "epoch: 814, weight: [[3.38895138]], train loss: [[22.81136772]]\n",
      "epoch: 815, weight: [[3.37031337]], train loss: [[22.81132276]]\n",
      "epoch: 816, weight: [[3.38218866]], train loss: [[22.7946327]]\n",
      "epoch: 817, weight: [[3.3785378]], train loss: [[22.79351294]]\n",
      "epoch: 818, weight: [[3.37825406]], train loss: [[22.79365832]]\n",
      "epoch: 819, weight: [[3.37532226]], train loss: [[22.79712314]]\n",
      "epoch: 820, weight: [[3.37627222]], train loss: [[22.79560854]]\n",
      "epoch: 821, weight: [[3.3795429]], train loss: [[22.79326761]]\n",
      "epoch: 822, weight: [[3.35546063]], train loss: [[22.91484157]]\n",
      "epoch: 823, weight: [[3.38164808]], train loss: [[22.79411687]]\n",
      "epoch: 824, weight: [[3.36200526]], train loss: [[22.85790817]]\n",
      "epoch: 825, weight: [[3.36563923]], train loss: [[22.83399563]]\n",
      "epoch: 826, weight: [[3.39135573]], train loss: [[22.821906]]\n",
      "epoch: 827, weight: [[3.38791532]], train loss: [[22.80756875]]\n",
      "epoch: 828, weight: [[3.36040129]], train loss: [[22.87021175]]\n",
      "epoch: 829, weight: [[3.39623304]], train loss: [[22.85067671]]\n",
      "epoch: 830, weight: [[3.37699019]], train loss: [[22.79471311]]\n",
      "epoch: 831, weight: [[3.40504217]], train loss: [[22.92774011]]\n",
      "epoch: 832, weight: [[3.38079755]], train loss: [[22.7935516]]\n",
      "epoch: 833, weight: [[3.36859728]], train loss: [[22.81859024]]\n",
      "epoch: 834, weight: [[3.38955275]], train loss: [[22.8137778]]\n",
      "epoch: 835, weight: [[3.37037256]], train loss: [[22.811094]]\n",
      "epoch: 836, weight: [[3.39025749]], train loss: [[22.81679378]]\n",
      "epoch: 837, weight: [[3.37110927]], train loss: [[22.80836844]]\n",
      "epoch: 838, weight: [[3.36180286]], train loss: [[22.85940162]]\n",
      "epoch: 839, weight: [[3.37586536]], train loss: [[22.79621122]]\n",
      "epoch: 840, weight: [[3.3704022]], train loss: [[22.81097996]]\n",
      "epoch: 841, weight: [[3.37625811]], train loss: [[22.79562828]]\n",
      "epoch: 842, weight: [[3.3533462]], train loss: [[22.93704706]]\n",
      "epoch: 843, weight: [[3.38077937]], train loss: [[22.7935428]]\n",
      "epoch: 844, weight: [[3.37484188]], train loss: [[22.79803207]]\n",
      "epoch: 845, weight: [[3.37050103]], train loss: [[22.81060244]]\n",
      "epoch: 846, weight: [[3.36814905]], train loss: [[22.82069043]]\n",
      "epoch: 847, weight: [[3.35374954]], train loss: [[22.93266752]]\n",
      "epoch: 848, weight: [[3.3530642]], train loss: [[22.94014924]]\n",
      "epoch: 849, weight: [[3.3462]], train loss: [[23.0258726]]\n",
      "epoch: 850, weight: [[3.38933516]], train loss: [[22.81288842]]\n",
      "epoch: 851, weight: [[3.36664938]], train loss: [[22.82832519]]\n",
      "epoch: 852, weight: [[3.38824815]], train loss: [[22.80874041]]\n",
      "epoch: 853, weight: [[3.36759821]], train loss: [[22.82338593]]\n",
      "epoch: 854, weight: [[3.37620321]], train loss: [[22.7957059]]\n",
      "epoch: 855, weight: [[3.34813138]], train loss: [[22.99976919]]\n",
      "epoch: 856, weight: [[3.37246201]], train loss: [[22.80395224]]\n",
      "epoch: 857, weight: [[3.37065019]], train loss: [[22.81004034]]\n",
      "epoch: 858, weight: [[3.38560069]], train loss: [[22.80069606]]\n",
      "epoch: 859, weight: [[3.378098]], train loss: [[22.79375257]]\n",
      "epoch: 860, weight: [[3.38269826]], train loss: [[22.79523036]]\n",
      "epoch: 861, weight: [[3.38003954]], train loss: [[22.79330165]]\n",
      "epoch: 862, weight: [[3.40453584]], train loss: [[22.92243546]]\n",
      "epoch: 863, weight: [[3.39092029]], train loss: [[22.81981899]]\n",
      "epoch: 864, weight: [[3.38533917]], train loss: [[22.80005981]]\n",
      "epoch: 865, weight: [[3.3612288]], train loss: [[22.86373037]]\n",
      "epoch: 866, weight: [[3.3888978]], train loss: [[22.81116031]]\n",
      "epoch: 867, weight: [[3.3779611]], train loss: [[22.79384361]]\n",
      "epoch: 868, weight: [[3.37976033]], train loss: [[22.79326987]]\n",
      "epoch: 869, weight: [[3.39723236]], train loss: [[22.85779414]]\n",
      "epoch: 870, weight: [[3.37183653]], train loss: [[22.80589949]]\n",
      "epoch: 871, weight: [[3.37127453]], train loss: [[22.80778807]]\n",
      "epoch: 872, weight: [[3.37609503]], train loss: [[22.79586253]]\n",
      "epoch: 873, weight: [[3.39982846]], train loss: [[22.87822746]]\n",
      "epoch: 874, weight: [[3.38434854]], train loss: [[22.7979079]]\n",
      "epoch: 875, weight: [[3.39696031]], train loss: [[22.85581534]]\n",
      "epoch: 876, weight: [[3.36147995]], train loss: [[22.86181967]]\n",
      "epoch: 877, weight: [[3.39349845]], train loss: [[22.83332587]]\n",
      "epoch: 878, weight: [[3.37132893]], train loss: [[22.80759952]]\n",
      "epoch: 879, weight: [[3.38595485]], train loss: [[22.80160311]]\n",
      "epoch: 880, weight: [[3.36184169]], train loss: [[22.85911377]]\n",
      "epoch: 881, weight: [[3.36714302]], train loss: [[22.82570869]]\n",
      "epoch: 882, weight: [[3.39858298]], train loss: [[22.86807436]]\n",
      "epoch: 883, weight: [[3.37102745]], train loss: [[22.80865999]]\n",
      "epoch: 884, weight: [[3.38682132]], train loss: [[22.80404241]]\n",
      "epoch: 885, weight: [[3.37071415]], train loss: [[22.80980216]]\n",
      "epoch: 886, weight: [[3.35678094]], train loss: [[22.90191983]]\n",
      "epoch: 887, weight: [[3.36886806]], train loss: [[22.81736205]]\n",
      "epoch: 888, weight: [[3.38781798]], train loss: [[22.80723477]]\n",
      "epoch: 889, weight: [[3.38459999]], train loss: [[22.79841542]]\n",
      "epoch: 890, weight: [[3.38159118]], train loss: [[22.79406965]]\n",
      "epoch: 891, weight: [[3.36542696]], train loss: [[22.83524124]]\n",
      "epoch: 892, weight: [[3.39162834]], train loss: [[22.82325278]]\n",
      "epoch: 893, weight: [[3.37847493]], train loss: [[22.79354226]]\n",
      "epoch: 894, weight: [[3.36906707]], train loss: [[22.81647886]]\n",
      "epoch: 895, weight: [[3.36312457]], train loss: [[22.84995684]]\n",
      "epoch: 896, weight: [[3.38448621]], train loss: [[22.79818251]]\n",
      "epoch: 897, weight: [[3.37905718]], train loss: [[22.79333365]]\n",
      "epoch: 898, weight: [[3.38520241]], train loss: [[22.79973841]]\n",
      "epoch: 899, weight: [[3.3759476]], train loss: [[22.79608384]]\n",
      "epoch: 900, weight: [[3.36118197]], train loss: [[22.86408955]]\n",
      "epoch: 901, weight: [[3.36818701]], train loss: [[22.82050932]]\n",
      "epoch: 902, weight: [[3.39339669]], train loss: [[22.83274027]]\n",
      "epoch: 903, weight: [[3.381992]], train loss: [[22.79443096]]\n",
      "epoch: 904, weight: [[3.38227463]], train loss: [[22.79472593]]\n",
      "epoch: 905, weight: [[3.3943982]], train loss: [[22.83869103]]\n",
      "epoch: 906, weight: [[3.38877701]], train loss: [[22.8106971]]\n",
      "epoch: 907, weight: [[3.37276024]], train loss: [[22.8030811]]\n",
      "epoch: 908, weight: [[3.37497416]], train loss: [[22.7977722]]\n",
      "epoch: 909, weight: [[3.38366854]], train loss: [[22.79666726]]\n",
      "epoch: 910, weight: [[3.37328167]], train loss: [[22.80164703]]\n",
      "epoch: 911, weight: [[3.40808222]], train loss: [[22.96183384]]\n",
      "epoch: 912, weight: [[3.34779203]], train loss: [[23.00424322]]\n",
      "epoch: 913, weight: [[3.36023103]], train loss: [[22.87158063]]\n",
      "epoch: 914, weight: [[3.39840953]], train loss: [[22.86671169]]\n",
      "epoch: 915, weight: [[3.40046802]], train loss: [[22.88369209]]\n",
      "epoch: 916, weight: [[3.37974763]], train loss: [[22.7932692]]\n",
      "epoch: 917, weight: [[3.3811347]], train loss: [[22.79373963]]\n",
      "epoch: 918, weight: [[3.38443408]], train loss: [[22.79807759]]\n",
      "epoch: 919, weight: [[3.37059255]], train loss: [[22.81025645]]\n",
      "epoch: 920, weight: [[3.36502603]], train loss: [[22.837645]]\n",
      "epoch: 921, weight: [[3.40298876]], train loss: [[22.9068887]]\n",
      "epoch: 922, weight: [[3.37890602]], train loss: [[22.79337424]]\n",
      "epoch: 923, weight: [[3.36715789]], train loss: [[22.82563145]]\n",
      "epoch: 924, weight: [[3.38051257]], train loss: [[22.79342956]]\n",
      "epoch: 925, weight: [[3.40806888]], train loss: [[22.96167585]]\n",
      "epoch: 926, weight: [[3.38197599]], train loss: [[22.79441524]]\n",
      "epoch: 927, weight: [[3.39565906]], train loss: [[22.84677663]]\n",
      "epoch: 928, weight: [[3.37869267]], train loss: [[22.79344772]]\n",
      "epoch: 929, weight: [[3.36417423]], train loss: [[22.8429742]]\n",
      "epoch: 930, weight: [[3.38158937]], train loss: [[22.79406817]]\n",
      "epoch: 931, weight: [[3.36486786]], train loss: [[22.83861177]]\n",
      "epoch: 932, weight: [[3.3796246]], train loss: [[22.79326615]]\n",
      "epoch: 933, weight: [[3.39450308]], train loss: [[22.83933838]]\n",
      "epoch: 934, weight: [[3.39585008]], train loss: [[22.84805935]]\n",
      "epoch: 935, weight: [[3.34475656]], train loss: [[23.04639529]]\n",
      "epoch: 936, weight: [[3.37384766]], train loss: [[22.80021849]]\n",
      "epoch: 937, weight: [[3.36274647]], train loss: [[22.85258439]]\n",
      "epoch: 938, weight: [[3.37319423]], train loss: [[22.80187961]]\n",
      "epoch: 939, weight: [[3.38133898]], train loss: [[22.7938766]]\n",
      "epoch: 940, weight: [[3.36810538]], train loss: [[22.82089947]]\n",
      "epoch: 941, weight: [[3.39475611]], train loss: [[22.84091894]]\n",
      "epoch: 942, weight: [[3.36383442]], train loss: [[22.84518448]]\n",
      "epoch: 943, weight: [[3.37340479]], train loss: [[22.80132491]]\n",
      "epoch: 944, weight: [[3.37700485]], train loss: [[22.79469706]]\n",
      "epoch: 945, weight: [[3.40026726]], train loss: [[22.88195841]]\n",
      "epoch: 946, weight: [[3.39260421]], train loss: [[22.82832747]]\n",
      "epoch: 947, weight: [[3.38636716]], train loss: [[22.80272485]]\n",
      "epoch: 948, weight: [[3.35711557]], train loss: [[22.8987602]]\n",
      "epoch: 949, weight: [[3.38474498]], train loss: [[22.79872004]]\n",
      "epoch: 950, weight: [[3.35965081]], train loss: [[22.87633626]]\n",
      "epoch: 951, weight: [[3.39340544]], train loss: [[22.83279044]]\n",
      "epoch: 952, weight: [[3.36044526]], train loss: [[22.86986013]]\n",
      "epoch: 953, weight: [[3.38493943]], train loss: [[22.79914229]]\n",
      "epoch: 954, weight: [[3.3662325]], train loss: [[22.83061386]]\n",
      "epoch: 955, weight: [[3.38334952]], train loss: [[22.79615157]]\n",
      "epoch: 956, weight: [[3.38546766]], train loss: [[22.80036884]]\n",
      "epoch: 957, weight: [[3.3706595]], train loss: [[22.81000557]]\n",
      "epoch: 958, weight: [[3.37773603]], train loss: [[22.79401022]]\n",
      "epoch: 959, weight: [[3.38942732]], train loss: [[22.8132627]]\n",
      "epoch: 960, weight: [[3.38498152]], train loss: [[22.79923578]]\n",
      "epoch: 961, weight: [[3.39889984]], train loss: [[22.87059615]]\n",
      "epoch: 962, weight: [[3.39961569]], train loss: [[22.87644721]]\n",
      "epoch: 963, weight: [[3.36021469]], train loss: [[22.87171262]]\n",
      "epoch: 964, weight: [[3.37491391]], train loss: [[22.79788967]]\n",
      "epoch: 965, weight: [[3.37737887]], train loss: [[22.79431791]]\n",
      "epoch: 966, weight: [[3.38658961]], train loss: [[22.80335947]]\n",
      "epoch: 967, weight: [[3.37644635]], train loss: [[22.79537165]]\n",
      "epoch: 968, weight: [[3.39055542]], train loss: [[22.81813101]]\n",
      "epoch: 969, weight: [[3.39332193]], train loss: [[22.83231282]]\n",
      "epoch: 970, weight: [[3.39146452]], train loss: [[22.82243974]]\n",
      "epoch: 971, weight: [[3.37116693]], train loss: [[22.80816467]]\n",
      "epoch: 972, weight: [[3.3544767]], train loss: [[22.92494309]]\n",
      "epoch: 973, weight: [[3.39090486]], train loss: [[22.81974649]]\n",
      "epoch: 974, weight: [[3.35121877]], train loss: [[22.96126772]]\n",
      "epoch: 975, weight: [[3.37779851]], train loss: [[22.79396185]]\n",
      "epoch: 976, weight: [[3.35842583]], train loss: [[22.88683692]]\n",
      "epoch: 977, weight: [[3.36845414]], train loss: [[22.81925185]]\n",
      "epoch: 978, weight: [[3.37772505]], train loss: [[22.79401889]]\n",
      "epoch: 979, weight: [[3.35782798]], train loss: [[22.8921886]]\n",
      "epoch: 980, weight: [[3.373957]], train loss: [[22.79995789]]\n",
      "epoch: 981, weight: [[3.37651086]], train loss: [[22.7952871]]\n",
      "epoch: 982, weight: [[3.36205998]], train loss: [[22.85750727]]\n",
      "epoch: 983, weight: [[3.36310321]], train loss: [[22.85010367]]\n",
      "epoch: 984, weight: [[3.376538]], train loss: [[22.79525204]]\n",
      "epoch: 985, weight: [[3.38561112]], train loss: [[22.80072202]]\n",
      "epoch: 986, weight: [[3.38652997]], train loss: [[22.8031873]]\n",
      "epoch: 987, weight: [[3.37730424]], train loss: [[22.79438892]]\n",
      "epoch: 988, weight: [[3.38435411]], train loss: [[22.79791886]]\n",
      "epoch: 989, weight: [[3.37702223]], train loss: [[22.79467816]]\n",
      "epoch: 990, weight: [[3.39980664]], train loss: [[22.878044]]\n",
      "epoch: 991, weight: [[3.39080818]], train loss: [[22.81929441]]\n",
      "epoch: 992, weight: [[3.38651325]], train loss: [[22.80313931]]\n",
      "epoch: 993, weight: [[3.3724191]], train loss: [[22.80408063]]\n",
      "epoch: 994, weight: [[3.36756363]], train loss: [[22.82355935]]\n",
      "epoch: 995, weight: [[3.40849919]], train loss: [[22.96681023]]\n",
      "epoch: 996, weight: [[3.36761841]], train loss: [[22.82328485]]\n",
      "epoch: 997, weight: [[3.36613576]], train loss: [[22.83115525]]\n",
      "epoch: 998, weight: [[3.35844989]], train loss: [[22.88662469]]\n",
      "epoch: 999, weight: [[3.36303773]], train loss: [[22.85055506]]\n",
      "epoch: 1000, weight: [[3.37785555]], train loss: [[22.79391911]]\n",
      "The time used: 0.8261764049530029\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.rand(1)\n",
    "eta = 0.1\n",
    "d_train = X.size\n",
    "n_batches = 64\n",
    "\n",
    "t1 = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, d_train, n_batches):\n",
    "        xi = X[i:i + n_batches]\n",
    "        yi = y[i:i + n_batches]\n",
    "        gradient = 2 / n_batches * (w.dot(xi.T) - yi.T).dot(xi)\n",
    "        w = w - eta * gradient\n",
    "    train_loss = 1/n_batches * (w.dot(X.T) - y.T).dot((w.dot(X.T) - y.T).T)\n",
    "    print('epoch: {}, weight: {}, train loss: {}'.format(epoch + 1, w, train_loss))\n",
    "\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices] #Suffle the input X\n",
    "    y = y[indices] #Suffle the input y\n",
    "\n",
    "t2 = time.time()\n",
    "print('The time used: {}'.format(t2 - t1))\n",
    "\n",
    "y_predict = X.dot(w.T)\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, y_predict, 'r-')\n",
    "plt.xlabel(\"X\", fontsize=18)\n",
    "plt.ylabel(\"y\", fontsize=14, rotation=0)\n",
    "plt.axis([0, 2, 0, 10])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.11.0",
   "language": "python",
   "name": "v3.11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
